#!/usr/bin/env python3
# validation_dashboard.py

import sys
from pathlib import Path
import streamlit as st
import pandas as pd
import json
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

# Configuration des chemins pour ex√©cution depuis src/scripts/
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "src"))

# Configuration
st.set_page_config(
    page_title="InsightDetector - Validation Dashboard",
    page_icon="",
    layout="wide"
)

@st.cache_data
def load_data():
    """Charge toutes les donn√©es n√©cessaires avec gestion d'erreurs robuste"""
    BASE_DIR = Path(__file__).parent
    RESULTS_DIR = BASE_DIR / "data" / "results"
    
    # V√©rification de l'existence du r√©pertoire
    if not RESULTS_DIR.exists():
        st.error(f"R√©pertoire de donn√©es non trouv√©: {RESULTS_DIR}")
        st.info("Assurez-vous d'avoir ex√©cut√© le notebook d'orchestration d'abord")
        return None, None, None
    
    # R√©sum√©s et √©valuations
    try:
        data_file = RESULTS_DIR / "all_summaries_and_scores.json"
        if not data_file.exists():
            st.error(f"Fichier de donn√©es manquant: {data_file}")
            st.info("Ex√©cutez d'abord la cellule de sauvegarde dans orchestration_notebook.ipynb")
            return None, None, None
            
        with open(data_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        summaries = data.get("summaries", [])
        evaluations = data.get("evaluations", [])
        
        if not summaries or not evaluations:
            st.warning("Donn√©es vides d√©tect√©es")
            st.info(f"R√©sum√©s: {len(summaries)}, √âvaluations: {len(evaluations)}")
            
    except json.JSONDecodeError as e:
        st.error(f"Erreur de lecture JSON: {e}")
        return None, None, None
    except Exception as e:
        st.error(f"Erreur inattendue lors du chargement: {e}")
        return None, None, None
    
    # Annotations humaines (optionnel)
    annotations_dir = RESULTS_DIR / "human_annotations"
    annotations = []
    if annotations_dir.exists():
        annotation_files = list(annotations_dir.glob("annotations_*.json"))
        if annotation_files:
            try:
                with open(max(annotation_files), "r", encoding="utf-8") as f:
                    annotations = json.load(f)
                st.sidebar.success(f"Annotations humaines charg√©es: {len(annotations)}")
            except Exception as e:
                st.sidebar.warning(f"Erreur chargement annotations: {e}")
    
    return summaries, evaluations, annotations

def create_metrics_overview(evaluations):
    """Vue d'ensemble des m√©triques"""
    df = pd.DataFrame(evaluations)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            " R√©sum√©s trait√©s",
            len(df),
            delta=None
        )
    
    with col2:
        avg_factuality = df['Factualit√©'].mean()
        st.metric(
            "Factualit√© moyenne", 
            f"{avg_factuality:.3f}",
            delta=f"+{(avg_factuality-0.5):.3f}" if avg_factuality > 0.5 else f"{(avg_factuality-0.5):.3f}"
        )
    
    with col3:
        avg_coherence = df['Coh√©rence'].mean()
        st.metric(
            " Coh√©rence moyenne",
            f"{avg_coherence:.3f}",
            delta=f"+{(avg_coherence-0.5):.3f}" if avg_coherence > 0.5 else f"{(avg_coherence-0.5):.3f}"
        )
    
    with col4:
        avg_composite = df['Score composite'].mean()
        st.metric(
            "Score global moyen",
            f"{avg_composite:.3f}",
            delta=f"+{(avg_composite-0.5):.3f}" if avg_composite > 0.5 else f"{(avg_composite-0.5):.3f}"
        )

def create_distribution_plots(evaluations):
    """Graphiques de distribution des scores"""
    df = pd.DataFrame(evaluations)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Distribution des scores principaux
        fig = px.histogram(
            df, 
            x=['Factualit√©', 'Coh√©rence', 'Lisibilit√©', 'Score composite'],
            title=" Distribution des Scores Principaux",
            nbins=20
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Box plot comparatif
        metrics_data = []
        for metric in ['Factualit√©', 'Coh√©rence', 'Lisibilit√©', 'Engagement']:
            for score in df[metric]:
                metrics_data.append({
                    'M√©trique': metric,
                    'Score': score
                })
        
        df_metrics = pd.DataFrame(metrics_data)
        fig = px.box(
            df_metrics,
            x='M√©trique',
            y='Score',
            title=" Comparaison des M√©triques (Box Plot)"
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

def summary_explorer(summaries, evaluations):
    """Explorateur de r√©sum√©s interactif"""
    st.subheader(" Explorateur de R√©sum√©s")
    
    df_eval = pd.DataFrame(evaluations)
    
    # Filtres
    col1, col2, col3 = st.columns(3)
    
    with col1:
        factuality_range = st.slider(
            "Plage Factualit√©",
            0.0, 1.0, (0.0, 1.0), 0.1
        )
    
    with col2:
        coherence_range = st.slider(
            "Plage Coh√©rence", 
            0.0, 1.0, (0.0, 1.0), 0.1
        )
    
    with col3:
        sort_by = st.selectbox(
            "Trier par",
            ['Score composite', 'Factualit√©', 'Coh√©rence', 'Lisibilit√©'],
            index=0
        )
    
    # Filtrage
    mask = (
        (df_eval['Factualit√©'] >= factuality_range[0]) &
        (df_eval['Factualit√©'] <= factuality_range[1]) &
        (df_eval['Coh√©rence'] >= coherence_range[0]) &
        (df_eval['Coh√©rence'] <= coherence_range[1])
    )
    
    filtered_df = df_eval[mask].sort_values(sort_by, ascending=False)
    
    st.write(f"**{len(filtered_df)} r√©sum√©s correspondants**")
    
    # S√©lection d'un r√©sum√© sp√©cifique
    if len(filtered_df) > 0:
        selected_idx = st.selectbox(
            "S√©lectionner un r√©sum√©",
            range(len(filtered_df)),
            format_func=lambda x: f"#{x+1} - {filtered_df.iloc[x]['summary_id']} (Score: {filtered_df.iloc[x]['Score composite']:.3f})"
        )
        
        # Afficher le r√©sum√© s√©lectionn√©
        if selected_idx is not None:
            selected_summary_id = filtered_df.iloc[selected_idx]['summary_id']
            summary_data = next(s for s in summaries if s['summary_id'] == selected_summary_id)
            eval_data = filtered_df.iloc[selected_idx]
            
            st.markdown("---")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("###  R√©sum√© G√©n√©r√©")
                st.info(summary_data['ensemble_summary']['summary'])
                
                # M√©tadonn√©es
                st.markdown("###  M√©tadonn√©es")
                metadata = summary_data.get('metadata', {})
                st.json({
                    'Longueur r√©sum√©': summary_data['ensemble_summary']['length'],
                    'Longueur source': metadata.get('source_length', 'N/A'),
                    'Temps g√©n√©ration': f"{metadata.get('runtime_seconds', 'N/A')}s",
                    'Strat√©gie': metadata.get('fusion_strategy', 'N/A')
                })
            
            with col2:
                st.markdown("###  Scores D√©taill√©s")
                
                # Graphique radar des scores
                scores = {
                    'Factualit√©': eval_data['Factualit√©'],
                    'Coh√©rence': eval_data['Coh√©rence'], 
                    'Lisibilit√©': eval_data['Lisibilit√©'],
                    'Engagement': eval_data['Engagement']
                }
                
                fig = go.Figure()
                fig.add_trace(go.Scatterpolar(
                    r=list(scores.values()),
                    theta=list(scores.keys()),
                    fill='toself',
                    name='Scores'
                ))
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(visible=True, range=[0, 1])
                    ),
                    height=300
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # Scores num√©riques
                for metric, score in scores.items():
                    st.metric(metric, f"{score:.3f}")

def human_vs_automatic_comparison(annotations, evaluations):
    """Comparaison √©valuations humaines vs automatiques"""
    if not annotations:
        st.info(" Aucune annotation humaine disponible pour comparaison")
        return
    
    st.subheader("üë• Comparaison Humain vs Automatique")
    
    # Pr√©parer les donn√©es
    human_data = []
    for ann in annotations:
        summary_id = ann['summary_id']
        eval_data = next((e for e in evaluations if e['summary_id'] == summary_id), None)
        if eval_data:
            human_data.append({
                'summary_id': summary_id,
                'human_factuality': ann['human_scores']['factuality'] / 5,  # Normaliser 1-5 -> 0-1
                'human_coherence': ann['human_scores']['coherence'] / 5,
                'human_overall': ann['human_scores']['overall'] / 5,
                'auto_factuality': eval_data['Factualit√©'],
                'auto_coherence': eval_data['Coh√©rence'],
                'auto_composite': eval_data['Score composite']
            })
    
    if not human_data:
        st.warning(" Aucune correspondance trouv√©e entre annotations humaines et automatiques")
        return
    
    df_comparison = pd.DataFrame(human_data)
    
    # Graphiques de corr√©lation
    col1, col2, col3 = st.columns(3)
    
    with col1:
        fig = px.scatter(
            df_comparison,
            x='auto_factuality',
            y='human_factuality',
            title="Factualit√©: Humain vs Auto",
            trendline="ols"
        )
        fig.add_shape(type="line", x0=0, y0=0, x1=1, y1=1, line=dict(dash="dash", color="red"))
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        fig = px.scatter(
            df_comparison,
            x='auto_coherence',
            y='human_coherence', 
            title="Coh√©rence: Humain vs Auto",
            trendline="ols"
        )
        fig.add_shape(type="line", x0=0, y0=0, x1=1, y1=1, line=dict(dash="dash", color="red"))
        st.plotly_chart(fig, use_container_width=True)
    
    with col3:
        fig = px.scatter(
            df_comparison,
            x='auto_composite',
            y='human_overall',
            title="Score Global: Humain vs Auto", 
            trendline="ols"
        )
        fig.add_shape(type="line", x0=0, y0=0, x1=1, y1=1, line=dict(dash="dash", color="red"))
        st.plotly_chart(fig, use_container_width=True)
    
    # Calcul des corr√©lations
    st.markdown("###  Corr√©lations")
    correlations = {
        'Factualit√©': df_comparison[['human_factuality', 'auto_factuality']].corr().iloc[0,1],
        'Coh√©rence': df_comparison[['human_coherence', 'auto_coherence']].corr().iloc[0,1],
        'Score Global': df_comparison[['human_overall', 'auto_composite']].corr().iloc[0,1]
    }
    
    col1, col2, col3 = st.columns(3)
    for i, (metric, corr) in enumerate(correlations.items()):
        with [col1, col2, col3][i]:
            color = "normal" if corr > 0.7 else "inverse"
            st.metric(f"r({metric})", f"{corr:.3f}", delta=None)

def main():
    """Application principale"""
    st.title(" InsightDetector - Dashboard de Validation")
    st.markdown("*Analyse et validation des r√©sum√©s g√©n√©r√©s par l'IA*")
    
    # Chargement des donn√©es
    with st.spinner("Chargement des donn√©es..."):
        summaries, evaluations, annotations = load_data()
    
    if summaries is None:
        st.stop()
    
    # Sidebar pour navigation
    st.sidebar.title(" Navigation")
    page = st.sidebar.selectbox(
        "Choisir une section",
        [" Vue d'ensemble", " Explorateur", "üë• Comparaison Humain/Auto", "‚öôÔ∏è Param√®tres"]
    )
    
    if page == " Vue d'ensemble":
        st.header(" Vue d'Ensemble des Performances")
        create_metrics_overview(evaluations)
        st.markdown("---")
        create_distribution_plots(evaluations)
        
    elif page == " Explorateur":
        summary_explorer(summaries, evaluations)
        
    elif page == " Comparaison Humain/Auto":
        human_vs_automatic_comparison(annotations, evaluations)
        
    elif page == " Param√®tres":
        st.header(" Configuration")
        st.info("Fonctionnalit√©s de configuration √† venir...")
        
        # Export des donn√©es
        if st.button(" Exporter les donn√©es"):
            export_data = {
                'summaries': summaries,
                'evaluations': evaluations,
                'annotations': annotations
            }
            st.download_button(
                "‚¨á T√©l√©charger JSON complet",
                json.dumps(export_data, ensure_ascii=False, indent=2),
                "insightdetector_export.json",
                "application/json"
            )

if __name__ == "__main__":
    main()