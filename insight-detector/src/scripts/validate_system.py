#!/usr/bin/env python3
"""
Script de validation compl√®te du syst√®me InsightDetector
V√©rifie tous les composants critiques avant le passage √† la Phase 4
"""

import sys
import json
import importlib
from pathlib import Path
import pandas as pd
from datetime import datetime

# Configuration des chemins pour ex√©cution depuis src/scripts/
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "src"))

def print_status(message, status="INFO"):
    """Affichage format√© des statuts"""
    symbols = {"INFO": "‚ÑπÔ∏è", "SUCCESS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå"}
    print(f"{symbols.get(status, '‚ÑπÔ∏è')} {message}")

def check_dependencies():
    """V√©rification des d√©pendances critiques"""
    print_status("=== V√âRIFICATION DES D√âPENDANCES ===")
    
    required_packages = [
        'torch', 'transformers', 'sentence_transformers',
        'rouge_score', 'bert_score', 'nltk', 'spacy',
        'streamlit', 'plotly', 'ipywidgets'
    ]
    
    missing = []
    for package in required_packages:
        try:
            importlib.import_module(package.replace('-', '_'))
            print_status(f"{package} install√©", "SUCCESS")
        except ImportError:
            print_status(f"{package} MANQUANT", "ERROR")
            missing.append(package)
    
    if missing:
        print_status(f"Installer: pip install {' '.join(missing)}", "WARNING")
        return False
    return True

def check_models():
    """V√©rification des mod√®les ML"""
    print_status("=== V√âRIFICATION DES MOD√àLES ===")
    
    try:
        sys.path.append(str(Path(__file__).parent / "src"))
        from models.abstractive_models import AbstractiveEnsemble
        
        ensemble = AbstractiveEnsemble(device="cpu")
        load_results = ensemble.load_models(["barthez", "french_t5"], max_models=2)
        
        loaded_count = sum(load_results.values())
        total_count = len(load_results)
        
        print_status(f"Mod√®les charg√©s: {loaded_count}/{total_count}", 
                    "SUCCESS" if loaded_count > 1 else "WARNING")
        
        for model, success in load_results.items():
            status = "SUCCESS" if success else "ERROR"
            print_status(f"  - {model}", status)
            
        return loaded_count > 0
        
    except Exception as e:
        print_status(f"Erreur mod√®les: {e}", "ERROR")
        return False

def check_data():
    """V√©rification des donn√©es"""
    print_status("=== V√âRIFICATION DES DONN√âES ===")
    
    base_dir = Path(__file__).parent
    results_dir = base_dir / "data" / "results"
    
    # Fichier principal
    data_file = results_dir / "all_summaries_and_scores.json"
    if not data_file.exists():
        print_status(f"Fichier principal manquant: {data_file}", "ERROR")
        return False
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        summaries = data.get("summaries", [])
        evaluations = data.get("evaluations", [])
        
        print_status(f"R√©sum√©s: {len(summaries)}", "SUCCESS" if summaries else "ERROR")
        print_status(f"√âvaluations: {len(evaluations)}", "SUCCESS" if evaluations else "ERROR")
        
        # V√©rification m√©triques ROUGE/BERTScore
        if evaluations:
            df = pd.DataFrame(evaluations)
            rouge_valid = df['ROUGE-1 (f)'].notna().sum()
            bert_valid = df['BERTScore (f1)'].notna().sum()
            
            print_status(f"ROUGE valides: {rouge_valid}/{len(evaluations)}", 
                        "SUCCESS" if rouge_valid > len(evaluations)//2 else "WARNING")
            print_status(f"BERTScore valides: {bert_valid}/{len(evaluations)}", 
                        "SUCCESS" if bert_valid > len(evaluations)//2 else "WARNING")
            
            # Statistiques de coh√©rence
            coherence_mean = df['Coh√©rence'].mean()
            coherence_low = (df['Coh√©rence'] < 0.3).sum()
            
            print_status(f"Coh√©rence moyenne: {coherence_mean:.3f}", 
                        "SUCCESS" if coherence_mean > 0.4 else "WARNING")
            print_status(f"R√©sum√©s coh√©rence < 0.3: {coherence_low}", 
                        "WARNING" if coherence_low > len(evaluations)//4 else "SUCCESS")
        
        return len(summaries) > 0 and len(evaluations) > 0
        
    except Exception as e:
        print_status(f"Erreur lecture donn√©es: {e}", "ERROR")
        return False

def check_interfaces():
    """V√©rification des interfaces"""
    print_status("=== V√âRIFICATION DES INTERFACES ===")
    
    base_dir = Path(__file__).parent
    
    # Dashboard Streamlit
    dashboard_file = base_dir / "validation_dashboard.py"
    if dashboard_file.exists():
        print_status("Dashboard Streamlit pr√©sent", "SUCCESS")
    else:
        print_status("Dashboard Streamlit manquant", "ERROR")
    
    # Notebook d'annotation
    annotation_file = base_dir / "notebooks" / "04_orchestration" / "annotation_tool.ipynb"
    if annotation_file.exists():
        print_status("Outil d'annotation pr√©sent", "SUCCESS")
    else:
        print_status("Outil d'annotation manquant", "ERROR")
    
    # Annotations humaines
    annotations_dir = base_dir / "data" / "results" / "human_annotations"
    if annotations_dir.exists():
        annotation_files = list(annotations_dir.glob("annotations_*.json"))
        if annotation_files:
            with open(max(annotation_files), 'r', encoding='utf-8') as f:
                annotations = json.load(f)
            print_status(f"Annotations humaines: {len(annotations)}", 
                        "SUCCESS" if len(annotations) >= 10 else "WARNING")
        else:
            print_status("Aucune annotation humaine", "WARNING")
    else:
        print_status("R√©pertoire annotations manquant", "WARNING")
    
    return True

def generate_report():
    """G√©n√©ration du rapport de validation"""
    print_status("=== RAPPORT DE VALIDATION ===")
    
    checks = {
        "D√©pendances": check_dependencies(),
        "Mod√®les ML": check_models(), 
        "Donn√©es": check_data(),
        "Interfaces": check_interfaces()
    }
    
    passed = sum(checks.values())
    total = len(checks)
    
    print_status(f"\nR√âSULTAT GLOBAL: {passed}/{total} v√©rifications r√©ussies")
    
    if passed == total:
        print_status("üéâ SYST√àME PR√äT POUR PHASE 4", "SUCCESS")
        next_steps = [
            "1. Lancer le dashboard: streamlit run validation_dashboard.py",
            "2. Compl√©ter les annotations humaines (20+ recommand√©)",
            "3. Corriger la coh√©rence si < 0.4",
            "4. Impl√©menter la d√©tection d'hallucinations"
        ]
    elif passed >= total * 0.75:
        print_status("‚ö†Ô∏è SYST√àME PARTIELLEMENT PR√äT", "WARNING")
        next_steps = [
            "1. Corriger les erreurs critiques",
            "2. Relancer le notebook d'orchestration si n√©cessaire",
            "3. V√©rifier l'installation des d√©pendances manquantes"
        ]
    else:
        print_status("‚ùå SYST√àME NON PR√äT", "ERROR")
        next_steps = [
            "1. R√©soudre tous les probl√®mes critiques",
            "2. R√©installer les d√©pendances: pip install -r requirements.txt",
            "3. Relancer compl√®tement le notebook d'orchestration"
        ]
    
    print_status("\nüîÑ PROCHAINES √âTAPES RECOMMAND√âES:")
    for step in next_steps:
        print_status(f"   {step}")
    
    # Sauvegarde du rapport
    report = {
        "timestamp": datetime.now().isoformat(),
        "checks": checks,
        "score": passed / total,
        "next_steps": next_steps
    }
    
    report_file = Path(__file__).parent / "data" / "results" / "validation_report.json"
    report_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print_status(f"Rapport sauvegard√©: {report_file}")
    
    return passed / total

if __name__ == "__main__":
    print("üß† InsightDetector - Validation Syst√®me")
    print("=" * 50)
    
    score = generate_report()
    
    # Code de sortie pour scripts automatis√©s
    sys.exit(0 if score >= 0.75 else 1)