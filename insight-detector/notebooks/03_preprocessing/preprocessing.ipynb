{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "839b63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Auto-reload pour d√©veloppement interactif\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP avanc√©\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# D√©tection de langue\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Preprocessing texte\n",
    "import unicodedata\n",
    "import ftfy  # Pour corriger les encodages\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Similarit√© et d√©duplication\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86cbfdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©pertoire de donn√©es: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\n",
      "R√©pertoire de sortie: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path().resolve().parent.parent\n",
    "sys.path.append(str(BASE_DIR / \"src\"))\n",
    "\n",
    "# R√©pertoires\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "EXPORTS_DIR = DATA_DIR / \"exports\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration des mod√®les\n",
    "#NLP_MODEL = \"fr_core_news_lg\"  # Mod√®le spaCy fran√ßais\n",
    "NLP_MODEL = \"fr_core_news_sm\"  # Mod√®le spaCy fran√ßais l√©ger pour √©viter les probl√®mes de m√©moire\n",
    "EMBEDDINGS_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "SIMILARITY_THRESHOLD = 0.85  # Seuil de similarit√© pour d√©duplication\n",
    "\n",
    "print(f\"R√©pertoire de donn√©es: {DATA_DIR}\")\n",
    "print(f\"R√©pertoire de sortie: {PROCESSED_DIR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2ad59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier JSON enrichi (priorit√©) ou brut (fallback)\n",
    "enriched_file = EXPORTS_DIR / \"enriched_article.json\"  # MODIFI√â: sans \"s\"\n",
    "enriched_files_alt = EXPORTS_DIR / \"enriched_articles.json\"  # Alternative\n",
    "raw_file = EXPORTS_DIR / \"raw_articles.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f852fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FICHIER ENRICHI ALTERNATIF D√âTECT√â: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\exports\\enriched_articles.json\n",
      "   Mode: Preprocessing avanc√© sur donn√©es pr√©-enrichies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# D√©tection automatique du fichier source - MODIFI√âE\n",
    "if enriched_file.exists():\n",
    "    source_file = enriched_file\n",
    "    print(f\" FICHIER ENRICHI D√âTECT√â: {enriched_file}\")\n",
    "    print(\"   Mode: Preprocessing avanc√© sur donn√©es pr√©-enrichies\")\n",
    "elif enriched_files_alt.exists():\n",
    "    source_file = enriched_files_alt\n",
    "    print(f\" FICHIER ENRICHI ALTERNATIF D√âTECT√â: {enriched_files_alt}\")\n",
    "    print(\"   Mode: Preprocessing avanc√© sur donn√©es pr√©-enrichies\")\n",
    "elif raw_file.exists():\n",
    "    source_file = raw_file\n",
    "    print(f\" FICHIER BRUT D√âTECT√â: {raw_file}\")\n",
    "    print(\"   ‚Üí Mode: Preprocessing complet depuis z√©ro\")\n",
    "else:\n",
    "    print(f\" ERREUR: Aucun fichier source trouv√©!\")\n",
    "    print(f\"   Recherche: {enriched_file} OU {raw_file}\")\n",
    "    print(\"   Solution: Ex√©cutez d'abord collect_articles.ipynb ou enrich_articles.ipynb\")\n",
    "    exit(1)\n",
    "\n",
    "with open(source_file, 'r', encoding='utf-8') as f:\n",
    "    articles_data = json.load(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d01366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 articles charg√©s depuis enriched_articles.json\n"
     ]
    }
   ],
   "source": [
    "# Mode adaptatif selon la source - MODIFI√â\n",
    "ENRICHED_MODE = \"enriched_article\" in str(source_file)  # Compatible avec les deux formats\n",
    "\n",
    "print(f\" {len(articles_data)} articles charg√©s depuis {source_file.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ec2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame pour manipulation\n",
    "df = pd.DataFrame(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0279f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HARMONISATION DES NOMS DE COLONNES\n",
      "    Harmonisation: cleaned_text ‚Üí text_cleaned\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n HARMONISATION DES NOMS DE COLONNES\")\n",
    "\n",
    "# Harmonisation text_cleaned vs cleaned_text\n",
    "if 'cleaned_text' in df.columns and 'text_cleaned' not in df.columns:\n",
    "    df['text_cleaned'] = df['cleaned_text']\n",
    "    print(\"    Harmonisation: cleaned_text ‚Üí text_cleaned\")\n",
    "elif 'text' in df.columns and 'text_cleaned' not in df.columns:\n",
    "    df['text_cleaned'] = df['text']\n",
    "    print(\"    Cr√©ation: text ‚Üí text_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6dfd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Toutes les colonnes essentielles pr√©sentes\n"
     ]
    }
   ],
   "source": [
    "# V√©rification des colonnes essentielles\n",
    "required_columns = ['title', 'text', 'source']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"     Colonnes manquantes: {missing_columns}\")\n",
    "else:\n",
    "    print(\"    Toutes les colonnes essentielles pr√©sentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bee8759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STRUCTURE DES DONN√âES:\n",
      "   Colonnes: ['id', 'title', 'summary', 'text', 'published', 'source', 'url', 'created_at', 'cleaned_text', 'language', 'entities', 'embedding', 'quality_score', 'text_cleaned']\n",
      "   Articles avec texte complet: 200\n",
      "   Articles sans texte: 0\n",
      "   Articles avec texte nettoy√©: 200\n",
      "   Longueur moyenne du texte: 6995 caract√®res\n"
     ]
    }
   ],
   "source": [
    "# Inspection rapide\n",
    "print(f\"\\n STRUCTURE DES DONN√âES:\")\n",
    "print(f\"   Colonnes: {list(df.columns)}\")\n",
    "print(f\"   Articles avec texte complet: {df['text'].notna().sum()}\")\n",
    "print(f\"   Articles sans texte: {df['text'].isna().sum()}\")\n",
    "if 'text_cleaned' in df.columns:\n",
    "    print(f\"   Articles avec texte nettoy√©: {df['text_cleaned'].notna().sum()}\")\n",
    "print(f\"   Longueur moyenne du texte: {df['text'].str.len().mean():.0f} caract√®res\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b59b4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " √âTAPE 2: Analyse adaptative des donn√©es\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n √âTAPE 2: Analyse adaptative des donn√©es\")\n",
    "\n",
    "# D√©tection des colonnes d'enrichissement d√©j√† pr√©sentes\n",
    "enrichment_columns = {\n",
    "    'language': 'language' in df.columns and df['language'].notna().sum() > 0,\n",
    "    'entities': 'entities' in df.columns and df['entities'].notna().sum() > 0,\n",
    "    'quality_score': 'quality_score' in df.columns and df['quality_score'].notna().sum() > 0,\n",
    "    'embedding': 'embedding' in df.columns and df['embedding'].notna().sum() > 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e46e3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " √âTAT DES ENRICHISSEMENTS EXISTANTS:\n",
      "   language:  Pr√©sent (200 articles)\n",
      "   entities:  Pr√©sent (200 articles)\n",
      "   quality_score:  Pr√©sent (200 articles)\n",
      "   embedding:  Pr√©sent (200 articles)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n √âTAT DES ENRICHISSEMENTS EXISTANTS:\")\n",
    "for col, present in enrichment_columns.items():\n",
    "    status = \" Pr√©sent\" if present else \" Absent\" \n",
    "    count = df[col].notna().sum() if present else 0\n",
    "    print(f\"   {col}: {status} ({count} articles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0299ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MODE D√âTECT√â: Preprocessing compl√©mentaire avanc√©\n",
      "   ‚Üí Focus: D√©duplication, biais, corpus calibration, m√©triques avanc√©es\n"
     ]
    }
   ],
   "source": [
    "# Adaptation de la strat√©gie\n",
    "if enrichment_columns['language'] and enrichment_columns['entities']:\n",
    "    print(f\"\\n MODE D√âTECT√â: Preprocessing compl√©mentaire avanc√©\")\n",
    "    print(f\"   ‚Üí Focus: D√©duplication, biais, corpus calibration, m√©triques avanc√©es\")\n",
    "    SKIP_BASIC_ENRICHMENT = True\n",
    "else:\n",
    "    print(f\"\\n MODE D√âTECT√â: Preprocessing complet depuis z√©ro\") \n",
    "    print(f\"   ‚Üí Pipeline: Enrichissement + Analyses avanc√©es\")\n",
    "    SKIP_BASIC_ENRICHMENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9abb81e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " √âTAPE 3: Nettoyage avanc√© des donn√©es\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n √âTAPE 3: Nettoyage avanc√© des donn√©es\")\n",
    "\n",
    "def clean_text_advanced(text):\n",
    "    \"\"\"Nettoyage robuste et avanc√© du texte\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Correction encodage\n",
    "    text = ftfy.fix_text(text)\n",
    "    \n",
    "    # Suppression HTML r√©siduel\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Normalisation Unicode\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Suppression caract√®res de contr√¥le\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # Normalisation espaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Suppression URLs et emails\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Suppression patterns RSS sp√©cifiques\n",
    "    text = re.sub(r'#xtor=RSS-\\d+.*', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]$', '', text)  # Cr√©dits en fin d'article\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b31837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Textes d√©j√† nettoy√©s d√©tect√©s\n"
     ]
    }
   ],
   "source": [
    "# Application du nettoyage si n√©cessaire\n",
    "if 'text_cleaned' not in df.columns or df['text_cleaned'].isna().any():\n",
    "    print(\"    Application du nettoyage avanc√©...\")\n",
    "    df['text_cleaned'] = df['text'].apply(clean_text_advanced)\n",
    "    print(f\"       {len(df)} textes nettoy√©s\")\n",
    "else:\n",
    "    print(\"    Textes d√©j√† nettoy√©s d√©tect√©s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9152712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Filtrage longueur minimum (100 chars): 200 ‚Üí 200 articles\n"
     ]
    }
   ],
   "source": [
    "# Filtrage des articles trop courts ou vides\n",
    "min_length = 100  # caract√®res minimum\n",
    "df_clean = df[df['text_cleaned'].str.len() >= min_length].copy()\n",
    "print(f\"    Filtrage longueur minimum ({min_length} chars): {len(df)} ‚Üí {len(df_clean)} articles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3bb1357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "√âTAPE 4: Gestion intelligente de la langue\n",
      "     Langues d√©j√† d√©tect√©es, validation des donn√©es...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n√âTAPE 4: Gestion intelligente de la langue\")\n",
    "\n",
    "if not enrichment_columns['language'] or not SKIP_BASIC_ENRICHMENT:\n",
    "    print(\"    D√©tection de langue en cours...\")\n",
    "    \n",
    "    def detect_language_robust(text):\n",
    "        \"\"\"D√©tection de langue avec fallback\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return 'unknown', 0.0\n",
    "        \n",
    "        try:\n",
    "            # langdetect avec probabilit√©s\n",
    "            langs = detect_langs(text)\n",
    "            primary_lang = langs[0]\n",
    "            return primary_lang.lang, primary_lang.prob\n",
    "        except LangDetectException:\n",
    "            # Fallback: d√©tection basique\n",
    "            try:\n",
    "                return detect(text), 0.5\n",
    "            except:\n",
    "                return 'unknown', 0.0\n",
    "\n",
    "    # Application de la d√©tection\n",
    "    language_results = df_clean['text_cleaned'].apply(detect_language_robust)\n",
    "    df_clean['language'] = [result[0] for result in language_results]\n",
    "    df_clean['language_confidence'] = [result[1] for result in language_results]\n",
    "    \n",
    "    print(f\"    D√©tection de langue termin√©e\")\n",
    "else:\n",
    "    print(\"     Langues d√©j√† d√©tect√©es, validation des donn√©es...\")\n",
    "    if 'language_confidence' not in df_clean.columns:\n",
    "        df_clean['language_confidence'] = df_clean['language'].apply(lambda x: 0.9 if x == 'fr' else 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaad1e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DISTRIBUTION DES LANGUES:\n",
      "   fr: 130 articles (65.0%)\n",
      "   en: 70 articles (35.0%)\n"
     ]
    }
   ],
   "source": [
    "# Analyse des langues d√©tect√©es\n",
    "lang_counts = df_clean['language'].value_counts()\n",
    "print(f\"\\n DISTRIBUTION DES LANGUES:\")\n",
    "for lang, count in lang_counts.head(5).items():\n",
    "    pct = count / len(df_clean) * 100\n",
    "    print(f\"   {lang}: {count} articles ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e62f0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üá´üá∑ Focus fran√ßais: 130 articles s√©lectionn√©s\n"
     ]
    }
   ],
   "source": [
    "# S√©lection intelligente selon la distribution\n",
    "if lang_counts.get('fr', 0) > len(df_clean) * 0.3:  # Si >30% en fran√ßais\n",
    "    df_filtered = df_clean[df_clean['language'] == 'fr'].copy()\n",
    "    print(f\"   üá´üá∑ Focus fran√ßais: {len(df_filtered)} articles s√©lectionn√©s\")\n",
    "else:\n",
    "    # Garder top 2 langues si pas assez de fran√ßais\n",
    "    top_langs = lang_counts.head(2).index.tolist()\n",
    "    df_filtered = df_clean[df_clean['language'].isin(top_langs)].copy()\n",
    "    print(f\"    Multi-langues: {len(df_filtered)} articles ({top_langs})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19aa9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Chargement du mod√®le sentence-transformers...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Le fichier de pagination est insuffisant pour terminer cette op√©ration. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Chargement du mod√®le d'embeddings\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m    Chargement du mod√®le sentence-transformers...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m embeddings_model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDINGS_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeduplicate_semantic\u001b[39m(df, threshold=\u001b[32m0.85\u001b[39m):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"D√©duplication s√©mantique avanc√©e avec FAISS\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2254\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2249\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2253\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2271\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:541\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m     **kwargs,\n\u001b[32m    527\u001b[39m ) -> Self:\n\u001b[32m    528\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    529\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    530\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m         backend=backend,\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:88\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m     87\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     91\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:186\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    184\u001b[39m         \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_onnx_model(model_name_or_path, config, cache_dir, **model_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5105\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5102\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5104\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5105\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5106\u001b[39m     )\n\u001b[32m   5108\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5109\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:532\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Use safetensors if possible\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    533\u001b[39m         metadata = f.metadata()\n\u001b[32m    535\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata.get(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmlx\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mOSError\u001b[39m: Le fichier de pagination est insuffisant pour terminer cette op√©ration. (os error 1455)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chargement du mod√®le d'embeddings\n",
    "print(\"    Chargement du mod√®le sentence-transformers...\")\n",
    "embeddings_model = SentenceTransformer(EMBEDDINGS_MODEL)\n",
    "\n",
    "def deduplicate_semantic(df, threshold=0.85):\n",
    "    \"\"\"D√©duplication s√©mantique avanc√©e avec FAISS\"\"\"\n",
    "    \n",
    "    print(f\"    G√©n√©ration des embeddings pour {len(df)} articles...\")\n",
    "    \n",
    "    # Utilisation des embeddings existants ou g√©n√©ration\n",
    "    if 'embedding' in df.columns and df['embedding'].notna().sum() > 0:\n",
    "        print(\"       Utilisation des embeddings existants\")\n",
    "        embeddings = []\n",
    "        for idx, emb in df['embedding'].items():\n",
    "            if isinstance(emb, (list, np.ndarray)) and len(emb) > 0:\n",
    "                embeddings.append(np.array(emb))\n",
    "            else:\n",
    "                # G√©n√©ration pour les embeddings manquants\n",
    "                text = df.loc[idx, 'text_cleaned']\n",
    "                embeddings.append(embeddings_model.encode(text))\n",
    "        embeddings = np.array(embeddings)\n",
    "    else:\n",
    "        print(\"       G√©n√©ration des embeddings...\")\n",
    "        texts = df['text_cleaned'].tolist()\n",
    "        embeddings = embeddings_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    # Configuration FAISS\n",
    "    print(\"     Configuration de l'index FAISS...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine apr√®s normalisation)\n",
    "    \n",
    "    # Normalisation pour cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    # Recherche des doublons\n",
    "    print(\"    Recherche des doublons s√©mantiques...\")\n",
    "    similarities, indices = index.search(embeddings.astype('float32'), k=5)  # Top 5 similaires\n",
    "    \n",
    "    to_remove = set()\n",
    "    duplicate_pairs = []\n",
    "    \n",
    "    for i, (sim_scores, sim_indices) in enumerate(zip(similarities, indices)):\n",
    "        for j, (score, idx) in enumerate(zip(sim_scores, sim_indices)):\n",
    "            if j > 0 and score > threshold and idx not in to_remove and i not in to_remove:\n",
    "                # Garde le plus r√©cent ou le mieux not√©\n",
    "                if df.iloc[i].get('quality_score', 0) >= df.iloc[idx].get('quality_score', 0):\n",
    "                    to_remove.add(idx)\n",
    "                else:\n",
    "                    to_remove.add(i)\n",
    "                \n",
    "                duplicate_pairs.append((i, idx, score))\n",
    "    \n",
    "    # Suppression des doublons\n",
    "    df_dedup = df.drop(df.index[list(to_remove)]).copy()\n",
    "    \n",
    "    print(f\"    R√©sultats d√©duplication:\")\n",
    "    print(f\"      Articles originaux: {len(df)}\")\n",
    "    print(f\"      Doublons d√©tect√©s: {len(to_remove)}\")\n",
    "    print(f\"      Articles finaux: {len(df_dedup)}\")\n",
    "    print(f\"      Taux de d√©duplication: {len(to_remove)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    # Exemples de doublons d√©tect√©s\n",
    "    if duplicate_pairs:\n",
    "        print(f\"    Exemples de doublons d√©tect√©s:\")\n",
    "        for i, (idx1, idx2, sim) in enumerate(duplicate_pairs[:3]):\n",
    "            title1 = df.iloc[idx1]['title'][:50]\n",
    "            title2 = df.iloc[idx2]['title'][:50]\n",
    "            print(f\"      {i+1}. Similarit√© {sim:.3f}:\")\n",
    "            print(f\"         A: {title1}...\")\n",
    "            print(f\"         B: {title2}...\")\n",
    "    \n",
    "    return df_dedup, embeddings\n",
    "\n",
    "# Application de la d√©duplication\n",
    "df_clean_dedup, article_embeddings = deduplicate_semantic(df_filtered, SIMILARITY_THRESHOLD)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n √âTAPE 6: Gestion avanc√©e des entit√©s nomm√©es\")\n",
    "\n",
    "if not enrichment_columns['entities'] or not SKIP_BASIC_ENRICHMENT:\n",
    "    print(\"    Extraction compl√®te des entit√©s avec spaCy...\")\n",
    "    \n",
    "    # Chargement du mod√®le spaCy fran√ßais\n",
    "    print(f\"       Chargement du mod√®le spaCy: {NLP_MODEL}\")\n",
    "    try:\n",
    "        nlp = spacy.load(NLP_MODEL)\n",
    "    except OSError:\n",
    "        print(f\"       Mod√®le {NLP_MODEL} non trouv√©. Installation...\")\n",
    "        import subprocess\n",
    "        subprocess.run(f\"python -m spacy download {NLP_MODEL}\", shell=True)\n",
    "        nlp = spacy.load(NLP_MODEL)\n",
    "\n",
    "    def extract_entities_advanced(text, nlp_model):\n",
    "        \"\"\"Extraction d'entit√©s avec enrichissements\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return {\n",
    "                'persons': [], 'organizations': [], 'locations': [],\n",
    "                'dates': [], 'money': [], 'misc': []\n",
    "            }\n",
    "        \n",
    "        # Traitement avec spaCy (limiter la longueur pour performance)\n",
    "        doc = nlp_model(text[:8000])  # Premier 8k caract√®res\n",
    "        \n",
    "        entities = {\n",
    "            'persons': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "            'dates': [],\n",
    "            'money': [],\n",
    "            'misc': []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_text = ent.text.strip()\n",
    "            if len(entity_text) < 2:  \n",
    "                continue\n",
    "                \n",
    "            if ent.label_ in ['PERSON']:\n",
    "                entities['persons'].append(entity_text)\n",
    "            elif ent.label_ in ['ORG']:\n",
    "                entities['organizations'].append(entity_text)\n",
    "            elif ent.label_ in ['GPE', 'LOC']:\n",
    "                entities['locations'].append(entity_text)\n",
    "            elif ent.label_ in ['DATE', 'TIME']:\n",
    "                entities['dates'].append(entity_text)\n",
    "            elif ent.label_ in ['MONEY']:\n",
    "                entities['money'].append(entity_text)\n",
    "            else:\n",
    "                entities['misc'].append(entity_text)\n",
    "        \n",
    "        # D√©duplication et nettoyage\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))  # Suppression doublons\n",
    "            entities[key] = [e for e in entities[key] if len(e) > 1]  # Filtrage longueur\n",
    "        \n",
    "        return entities\n",
    "\n",
    "    # Application de l'extraction d'entit√©s\n",
    "    entities_results = []\n",
    "    for text in tqdm(df_clean_dedup['text_cleaned'], desc=\"Extraction NER\"):\n",
    "        entities = extract_entities_advanced(text, nlp)\n",
    "        entities_results.append(entities)\n",
    "\n",
    "    # Ajout des r√©sultats au DataFrame\n",
    "    df_clean_dedup['entities_advanced'] = entities_results\n",
    "    \n",
    "    print(f\"       Extraction NER termin√©e\")\n",
    "    \n",
    "else:\n",
    "    print(\"    Am√©lioration des entit√©s existantes...\")\n",
    "    \n",
    "    def improve_entities(existing_entities):\n",
    "        \"\"\"Am√©lioration et nettoyage des entit√©s existantes\"\"\"\n",
    "        if not existing_entities or not isinstance(existing_entities, dict):\n",
    "            return {\n",
    "                'persons': [], 'organizations': [], 'locations': [],\n",
    "                'dates': [], 'money': [], 'misc': []\n",
    "            }\n",
    "        \n",
    "        improved = {\n",
    "            'persons': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "            'dates': [],\n",
    "            'money': [],\n",
    "            'misc': []\n",
    "        }\n",
    "        \n",
    "        # Nettoyage et d√©duplication\n",
    "        for key in improved.keys():\n",
    "            if key in existing_entities and isinstance(existing_entities[key], list):\n",
    "                # Nettoyage des entit√©s\n",
    "                cleaned = [str(e).strip() for e in existing_entities[key] if e and len(str(e)) > 1]\n",
    "                # D√©duplication case-insensitive\n",
    "                seen = set()\n",
    "                for entity in cleaned:\n",
    "                    if entity.lower() not in seen:\n",
    "                        improved[key].append(entity)\n",
    "                        seen.add(entity.lower())\n",
    "        \n",
    "        return improved\n",
    "    \n",
    "    # Application de l'am√©lioration\n",
    "    df_clean_dedup['entities_advanced'] = df_clean_dedup['entities'].apply(improve_entities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366da564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de colonnes m√©triques enrichies\n",
    "df_clean_dedup['persons_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('persons', [])))\n",
    "df_clean_dedup['organizations_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('organizations', [])))\n",
    "df_clean_dedup['locations_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('locations', [])))\n",
    "df_clean_dedup['entities_total'] = (df_clean_dedup['persons_count'] + \n",
    "                                   df_clean_dedup['organizations_count'] + \n",
    "                                   df_clean_dedup['locations_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n    STATISTIQUES ENTIT√âS AVANC√âES:\")\n",
    "print(f\"      Moyenne personnes/article: {df_clean_dedup['persons_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne organisations/article: {df_clean_dedup['organizations_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne lieux/article: {df_clean_dedup['locations_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne total entit√©s/article: {df_clean_dedup['entities_total'].mean():.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top entit√©s par cat√©gorie\n",
    "print(f\"\\n    TOP ENTIT√âS D√âTECT√âES:\")\n",
    "all_persons = [p for entities in df_clean_dedup['entities_advanced'] for p in entities.get('persons', [])]\n",
    "all_orgs = [o for entities in df_clean_dedup['entities_advanced'] for o in entities.get('organizations', [])]\n",
    "all_locs = [l for entities in df_clean_dedup['entities_advanced'] for l in entities.get('locations', [])]\n",
    "\n",
    "if all_persons:\n",
    "    top_persons = Counter(all_persons).most_common(3)\n",
    "    print(f\"      Personnes: {', '.join([f'{p} ({c})' for p, c in top_persons])}\")\n",
    "\n",
    "if all_orgs:\n",
    "    top_orgs = Counter(all_orgs).most_common(3)\n",
    "    print(f\"      Organisations: {', '.join([f'{o} ({c})' for o, c in top_orgs])}\")\n",
    "\n",
    "if all_locs:\n",
    "    top_locs = Counter(all_locs).most_common(3)\n",
    "    print(f\"      Lieux: {', '.join([f'{l} ({c})' for l, c in top_locs])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138328df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n √âTAPE 7: Segmentation s√©mantique avanc√©e\")\n",
    "\n",
    "# T√©l√©chargement des ressources NLTK si n√©cessaire\n",
    "try:\n",
    "    import ssl\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "except:\n",
    "    pass\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def segment_text_advanced(text):\n",
    "    \"\"\"Segmentation en phrases avec analyse s√©mantique avanc√©e\"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return {\n",
    "            'sentences': [],\n",
    "            'sentence_count': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "            'paragraphs': [],\n",
    "            'paragraph_count': 0,\n",
    "            'text_complexity': 0,\n",
    "            'readability_score': 0\n",
    "        }\n",
    "    \n",
    "    # Segmentation en phrases (multi-langue)\n",
    "    sentences = sent_tokenize(text[:5000], language='french')  # Limiter pour performance\n",
    "    \n",
    "    # Segmentation en paragraphes\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip() and len(p) > 20]\n",
    "    \n",
    "    # M√©triques avanc√©es\n",
    "    sentence_lengths = [len(s.split()) for s in sentences]\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Score de complexit√© bas√© sur la longueur des phrases\n",
    "    complexity = 0\n",
    "    if sentence_lengths:\n",
    "        variance = np.var(sentence_lengths)\n",
    "        long_sentences = sum(1 for length in sentence_lengths if length > 20)\n",
    "        complexity = min((variance / 100) + (long_sentences / len(sentences)), 1.0)\n",
    "    \n",
    "    # Score de lisibilit√© approximatif (Flesch-like)\n",
    "    if sentence_lengths and avg_sentence_length > 0:\n",
    "        readability = max(0, min(1, 1 - (avg_sentence_length - 10) / 20))\n",
    "    else:\n",
    "        readability = 0.5\n",
    "    \n",
    "    return {\n",
    "        'sentences': sentences[:50],  # Limiter pour stockage\n",
    "        'sentence_count': len(sentences),\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'paragraphs': paragraphs[:20],  # Limiter pour stockage\n",
    "        'paragraph_count': len(paragraphs),\n",
    "        'text_complexity': complexity,\n",
    "        'readability_score': readability\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la segmentation\n",
    "print(\"    Segmentation avanc√©e en cours...\")\n",
    "segmentation_results = []\n",
    "\n",
    "for text in tqdm(df_clean_dedup['text_cleaned'], desc=\"Segmentation\"):\n",
    "    segments = segment_text_advanced(text)\n",
    "    segmentation_results.append(segments)\n",
    "\n",
    "df_clean_dedup['segmentation'] = segmentation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f579926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des m√©triques de segmentation\n",
    "df_clean_dedup['sentence_count'] = df_clean_dedup['segmentation'].apply(lambda x: x['sentence_count'])\n",
    "df_clean_dedup['paragraph_count'] = df_clean_dedup['segmentation'].apply(lambda x: x['paragraph_count'])\n",
    "df_clean_dedup['avg_sentence_length'] = df_clean_dedup['segmentation'].apply(lambda x: x['avg_sentence_length'])\n",
    "df_clean_dedup['text_complexity'] = df_clean_dedup['segmentation'].apply(lambda x: x['text_complexity'])\n",
    "df_clean_dedup['readability_score'] = df_clean_dedup['segmentation'].apply(lambda x: x['readability_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n    STATISTIQUES SEGMENTATION AVANC√âES:\")\n",
    "print(f\"      Moyenne phrases/article: {df_clean_dedup['sentence_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne paragraphes/article: {df_clean_dedup['paragraph_count'].mean():.1f}\")\n",
    "print(f\"      Longueur moyenne phrases: {df_clean_dedup['avg_sentence_length'].mean():.1f} mots\")\n",
    "print(f\"      Score complexit√© moyen: {df_clean_dedup['text_complexity'].mean():.2f}\")\n",
    "print(f\"      Score lisibilit√© moyen: {df_clean_dedup['readability_score'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n √âTAPE 8: Analyse des biais temporels et g√©ographiques\")\n",
    "\n",
    "# Conversion et nettoyage des dates\n",
    "print(\"    Analyse temporelle...\")\n",
    "df_clean_dedup['published_clean'] = pd.to_datetime(df_clean_dedup['published'], errors='coerce')\n",
    "\n",
    "# Extraction des composants temporels\n",
    "df_clean_dedup['hour'] = df_clean_dedup['published_clean'].dt.hour\n",
    "df_clean_dedup['day_of_week'] = df_clean_dedup['published_clean'].dt.day_name()\n",
    "df_clean_dedup['month'] = df_clean_dedup['published_clean'].dt.month\n",
    "df_clean_dedup['date_only'] = df_clean_dedup['published_clean'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des biais temporels\n",
    "valid_dates = df_clean_dedup[df_clean_dedup['published_clean'].notna()]\n",
    "\n",
    "if len(valid_dates) > 0:\n",
    "    print(f\"       Analyse sur {len(valid_dates)} articles avec dates valides\")\n",
    "    \n",
    "    # Distribution horaire\n",
    "    hour_dist = valid_dates['hour'].value_counts().head(3)\n",
    "    print(f\"       Heures de publication principales:\")\n",
    "    for hour, count in hour_dist.items():\n",
    "        pct = count / len(valid_dates) * 100\n",
    "        print(f\"         {hour}h: {count} articles ({pct:.1f}%)\")\n",
    "    \n",
    "    # Distribution par jour\n",
    "    day_dist = valid_dates['day_of_week'].value_counts().head(3)\n",
    "    print(f\"       Jours de publication principaux:\")\n",
    "    for day, count in day_dist.items():\n",
    "        pct = count / len(valid_dates) * 100\n",
    "        print(f\"         {day}: {count} articles ({pct:.1f}%)\")\n",
    "    \n",
    "    # Calcul du score de biais temporel\n",
    "    hour_entropy = -sum((p := hour_dist / len(valid_dates)) * np.log2(p + 1e-10))\n",
    "    day_entropy = -sum((p := day_dist / len(valid_dates)) * np.log2(p + 1e-10))\n",
    "    \n",
    "    # Normalisation (entropie max = log2(24) pour heures, log2(7) pour jours)\n",
    "    hour_bias = 1 - (hour_entropy / np.log2(24))  # 0 = uniforme, 1 = tr√®s biais√©\n",
    "    day_bias = 1 - (day_entropy / np.log2(7))\n",
    "    \n",
    "    print(f\"       Scores de biais temporel:\")\n",
    "    print(f\"         Biais horaire: {hour_bias:.2f} (0=uniforme, 1=concentr√©)\")\n",
    "    print(f\"         Biais quotidien: {day_bias:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse g√©ographique via entit√©s lieux\n",
    "print(\"\\n    Analyse g√©ographique...\")\n",
    "all_locations = []\n",
    "location_counts_by_article = []\n",
    "\n",
    "for entities in df_clean_dedup['entities_advanced']:\n",
    "    article_locations = entities.get('locations', [])\n",
    "    location_counts_by_article.append(len(article_locations))\n",
    "    all_locations.extend(article_locations)\n",
    "\n",
    "location_distribution = Counter(all_locations)\n",
    "df_clean_dedup['locations_count'] = location_counts_by_article\n",
    "\n",
    "if location_distribution:\n",
    "    print(f\"       Lieux les plus mentionn√©s:\")\n",
    "    for location, count in location_distribution.most_common(5):\n",
    "        pct = count / len(all_locations) * 100 if len(all_locations) > 0 else 0\n",
    "        print(f\"         {location}: {count} mentions ({pct:.1f}%)\")\n",
    "    \n",
    "    # Score de biais g√©ographique\n",
    "    if len(location_distribution) > 1:\n",
    "        geo_probs = np.array(list(location_distribution.values())) / len(all_locations)\n",
    "        geo_entropy = -sum(geo_probs * np.log2(geo_probs + 1e-10))\n",
    "        max_entropy = np.log2(min(len(location_distribution), 50))  # Entropie max th√©orique\n",
    "        geo_bias = 1 - (geo_entropy / max_entropy) if max_entropy > 0 else 0\n",
    "        \n",
    "        print(f\"       Score de biais g√©ographique: {geo_bias:.2f}\")\n",
    "        print(f\"         (0=distribution √©quilibr√©e, 1=concentration forte)\")\n",
    "    else:\n",
    "        geo_bias = 1.0  # Maximum bias if only one location\n",
    "        print(f\"        Biais g√©ographique maximal d√©tect√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des scores de biais au DataFrame\n",
    "df_clean_dedup['temporal_bias_hour'] = hour_bias if 'hour_bias' in locals() else 0\n",
    "df_clean_dedup['temporal_bias_day'] = day_bias if 'day_bias' in locals() else 0\n",
    "df_clean_dedup['geographic_bias'] = geo_bias if 'geo_bias' in locals() else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_quality_score(row):\n",
    "    \"\"\"Calcul d'un score de qualit√© multi-dimensionnel\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # 1. Score de longueur (0-1)\n",
    "    text_len = len(str(row.get('text_cleaned', '')))\n",
    "    scores['length'] = min(text_len / 2000, 1.0)  # Optimal √† 2000 caract√®res\n",
    "    \n",
    "    # 2. Score d'entit√©s (0-1)\n",
    "    entities_count = row.get('entities_total', 0)\n",
    "    scores['entities'] = min(entities_count / 10, 1.0)  # Optimal √† 10 entit√©s\n",
    "    \n",
    "    # 3. Score de lisibilit√© (0-1)\n",
    "    scores['readability'] = row.get('readability_score', 0.5)\n",
    "    \n",
    "    # 4. Score de complexit√© invers√© (0-1)\n",
    "    complexity = row.get('text_complexity', 0.5)\n",
    "    scores['complexity'] = 1 - complexity  # Moins complexe = meilleur\n",
    "    \n",
    "    # 5. Score de structure (0-1)\n",
    "    sentence_count = row.get('sentence_count', 0)\n",
    "    paragraph_count = row.get('paragraph_count', 0)\n",
    "    if sentence_count > 0 and paragraph_count > 0:\n",
    "        structure_ratio = min(sentence_count / paragraph_count, 10) / 10  # Ratio phrases/paragraphes\n",
    "        scores['structure'] = structure_ratio\n",
    "    else:\n",
    "        scores['structure'] = 0.1\n",
    "    \n",
    "    # 6. Score de langue (0-1)\n",
    "    lang_confidence = row.get('language_confidence', 0.5)\n",
    "    scores['language'] = lang_confidence\n",
    "    \n",
    "    # Score global pond√©r√©\n",
    "    weights = {\n",
    "        'length': 0.2,\n",
    "        'entities': 0.25, \n",
    "        'readability': 0.2,\n",
    "        'complexity': 0.15,\n",
    "        'structure': 0.1,\n",
    "        'language': 0.1\n",
    "    }\n",
    "    \n",
    "    final_score = sum(scores[key] * weights[key] for key in scores)\n",
    "    \n",
    "    return {\n",
    "        'quality_score_advanced': final_score,\n",
    "        'quality_breakdown': scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0427f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du calcul de qualit√©\n",
    "print(\"    Calcul des scores de qualit√©...\")\n",
    "quality_results = []\n",
    "\n",
    "for _, row in tqdm(df_clean_dedup.iterrows(), total=len(df_clean_dedup), desc=\"Qualit√©\"):\n",
    "    quality_result = calculate_advanced_quality_score(row)\n",
    "    quality_results.append(quality_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des r√©sultats\n",
    "df_clean_dedup['quality_score_advanced'] = [r['quality_score_advanced'] for r in quality_results]\n",
    "df_clean_dedup['quality_breakdown'] = [r['quality_breakdown'] for r in quality_results]\n",
    "\n",
    "print(f\"\\n    STATISTIQUES QUALIT√â AVANC√âES:\")\n",
    "print(f\"      Score moyen: {df_clean_dedup['quality_score_advanced'].mean():.3f}\")\n",
    "print(f\"      Score m√©dian: {df_clean_dedup['quality_score_advanced'].median():.3f}\")\n",
    "print(f\"      Score min: {df_clean_dedup['quality_score_advanced'].min():.3f}\")\n",
    "print(f\"      Score max: {df_clean_dedup['quality_score_advanced'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par quartiles\n",
    "quartiles = df_clean_dedup['quality_score_advanced'].quantile([0.25, 0.5, 0.75])\n",
    "print(f\"      Quartiles: Q1={quartiles[0.25]:.3f}, Q2={quartiles[0.5]:.3f}, Q3={quartiles[0.75]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_stratified_calibration_corpus(df, n_samples=300):\n",
    "    \"\"\"Cr√©ation d'un corpus stratifi√© pour calibration\"\"\"\n",
    "    \n",
    "    # D√©finition des strates multi-dimensionnelles\n",
    "    print(\"   D√©finition des strates...\")\n",
    "    \n",
    "    # 1. Strate par qualit√© (3 niveaux)\n",
    "    quality_tertiles = df['quality_score_advanced'].quantile([0.33, 0.67])\n",
    "    df['quality_stratum'] = pd.cut(df['quality_score_advanced'], \n",
    "                                  bins=[0, quality_tertiles[0.33], quality_tertiles[0.67], 1],\n",
    "                                  labels=['low', 'medium', 'high'])\n",
    "    \n",
    "    # 2. Strate par longueur (3 niveaux)\n",
    "    df['text_length'] = df['text_cleaned'].str.len()\n",
    "    length_tertiles = df['text_length'].quantile([0.33, 0.67])\n",
    "    df['length_stratum'] = pd.cut(df['text_length'],\n",
    "                                 bins=[0, length_tertiles[0.33], length_tertiles[0.67], float('inf')],\n",
    "                                 labels=['short', 'medium', 'long'])\n",
    "    \n",
    "    # 3. Strate par richesse en entit√©s (3 niveaux)\n",
    "    if df['entities_total'].max() > 0:\n",
    "        entity_tertiles = df['entities_total'].quantile([0.33, 0.67])\n",
    "        df['entity_stratum'] = pd.cut(df['entities_total'],\n",
    "                                     bins=[-1, entity_tertiles[0.33], entity_tertiles[0.67], float('inf')],\n",
    "                                     labels=['sparse', 'moderate', 'rich'])\n",
    "    else:\n",
    "        df['entity_stratum'] = 'sparse'\n",
    "    \n",
    "    # 4. Strate par source (top sources + autres)\n",
    "    source_counts = df['source'].value_counts()\n",
    "    top_sources = source_counts.head(5).index.tolist()\n",
    "    df['source_stratum'] = df['source'].apply(lambda x: x if x in top_sources else 'other')\n",
    "    \n",
    "    print(f\"      Strates cr√©√©es:\")\n",
    "    print(f\"         Qualit√©: {df['quality_stratum'].value_counts().to_dict()}\")\n",
    "    print(f\"         Longueur: {df['length_stratum'].value_counts().to_dict()}\")\n",
    "    print(f\"         Entit√©s: {df['entity_stratum'].value_counts().to_dict()}\")\n",
    "    print(f\"         Sources: {len(df['source_stratum'].unique())} cat√©gories\")\n",
    "    \n",
    "    # √âchantillonnage stratifi√© proportionnel\n",
    "    print(\"   √âchantillonnage stratifi√©...\")\n",
    "    \n",
    "    # Groupement par strates multiples\n",
    "    strata_cols = ['quality_stratum', 'length_stratum', 'entity_stratum', 'source_stratum']\n",
    "    grouped = df.groupby(strata_cols, group_keys=False)\n",
    "    \n",
    "    # Calcul des tailles d'√©chantillon par strate\n",
    "    strata_sizes = grouped.size()\n",
    "    total_size = len(df)\n",
    "    \n",
    "    sample_dfs = []\n",
    "    remaining_samples = n_samples\n",
    "    \n",
    "    for stratum, group in grouped:\n",
    "        if remaining_samples <= 0:\n",
    "            break\n",
    "            \n",
    "        # Taille proportionnelle de l'√©chantillon pour cette strate\n",
    "        stratum_size = len(group)\n",
    "        proportion = stratum_size / total_size\n",
    "        target_sample_size = max(1, int(proportion * n_samples))\n",
    "        \n",
    "        # Ajustement si on d√©passe le nombre d'√©chantillons restants\n",
    "        actual_sample_size = min(target_sample_size, remaining_samples, stratum_size)\n",
    "        \n",
    "        if actual_sample_size > 0:\n",
    "            # √âchantillonnage au sein de la strate\n",
    "            if len(group) >= actual_sample_size:\n",
    "                # Tri par score de qualit√© pour prendre les meilleurs\n",
    "                group_sorted = group.sort_values('quality_score_advanced', ascending=False)\n",
    "                stratum_sample = group_sorted.head(actual_sample_size)\n",
    "                sample_dfs.append(stratum_sample)\n",
    "                remaining_samples -= actual_sample_size\n",
    "    \n",
    "    # Combinaison des √©chantillons de toutes les strates\n",
    "    if sample_dfs:\n",
    "        calibration_corpus = pd.concat(sample_dfs, ignore_index=True)\n",
    "    else:\n",
    "        # Fallback: √©chantillonnage simple par qualit√©\n",
    "        calibration_corpus = df.nlargest(n_samples, 'quality_score_advanced')\n",
    "    \n",
    "    # Compl√©ment al√©atoire si n√©cessaire\n",
    "    if len(calibration_corpus) < n_samples:\n",
    "        remaining_df = df[~df.index.isin(calibration_corpus.index)]\n",
    "        if len(remaining_df) > 0:\n",
    "            additional_samples = min(n_samples - len(calibration_corpus), len(remaining_df))\n",
    "            additional = remaining_df.sample(n=additional_samples, random_state=42)\n",
    "            calibration_corpus = pd.concat([calibration_corpus, additional], ignore_index=True)\n",
    "    \n",
    "    return calibration_corpus.head(n_samples)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du corpus de calibration\n",
    "calibration_corpus = create_stratified_calibration_corpus(df_clean_dedup, n_samples=300)\n",
    "\n",
    "print(f\"\\n    CORPUS DE CALIBRATION CR√â√â:\")\n",
    "print(f\"      Taille finale: {len(calibration_corpus)} articles\")\n",
    "print(f\"      Score qualit√© moyen: {calibration_corpus['quality_score_advanced'].mean():.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effda8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la repr√©sentativit√©\n",
    "print(f\"\\n    REPR√âSENTATIVIT√â DU CORPUS:\")\n",
    "print(f\"      Sources: {calibration_corpus['source'].nunique()} uniques\")\n",
    "print(f\"      Langues: {calibration_corpus['language'].value_counts().to_dict()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top sources dans le corpus\n",
    "source_dist = calibration_corpus['source'].value_counts().head(5)\n",
    "print(f\"      Top sources:\")\n",
    "for source, count in source_dist.items():\n",
    "    pct = count / len(calibration_corpus) * 100\n",
    "    source_name = source.split('/')[-1] if '/' in source else source\n",
    "    print(f\"         {source_name}: {count} articles ({pct:.1f}%)\")\n",
    "\n",
    "# Distribution qualit√©\n",
    "quality_dist = calibration_corpus['quality_stratum'].value_counts()\n",
    "print(f\"      Distribution qualit√©: {quality_dist.to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d23313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©triques de qualit√© finales\n",
    "quality_metrics = {\n",
    "    'source_file': str(source_file.name),\n",
    "    'enriched_mode': ENRICHED_MODE,\n",
    "    'total_articles_input': len(articles_data),\n",
    "    'articles_after_deduplication': len(df_clean_dedup),\n",
    "    'calibration_corpus_size': len(calibration_corpus),\n",
    "    'deduplication_rate': ((len(df) - len(df_clean_dedup)) / len(df)) if len(df) > 0 else 0,\n",
    "    'avg_quality_score': df_clean_dedup['quality_score_advanced'].mean(),\n",
    "    'language_distribution': df_clean_dedup['language'].value_counts().to_dict(),\n",
    "    'entities_avg_per_article': df_clean_dedup['entities_total'].mean(),\n",
    "    'temporal_bias_detected': df_clean_dedup['temporal_bias_hour'].iloc[0] if len(df_clean_dedup) > 0 else 0,\n",
    "    'geographic_bias_detected': df_clean_dedup['geographic_bias'].iloc[0] if len(df_clean_dedup) > 0 else 0,\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du DataFrame principal (format optimis√©)\n",
    "output_file = PROCESSED_DIR / \"articles_preprocessed_advanced.pkl\"\n",
    "df_clean_dedup.to_pickle(output_file)\n",
    "print(f\"    Dataset principal sauvegard√©: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du corpus de calibration\n",
    "calibration_file = PROCESSED_DIR / \"calibration_corpus_stratified.pkl\"\n",
    "calibration_corpus.to_pickle(calibration_file)\n",
    "print(f\"    Corpus de calibration sauvegard√©: {calibration_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa272eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des m√©triques\n",
    "metrics_file = PROCESSED_DIR / \"advanced_preprocessing_metrics.json\"\n",
    "with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(quality_metrics, f, indent=2, ensure_ascii=False, default=str)\n",
    "print(f\"    M√©triques sauvegard√©es: {metrics_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV l√©ger pour analyse externe\n",
    "csv_file = PROCESSED_DIR / \"articles_preprocessed_summary.csv\"\n",
    "df_export = df_clean_dedup[[\n",
    "    'title', 'source', 'published', 'language', 'quality_score_advanced',\n",
    "    'entities_total', 'sentence_count', 'readability_score', 'text_complexity'\n",
    "]].copy()\n",
    "df_export.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "print(f\"    Export CSV r√©sum√©: {csv_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export JSON du corpus de calibration \n",
    "calibration_json = PROCESSED_DIR / \"calibration_corpus_300.json\"\n",
    "calibration_export = calibration_corpus[[\n",
    "    'id', 'title', 'text_cleaned', 'source', 'published', 'language',\n",
    "    'quality_score_advanced', 'entities_advanced'\n",
    "]].to_dict('records')\n",
    "\n",
    "with open(calibration_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(calibration_export, f, ensure_ascii=False, indent=2, default=str)\n",
    "print(f\"    Corpus calibration JSON: {calibration_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des performances\n",
    "processing_time = (datetime.now() - datetime.fromisoformat(quality_metrics['processing_timestamp'])).total_seconds()\n",
    "\n",
    "print(f\" R√âSULTATS FINAUX:\")\n",
    "print(f\"   Source: {quality_metrics['source_file']}\")\n",
    "print(f\"   Mode: {'Enrichissement compl√©mentaire' if ENRICHED_MODE else 'Pipeline complet'}\")\n",
    "print(f\"   Articles trait√©s: {quality_metrics['total_articles_input']}\")\n",
    "print(f\"   Articles finaux: {quality_metrics['articles_after_deduplication']}\")\n",
    "print(f\"   Corpus de calibration: {quality_metrics['calibration_corpus_size']}\")\n",
    "print(f\"   Taux de d√©duplication: {quality_metrics['deduplication_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n M√âTRIQUES DE QUALIT√â:\")\n",
    "print(f\"   Score qualit√© moyen: {quality_metrics['avg_quality_score']:.3f}\")\n",
    "print(f\"   Entit√©s par article: {quality_metrics['entities_avg_per_article']:.1f}\")\n",
    "print(f\"   Biais temporel d√©tect√©: {quality_metrics['temporal_bias_detected']:.2f}\")\n",
    "print(f\"   Biais g√©ographique: {quality_metrics['geographic_bias_detected']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n DISTRIBUTION LINGUISTIQUE:\")\n",
    "for lang, count in quality_metrics['language_distribution'].items():\n",
    "    pct = count / quality_metrics['articles_after_deduplication'] * 100\n",
    "    print(f\"   {lang}: {count} articles ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c386cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFICHIERS G√âN√âR√âS:\")\n",
    "print(f\"   1. {output_file.name} - Dataset principal avec preprocessing avanc√©\")\n",
    "print(f\"   2. {calibration_file.name} - Corpus stratifi√© pour calibration\")\n",
    "print(f\"   3. {metrics_file.name} - M√©triques d√©taill√©es\")\n",
    "print(f\"   4. {csv_file.name} - Export CSV pour analyse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nINSTRUCTIONS D'INSTALLATION SI ERREURS:\")\n",
    "print(f\"   Si erreur spaCy: !python -m spacy download fr_core_news_lg\")\n",
    "print(f\"   Si erreur FAISS: !pip install faiss-cpu\")\n",
    "print(f\"   Si erreur sentence-transformers: !pip install sentence-transformers\")\n",
    "print(f\"   Si erreur langdetect: !pip install langdetect\")\n",
    "print(f\"   Si erreur ftfy: !pip install ftfy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
