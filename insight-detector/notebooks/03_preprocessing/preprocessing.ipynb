{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "839b63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Auto-reload pour développement interactif\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP avancé\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Détection de langue\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Preprocessing texte\n",
    "import unicodedata\n",
    "import ftfy  # Pour corriger les encodages\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Similarité et déduplication\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86cbfdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire de données: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\n",
      "Répertoire de sortie: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path().resolve().parent.parent\n",
    "sys.path.append(str(BASE_DIR / \"src\"))\n",
    "\n",
    "# Répertoires\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "EXPORTS_DIR = DATA_DIR / \"exports\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration des modèles\n",
    "#NLP_MODEL = \"fr_core_news_lg\"  # Modèle spaCy français\n",
    "NLP_MODEL = \"fr_core_news_sm\"  # Modèle spaCy français léger pour éviter les problèmes de mémoire\n",
    "EMBEDDINGS_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "SIMILARITY_THRESHOLD = 0.85  # Seuil de similarité pour déduplication\n",
    "\n",
    "print(f\"Répertoire de données: {DATA_DIR}\")\n",
    "print(f\"Répertoire de sortie: {PROCESSED_DIR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2ad59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier JSON enrichi (priorité) ou brut (fallback)\n",
    "enriched_file = EXPORTS_DIR / \"enriched_article.json\"  # MODIFIÉ: sans \"s\"\n",
    "enriched_files_alt = EXPORTS_DIR / \"enriched_articles.json\"  # Alternative\n",
    "raw_file = EXPORTS_DIR / \"raw_articles.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f852fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FICHIER ENRICHI ALTERNATIF DÉTECTÉ: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\exports\\enriched_articles.json\n",
      "   Mode: Preprocessing avancé sur données pré-enrichies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Détection automatique du fichier source - MODIFIÉE\n",
    "if enriched_file.exists():\n",
    "    source_file = enriched_file\n",
    "    print(f\" FICHIER ENRICHI DÉTECTÉ: {enriched_file}\")\n",
    "    print(\"   Mode: Preprocessing avancé sur données pré-enrichies\")\n",
    "elif enriched_files_alt.exists():\n",
    "    source_file = enriched_files_alt\n",
    "    print(f\" FICHIER ENRICHI ALTERNATIF DÉTECTÉ: {enriched_files_alt}\")\n",
    "    print(\"   Mode: Preprocessing avancé sur données pré-enrichies\")\n",
    "elif raw_file.exists():\n",
    "    source_file = raw_file\n",
    "    print(f\" FICHIER BRUT DÉTECTÉ: {raw_file}\")\n",
    "    print(\"   → Mode: Preprocessing complet depuis zéro\")\n",
    "else:\n",
    "    print(f\" ERREUR: Aucun fichier source trouvé!\")\n",
    "    print(f\"   Recherche: {enriched_file} OU {raw_file}\")\n",
    "    print(\"   Solution: Exécutez d'abord collect_articles.ipynb ou enrich_articles.ipynb\")\n",
    "    exit(1)\n",
    "\n",
    "with open(source_file, 'r', encoding='utf-8') as f:\n",
    "    articles_data = json.load(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98d01366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 articles chargés depuis enriched_articles.json\n"
     ]
    }
   ],
   "source": [
    "# Mode adaptatif selon la source - MODIFIÉ\n",
    "ENRICHED_MODE = \"enriched_article\" in str(source_file)  # Compatible avec les deux formats\n",
    "\n",
    "print(f\" {len(articles_data)} articles chargés depuis {source_file.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ec2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame pour manipulation\n",
    "df = pd.DataFrame(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0279f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HARMONISATION DES NOMS DE COLONNES\n",
      "    Harmonisation: cleaned_text → text_cleaned\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n HARMONISATION DES NOMS DE COLONNES\")\n",
    "\n",
    "# Harmonisation text_cleaned vs cleaned_text\n",
    "if 'cleaned_text' in df.columns and 'text_cleaned' not in df.columns:\n",
    "    df['text_cleaned'] = df['cleaned_text']\n",
    "    print(\"    Harmonisation: cleaned_text → text_cleaned\")\n",
    "elif 'text' in df.columns and 'text_cleaned' not in df.columns:\n",
    "    df['text_cleaned'] = df['text']\n",
    "    print(\"    Création: text → text_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6dfd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Toutes les colonnes essentielles présentes\n"
     ]
    }
   ],
   "source": [
    "# Vérification des colonnes essentielles\n",
    "required_columns = ['title', 'text', 'source']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"     Colonnes manquantes: {missing_columns}\")\n",
    "else:\n",
    "    print(\"    Toutes les colonnes essentielles présentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bee8759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STRUCTURE DES DONNÉES:\n",
      "   Colonnes: ['id', 'title', 'summary', 'text', 'published', 'source', 'url', 'created_at', 'cleaned_text', 'language', 'entities', 'embedding', 'quality_score', 'text_cleaned']\n",
      "   Articles avec texte complet: 200\n",
      "   Articles sans texte: 0\n",
      "   Articles avec texte nettoyé: 200\n",
      "   Longueur moyenne du texte: 6995 caractères\n"
     ]
    }
   ],
   "source": [
    "# Inspection rapide\n",
    "print(f\"\\n STRUCTURE DES DONNÉES:\")\n",
    "print(f\"   Colonnes: {list(df.columns)}\")\n",
    "print(f\"   Articles avec texte complet: {df['text'].notna().sum()}\")\n",
    "print(f\"   Articles sans texte: {df['text'].isna().sum()}\")\n",
    "if 'text_cleaned' in df.columns:\n",
    "    print(f\"   Articles avec texte nettoyé: {df['text_cleaned'].notna().sum()}\")\n",
    "print(f\"   Longueur moyenne du texte: {df['text'].str.len().mean():.0f} caractères\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b59b4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ÉTAPE 2: Analyse adaptative des données\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n ÉTAPE 2: Analyse adaptative des données\")\n",
    "\n",
    "# Détection des colonnes d'enrichissement déjà présentes\n",
    "enrichment_columns = {\n",
    "    'language': 'language' in df.columns and df['language'].notna().sum() > 0,\n",
    "    'entities': 'entities' in df.columns and df['entities'].notna().sum() > 0,\n",
    "    'quality_score': 'quality_score' in df.columns and df['quality_score'].notna().sum() > 0,\n",
    "    'embedding': 'embedding' in df.columns and df['embedding'].notna().sum() > 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e46e3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ÉTAT DES ENRICHISSEMENTS EXISTANTS:\n",
      "   language:  Présent (200 articles)\n",
      "   entities:  Présent (200 articles)\n",
      "   quality_score:  Présent (200 articles)\n",
      "   embedding:  Présent (200 articles)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n ÉTAT DES ENRICHISSEMENTS EXISTANTS:\")\n",
    "for col, present in enrichment_columns.items():\n",
    "    status = \" Présent\" if present else \" Absent\" \n",
    "    count = df[col].notna().sum() if present else 0\n",
    "    print(f\"   {col}: {status} ({count} articles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0299ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MODE DÉTECTÉ: Preprocessing complémentaire avancé\n",
      "   → Focus: Déduplication, biais, corpus calibration, métriques avancées\n"
     ]
    }
   ],
   "source": [
    "# Adaptation de la stratégie\n",
    "if enrichment_columns['language'] and enrichment_columns['entities']:\n",
    "    print(f\"\\n MODE DÉTECTÉ: Preprocessing complémentaire avancé\")\n",
    "    print(f\"   → Focus: Déduplication, biais, corpus calibration, métriques avancées\")\n",
    "    SKIP_BASIC_ENRICHMENT = True\n",
    "else:\n",
    "    print(f\"\\n MODE DÉTECTÉ: Preprocessing complet depuis zéro\") \n",
    "    print(f\"   → Pipeline: Enrichissement + Analyses avancées\")\n",
    "    SKIP_BASIC_ENRICHMENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9abb81e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ÉTAPE 3: Nettoyage avancé des données\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n ÉTAPE 3: Nettoyage avancé des données\")\n",
    "\n",
    "def clean_text_advanced(text):\n",
    "    \"\"\"Nettoyage robuste et avancé du texte\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Correction encodage\n",
    "    text = ftfy.fix_text(text)\n",
    "    \n",
    "    # Suppression HTML résiduel\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Normalisation Unicode\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Suppression caractères de contrôle\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # Normalisation espaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Suppression URLs et emails\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Suppression patterns RSS spécifiques\n",
    "    text = re.sub(r'#xtor=RSS-\\d+.*', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]$', '', text)  # Crédits en fin d'article\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b31837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Textes déjà nettoyés détectés\n"
     ]
    }
   ],
   "source": [
    "# Application du nettoyage si nécessaire\n",
    "if 'text_cleaned' not in df.columns or df['text_cleaned'].isna().any():\n",
    "    print(\"    Application du nettoyage avancé...\")\n",
    "    df['text_cleaned'] = df['text'].apply(clean_text_advanced)\n",
    "    print(f\"       {len(df)} textes nettoyés\")\n",
    "else:\n",
    "    print(\"    Textes déjà nettoyés détectés\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9152712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Filtrage longueur minimum (100 chars): 200 → 200 articles\n"
     ]
    }
   ],
   "source": [
    "# Filtrage des articles trop courts ou vides\n",
    "min_length = 100  # caractères minimum\n",
    "df_clean = df[df['text_cleaned'].str.len() >= min_length].copy()\n",
    "print(f\"    Filtrage longueur minimum ({min_length} chars): {len(df)} → {len(df_clean)} articles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3bb1357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÉTAPE 4: Gestion intelligente de la langue\n",
      "     Langues déjà détectées, validation des données...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nÉTAPE 4: Gestion intelligente de la langue\")\n",
    "\n",
    "if not enrichment_columns['language'] or not SKIP_BASIC_ENRICHMENT:\n",
    "    print(\"    Détection de langue en cours...\")\n",
    "    \n",
    "    def detect_language_robust(text):\n",
    "        \"\"\"Détection de langue avec fallback\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return 'unknown', 0.0\n",
    "        \n",
    "        try:\n",
    "            # langdetect avec probabilités\n",
    "            langs = detect_langs(text)\n",
    "            primary_lang = langs[0]\n",
    "            return primary_lang.lang, primary_lang.prob\n",
    "        except LangDetectException:\n",
    "            # Fallback: détection basique\n",
    "            try:\n",
    "                return detect(text), 0.5\n",
    "            except:\n",
    "                return 'unknown', 0.0\n",
    "\n",
    "    # Application de la détection\n",
    "    language_results = df_clean['text_cleaned'].apply(detect_language_robust)\n",
    "    df_clean['language'] = [result[0] for result in language_results]\n",
    "    df_clean['language_confidence'] = [result[1] for result in language_results]\n",
    "    \n",
    "    print(f\"    Détection de langue terminée\")\n",
    "else:\n",
    "    print(\"     Langues déjà détectées, validation des données...\")\n",
    "    if 'language_confidence' not in df_clean.columns:\n",
    "        df_clean['language_confidence'] = df_clean['language'].apply(lambda x: 0.9 if x == 'fr' else 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaad1e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DISTRIBUTION DES LANGUES:\n",
      "   fr: 130 articles (65.0%)\n",
      "   en: 70 articles (35.0%)\n"
     ]
    }
   ],
   "source": [
    "# Analyse des langues détectées\n",
    "lang_counts = df_clean['language'].value_counts()\n",
    "print(f\"\\n DISTRIBUTION DES LANGUES:\")\n",
    "for lang, count in lang_counts.head(5).items():\n",
    "    pct = count / len(df_clean) * 100\n",
    "    print(f\"   {lang}: {count} articles ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e62f0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🇫🇷 Focus français: 130 articles sélectionnés\n"
     ]
    }
   ],
   "source": [
    "# Sélection intelligente selon la distribution\n",
    "if lang_counts.get('fr', 0) > len(df_clean) * 0.3:  # Si >30% en français\n",
    "    df_filtered = df_clean[df_clean['language'] == 'fr'].copy()\n",
    "    print(f\"   🇫🇷 Focus français: {len(df_filtered)} articles sélectionnés\")\n",
    "else:\n",
    "    # Garder top 2 langues si pas assez de français\n",
    "    top_langs = lang_counts.head(2).index.tolist()\n",
    "    df_filtered = df_clean[df_clean['language'].isin(top_langs)].copy()\n",
    "    print(f\"    Multi-langues: {len(df_filtered)} articles ({top_langs})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19aa9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Chargement du modèle sentence-transformers...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Le fichier de pagination est insuffisant pour terminer cette opération. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Chargement du modèle d'embeddings\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m    Chargement du modèle sentence-transformers...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m embeddings_model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDINGS_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeduplicate_semantic\u001b[39m(df, threshold=\u001b[32m0.85\u001b[39m):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Déduplication sémantique avancée avec FAISS\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2254\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2249\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2253\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2271\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:541\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m     **kwargs,\n\u001b[32m    527\u001b[39m ) -> Self:\n\u001b[32m    528\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    529\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    530\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m         backend=backend,\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:88\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m     87\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     91\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:186\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    184\u001b[39m         \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_onnx_model(model_name_or_path, config, cache_dir, **model_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5105\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5102\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5104\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5105\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5106\u001b[39m     )\n\u001b[32m   5108\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5109\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:532\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Use safetensors if possible\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    533\u001b[39m         metadata = f.metadata()\n\u001b[32m    535\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata.get(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmlx\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mOSError\u001b[39m: Le fichier de pagination est insuffisant pour terminer cette opération. (os error 1455)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chargement du modèle d'embeddings\n",
    "print(\"    Chargement du modèle sentence-transformers...\")\n",
    "embeddings_model = SentenceTransformer(EMBEDDINGS_MODEL)\n",
    "\n",
    "def deduplicate_semantic(df, threshold=0.85):\n",
    "    \"\"\"Déduplication sémantique avancée avec FAISS\"\"\"\n",
    "    \n",
    "    print(f\"    Génération des embeddings pour {len(df)} articles...\")\n",
    "    \n",
    "    # Utilisation des embeddings existants ou génération\n",
    "    if 'embedding' in df.columns and df['embedding'].notna().sum() > 0:\n",
    "        print(\"       Utilisation des embeddings existants\")\n",
    "        embeddings = []\n",
    "        for idx, emb in df['embedding'].items():\n",
    "            if isinstance(emb, (list, np.ndarray)) and len(emb) > 0:\n",
    "                embeddings.append(np.array(emb))\n",
    "            else:\n",
    "                # Génération pour les embeddings manquants\n",
    "                text = df.loc[idx, 'text_cleaned']\n",
    "                embeddings.append(embeddings_model.encode(text))\n",
    "        embeddings = np.array(embeddings)\n",
    "    else:\n",
    "        print(\"       Génération des embeddings...\")\n",
    "        texts = df['text_cleaned'].tolist()\n",
    "        embeddings = embeddings_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    # Configuration FAISS\n",
    "    print(\"     Configuration de l'index FAISS...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine après normalisation)\n",
    "    \n",
    "    # Normalisation pour cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    # Recherche des doublons\n",
    "    print(\"    Recherche des doublons sémantiques...\")\n",
    "    similarities, indices = index.search(embeddings.astype('float32'), k=5)  # Top 5 similaires\n",
    "    \n",
    "    to_remove = set()\n",
    "    duplicate_pairs = []\n",
    "    \n",
    "    for i, (sim_scores, sim_indices) in enumerate(zip(similarities, indices)):\n",
    "        for j, (score, idx) in enumerate(zip(sim_scores, sim_indices)):\n",
    "            if j > 0 and score > threshold and idx not in to_remove and i not in to_remove:\n",
    "                # Garde le plus récent ou le mieux noté\n",
    "                if df.iloc[i].get('quality_score', 0) >= df.iloc[idx].get('quality_score', 0):\n",
    "                    to_remove.add(idx)\n",
    "                else:\n",
    "                    to_remove.add(i)\n",
    "                \n",
    "                duplicate_pairs.append((i, idx, score))\n",
    "    \n",
    "    # Suppression des doublons\n",
    "    df_dedup = df.drop(df.index[list(to_remove)]).copy()\n",
    "    \n",
    "    print(f\"    Résultats déduplication:\")\n",
    "    print(f\"      Articles originaux: {len(df)}\")\n",
    "    print(f\"      Doublons détectés: {len(to_remove)}\")\n",
    "    print(f\"      Articles finaux: {len(df_dedup)}\")\n",
    "    print(f\"      Taux de déduplication: {len(to_remove)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    # Exemples de doublons détectés\n",
    "    if duplicate_pairs:\n",
    "        print(f\"    Exemples de doublons détectés:\")\n",
    "        for i, (idx1, idx2, sim) in enumerate(duplicate_pairs[:3]):\n",
    "            title1 = df.iloc[idx1]['title'][:50]\n",
    "            title2 = df.iloc[idx2]['title'][:50]\n",
    "            print(f\"      {i+1}. Similarité {sim:.3f}:\")\n",
    "            print(f\"         A: {title1}...\")\n",
    "            print(f\"         B: {title2}...\")\n",
    "    \n",
    "    return df_dedup, embeddings\n",
    "\n",
    "# Application de la déduplication\n",
    "df_clean_dedup, article_embeddings = deduplicate_semantic(df_filtered, SIMILARITY_THRESHOLD)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ÉTAPE 6: Gestion avancée des entités nommées\")\n",
    "\n",
    "if not enrichment_columns['entities'] or not SKIP_BASIC_ENRICHMENT:\n",
    "    print(\"    Extraction complète des entités avec spaCy...\")\n",
    "    \n",
    "    # Chargement du modèle spaCy français\n",
    "    print(f\"       Chargement du modèle spaCy: {NLP_MODEL}\")\n",
    "    try:\n",
    "        nlp = spacy.load(NLP_MODEL)\n",
    "    except OSError:\n",
    "        print(f\"       Modèle {NLP_MODEL} non trouvé. Installation...\")\n",
    "        import subprocess\n",
    "        subprocess.run(f\"python -m spacy download {NLP_MODEL}\", shell=True)\n",
    "        nlp = spacy.load(NLP_MODEL)\n",
    "\n",
    "    def extract_entities_advanced(text, nlp_model):\n",
    "        \"\"\"Extraction d'entités avec enrichissements\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return {\n",
    "                'persons': [], 'organizations': [], 'locations': [],\n",
    "                'dates': [], 'money': [], 'misc': []\n",
    "            }\n",
    "        \n",
    "        # Traitement avec spaCy (limiter la longueur pour performance)\n",
    "        doc = nlp_model(text[:8000])  # Premier 8k caractères\n",
    "        \n",
    "        entities = {\n",
    "            'persons': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "            'dates': [],\n",
    "            'money': [],\n",
    "            'misc': []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_text = ent.text.strip()\n",
    "            if len(entity_text) < 2:  \n",
    "                continue\n",
    "                \n",
    "            if ent.label_ in ['PERSON']:\n",
    "                entities['persons'].append(entity_text)\n",
    "            elif ent.label_ in ['ORG']:\n",
    "                entities['organizations'].append(entity_text)\n",
    "            elif ent.label_ in ['GPE', 'LOC']:\n",
    "                entities['locations'].append(entity_text)\n",
    "            elif ent.label_ in ['DATE', 'TIME']:\n",
    "                entities['dates'].append(entity_text)\n",
    "            elif ent.label_ in ['MONEY']:\n",
    "                entities['money'].append(entity_text)\n",
    "            else:\n",
    "                entities['misc'].append(entity_text)\n",
    "        \n",
    "        # Déduplication et nettoyage\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))  # Suppression doublons\n",
    "            entities[key] = [e for e in entities[key] if len(e) > 1]  # Filtrage longueur\n",
    "        \n",
    "        return entities\n",
    "\n",
    "    # Application de l'extraction d'entités\n",
    "    entities_results = []\n",
    "    for text in tqdm(df_clean_dedup['text_cleaned'], desc=\"Extraction NER\"):\n",
    "        entities = extract_entities_advanced(text, nlp)\n",
    "        entities_results.append(entities)\n",
    "\n",
    "    # Ajout des résultats au DataFrame\n",
    "    df_clean_dedup['entities_advanced'] = entities_results\n",
    "    \n",
    "    print(f\"       Extraction NER terminée\")\n",
    "    \n",
    "else:\n",
    "    print(\"    Amélioration des entités existantes...\")\n",
    "    \n",
    "    def improve_entities(existing_entities):\n",
    "        \"\"\"Amélioration et nettoyage des entités existantes\"\"\"\n",
    "        if not existing_entities or not isinstance(existing_entities, dict):\n",
    "            return {\n",
    "                'persons': [], 'organizations': [], 'locations': [],\n",
    "                'dates': [], 'money': [], 'misc': []\n",
    "            }\n",
    "        \n",
    "        improved = {\n",
    "            'persons': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "            'dates': [],\n",
    "            'money': [],\n",
    "            'misc': []\n",
    "        }\n",
    "        \n",
    "        # Nettoyage et déduplication\n",
    "        for key in improved.keys():\n",
    "            if key in existing_entities and isinstance(existing_entities[key], list):\n",
    "                # Nettoyage des entités\n",
    "                cleaned = [str(e).strip() for e in existing_entities[key] if e and len(str(e)) > 1]\n",
    "                # Déduplication case-insensitive\n",
    "                seen = set()\n",
    "                for entity in cleaned:\n",
    "                    if entity.lower() not in seen:\n",
    "                        improved[key].append(entity)\n",
    "                        seen.add(entity.lower())\n",
    "        \n",
    "        return improved\n",
    "    \n",
    "    # Application de l'amélioration\n",
    "    df_clean_dedup['entities_advanced'] = df_clean_dedup['entities'].apply(improve_entities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366da564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de colonnes métriques enrichies\n",
    "df_clean_dedup['persons_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('persons', [])))\n",
    "df_clean_dedup['organizations_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('organizations', [])))\n",
    "df_clean_dedup['locations_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('locations', [])))\n",
    "df_clean_dedup['entities_total'] = (df_clean_dedup['persons_count'] + \n",
    "                                   df_clean_dedup['organizations_count'] + \n",
    "                                   df_clean_dedup['locations_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n    STATISTIQUES ENTITÉS AVANCÉES:\")\n",
    "print(f\"      Moyenne personnes/article: {df_clean_dedup['persons_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne organisations/article: {df_clean_dedup['organizations_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne lieux/article: {df_clean_dedup['locations_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne total entités/article: {df_clean_dedup['entities_total'].mean():.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top entités par catégorie\n",
    "print(f\"\\n    TOP ENTITÉS DÉTECTÉES:\")\n",
    "all_persons = [p for entities in df_clean_dedup['entities_advanced'] for p in entities.get('persons', [])]\n",
    "all_orgs = [o for entities in df_clean_dedup['entities_advanced'] for o in entities.get('organizations', [])]\n",
    "all_locs = [l for entities in df_clean_dedup['entities_advanced'] for l in entities.get('locations', [])]\n",
    "\n",
    "if all_persons:\n",
    "    top_persons = Counter(all_persons).most_common(3)\n",
    "    print(f\"      Personnes: {', '.join([f'{p} ({c})' for p, c in top_persons])}\")\n",
    "\n",
    "if all_orgs:\n",
    "    top_orgs = Counter(all_orgs).most_common(3)\n",
    "    print(f\"      Organisations: {', '.join([f'{o} ({c})' for o, c in top_orgs])}\")\n",
    "\n",
    "if all_locs:\n",
    "    top_locs = Counter(all_locs).most_common(3)\n",
    "    print(f\"      Lieux: {', '.join([f'{l} ({c})' for l, c in top_locs])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138328df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ÉTAPE 7: Segmentation sémantique avancée\")\n",
    "\n",
    "# Téléchargement des ressources NLTK si nécessaire\n",
    "try:\n",
    "    import ssl\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "except:\n",
    "    pass\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def segment_text_advanced(text):\n",
    "    \"\"\"Segmentation en phrases avec analyse sémantique avancée\"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return {\n",
    "            'sentences': [],\n",
    "            'sentence_count': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "            'paragraphs': [],\n",
    "            'paragraph_count': 0,\n",
    "            'text_complexity': 0,\n",
    "            'readability_score': 0\n",
    "        }\n",
    "    \n",
    "    # Segmentation en phrases (multi-langue)\n",
    "    sentences = sent_tokenize(text[:5000], language='french')  # Limiter pour performance\n",
    "    \n",
    "    # Segmentation en paragraphes\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip() and len(p) > 20]\n",
    "    \n",
    "    # Métriques avancées\n",
    "    sentence_lengths = [len(s.split()) for s in sentences]\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Score de complexité basé sur la longueur des phrases\n",
    "    complexity = 0\n",
    "    if sentence_lengths:\n",
    "        variance = np.var(sentence_lengths)\n",
    "        long_sentences = sum(1 for length in sentence_lengths if length > 20)\n",
    "        complexity = min((variance / 100) + (long_sentences / len(sentences)), 1.0)\n",
    "    \n",
    "    # Score de lisibilité approximatif (Flesch-like)\n",
    "    if sentence_lengths and avg_sentence_length > 0:\n",
    "        readability = max(0, min(1, 1 - (avg_sentence_length - 10) / 20))\n",
    "    else:\n",
    "        readability = 0.5\n",
    "    \n",
    "    return {\n",
    "        'sentences': sentences[:50],  # Limiter pour stockage\n",
    "        'sentence_count': len(sentences),\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'paragraphs': paragraphs[:20],  # Limiter pour stockage\n",
    "        'paragraph_count': len(paragraphs),\n",
    "        'text_complexity': complexity,\n",
    "        'readability_score': readability\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la segmentation\n",
    "print(\"    Segmentation avancée en cours...\")\n",
    "segmentation_results = []\n",
    "\n",
    "for text in tqdm(df_clean_dedup['text_cleaned'], desc=\"Segmentation\"):\n",
    "    segments = segment_text_advanced(text)\n",
    "    segmentation_results.append(segments)\n",
    "\n",
    "df_clean_dedup['segmentation'] = segmentation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f579926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des métriques de segmentation\n",
    "df_clean_dedup['sentence_count'] = df_clean_dedup['segmentation'].apply(lambda x: x['sentence_count'])\n",
    "df_clean_dedup['paragraph_count'] = df_clean_dedup['segmentation'].apply(lambda x: x['paragraph_count'])\n",
    "df_clean_dedup['avg_sentence_length'] = df_clean_dedup['segmentation'].apply(lambda x: x['avg_sentence_length'])\n",
    "df_clean_dedup['text_complexity'] = df_clean_dedup['segmentation'].apply(lambda x: x['text_complexity'])\n",
    "df_clean_dedup['readability_score'] = df_clean_dedup['segmentation'].apply(lambda x: x['readability_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n    STATISTIQUES SEGMENTATION AVANCÉES:\")\n",
    "print(f\"      Moyenne phrases/article: {df_clean_dedup['sentence_count'].mean():.1f}\")\n",
    "print(f\"      Moyenne paragraphes/article: {df_clean_dedup['paragraph_count'].mean():.1f}\")\n",
    "print(f\"      Longueur moyenne phrases: {df_clean_dedup['avg_sentence_length'].mean():.1f} mots\")\n",
    "print(f\"      Score complexité moyen: {df_clean_dedup['text_complexity'].mean():.2f}\")\n",
    "print(f\"      Score lisibilité moyen: {df_clean_dedup['readability_score'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ÉTAPE 8: Analyse des biais temporels et géographiques\")\n",
    "\n",
    "# Conversion et nettoyage des dates\n",
    "print(\"    Analyse temporelle...\")\n",
    "df_clean_dedup['published_clean'] = pd.to_datetime(df_clean_dedup['published'], errors='coerce')\n",
    "\n",
    "# Extraction des composants temporels\n",
    "df_clean_dedup['hour'] = df_clean_dedup['published_clean'].dt.hour\n",
    "df_clean_dedup['day_of_week'] = df_clean_dedup['published_clean'].dt.day_name()\n",
    "df_clean_dedup['month'] = df_clean_dedup['published_clean'].dt.month\n",
    "df_clean_dedup['date_only'] = df_clean_dedup['published_clean'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des biais temporels\n",
    "valid_dates = df_clean_dedup[df_clean_dedup['published_clean'].notna()]\n",
    "\n",
    "if len(valid_dates) > 0:\n",
    "    print(f\"       Analyse sur {len(valid_dates)} articles avec dates valides\")\n",
    "    \n",
    "    # Distribution horaire\n",
    "    hour_dist = valid_dates['hour'].value_counts().head(3)\n",
    "    print(f\"       Heures de publication principales:\")\n",
    "    for hour, count in hour_dist.items():\n",
    "        pct = count / len(valid_dates) * 100\n",
    "        print(f\"         {hour}h: {count} articles ({pct:.1f}%)\")\n",
    "    \n",
    "    # Distribution par jour\n",
    "    day_dist = valid_dates['day_of_week'].value_counts().head(3)\n",
    "    print(f\"       Jours de publication principaux:\")\n",
    "    for day, count in day_dist.items():\n",
    "        pct = count / len(valid_dates) * 100\n",
    "        print(f\"         {day}: {count} articles ({pct:.1f}%)\")\n",
    "    \n",
    "    # Calcul du score de biais temporel\n",
    "    hour_entropy = -sum((p := hour_dist / len(valid_dates)) * np.log2(p + 1e-10))\n",
    "    day_entropy = -sum((p := day_dist / len(valid_dates)) * np.log2(p + 1e-10))\n",
    "    \n",
    "    # Normalisation (entropie max = log2(24) pour heures, log2(7) pour jours)\n",
    "    hour_bias = 1 - (hour_entropy / np.log2(24))  # 0 = uniforme, 1 = très biaisé\n",
    "    day_bias = 1 - (day_entropy / np.log2(7))\n",
    "    \n",
    "    print(f\"       Scores de biais temporel:\")\n",
    "    print(f\"         Biais horaire: {hour_bias:.2f} (0=uniforme, 1=concentré)\")\n",
    "    print(f\"         Biais quotidien: {day_bias:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse géographique via entités lieux\n",
    "print(\"\\n    Analyse géographique...\")\n",
    "all_locations = []\n",
    "location_counts_by_article = []\n",
    "\n",
    "for entities in df_clean_dedup['entities_advanced']:\n",
    "    article_locations = entities.get('locations', [])\n",
    "    location_counts_by_article.append(len(article_locations))\n",
    "    all_locations.extend(article_locations)\n",
    "\n",
    "location_distribution = Counter(all_locations)\n",
    "df_clean_dedup['locations_count'] = location_counts_by_article\n",
    "\n",
    "if location_distribution:\n",
    "    print(f\"       Lieux les plus mentionnés:\")\n",
    "    for location, count in location_distribution.most_common(5):\n",
    "        pct = count / len(all_locations) * 100 if len(all_locations) > 0 else 0\n",
    "        print(f\"         {location}: {count} mentions ({pct:.1f}%)\")\n",
    "    \n",
    "    # Score de biais géographique\n",
    "    if len(location_distribution) > 1:\n",
    "        geo_probs = np.array(list(location_distribution.values())) / len(all_locations)\n",
    "        geo_entropy = -sum(geo_probs * np.log2(geo_probs + 1e-10))\n",
    "        max_entropy = np.log2(min(len(location_distribution), 50))  # Entropie max théorique\n",
    "        geo_bias = 1 - (geo_entropy / max_entropy) if max_entropy > 0 else 0\n",
    "        \n",
    "        print(f\"       Score de biais géographique: {geo_bias:.2f}\")\n",
    "        print(f\"         (0=distribution équilibrée, 1=concentration forte)\")\n",
    "    else:\n",
    "        geo_bias = 1.0  # Maximum bias if only one location\n",
    "        print(f\"        Biais géographique maximal détecté\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des scores de biais au DataFrame\n",
    "df_clean_dedup['temporal_bias_hour'] = hour_bias if 'hour_bias' in locals() else 0\n",
    "df_clean_dedup['temporal_bias_day'] = day_bias if 'day_bias' in locals() else 0\n",
    "df_clean_dedup['geographic_bias'] = geo_bias if 'geo_bias' in locals() else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_quality_score(row):\n",
    "    \"\"\"Calcul d'un score de qualité multi-dimensionnel\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # 1. Score de longueur (0-1)\n",
    "    text_len = len(str(row.get('text_cleaned', '')))\n",
    "    scores['length'] = min(text_len / 2000, 1.0)  # Optimal à 2000 caractères\n",
    "    \n",
    "    # 2. Score d'entités (0-1)\n",
    "    entities_count = row.get('entities_total', 0)\n",
    "    scores['entities'] = min(entities_count / 10, 1.0)  # Optimal à 10 entités\n",
    "    \n",
    "    # 3. Score de lisibilité (0-1)\n",
    "    scores['readability'] = row.get('readability_score', 0.5)\n",
    "    \n",
    "    # 4. Score de complexité inversé (0-1)\n",
    "    complexity = row.get('text_complexity', 0.5)\n",
    "    scores['complexity'] = 1 - complexity  # Moins complexe = meilleur\n",
    "    \n",
    "    # 5. Score de structure (0-1)\n",
    "    sentence_count = row.get('sentence_count', 0)\n",
    "    paragraph_count = row.get('paragraph_count', 0)\n",
    "    if sentence_count > 0 and paragraph_count > 0:\n",
    "        structure_ratio = min(sentence_count / paragraph_count, 10) / 10  # Ratio phrases/paragraphes\n",
    "        scores['structure'] = structure_ratio\n",
    "    else:\n",
    "        scores['structure'] = 0.1\n",
    "    \n",
    "    # 6. Score de langue (0-1)\n",
    "    lang_confidence = row.get('language_confidence', 0.5)\n",
    "    scores['language'] = lang_confidence\n",
    "    \n",
    "    # Score global pondéré\n",
    "    weights = {\n",
    "        'length': 0.2,\n",
    "        'entities': 0.25, \n",
    "        'readability': 0.2,\n",
    "        'complexity': 0.15,\n",
    "        'structure': 0.1,\n",
    "        'language': 0.1\n",
    "    }\n",
    "    \n",
    "    final_score = sum(scores[key] * weights[key] for key in scores)\n",
    "    \n",
    "    return {\n",
    "        'quality_score_advanced': final_score,\n",
    "        'quality_breakdown': scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0427f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du calcul de qualité\n",
    "print(\"    Calcul des scores de qualité...\")\n",
    "quality_results = []\n",
    "\n",
    "for _, row in tqdm(df_clean_dedup.iterrows(), total=len(df_clean_dedup), desc=\"Qualité\"):\n",
    "    quality_result = calculate_advanced_quality_score(row)\n",
    "    quality_results.append(quality_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des résultats\n",
    "df_clean_dedup['quality_score_advanced'] = [r['quality_score_advanced'] for r in quality_results]\n",
    "df_clean_dedup['quality_breakdown'] = [r['quality_breakdown'] for r in quality_results]\n",
    "\n",
    "print(f\"\\n    STATISTIQUES QUALITÉ AVANCÉES:\")\n",
    "print(f\"      Score moyen: {df_clean_dedup['quality_score_advanced'].mean():.3f}\")\n",
    "print(f\"      Score médian: {df_clean_dedup['quality_score_advanced'].median():.3f}\")\n",
    "print(f\"      Score min: {df_clean_dedup['quality_score_advanced'].min():.3f}\")\n",
    "print(f\"      Score max: {df_clean_dedup['quality_score_advanced'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par quartiles\n",
    "quartiles = df_clean_dedup['quality_score_advanced'].quantile([0.25, 0.5, 0.75])\n",
    "print(f\"      Quartiles: Q1={quartiles[0.25]:.3f}, Q2={quartiles[0.5]:.3f}, Q3={quartiles[0.75]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_stratified_calibration_corpus(df, n_samples=300):\n",
    "    \"\"\"Création d'un corpus stratifié pour calibration\"\"\"\n",
    "    \n",
    "    # Définition des strates multi-dimensionnelles\n",
    "    print(\"   Définition des strates...\")\n",
    "    \n",
    "    # 1. Strate par qualité (3 niveaux)\n",
    "    quality_tertiles = df['quality_score_advanced'].quantile([0.33, 0.67])\n",
    "    df['quality_stratum'] = pd.cut(df['quality_score_advanced'], \n",
    "                                  bins=[0, quality_tertiles[0.33], quality_tertiles[0.67], 1],\n",
    "                                  labels=['low', 'medium', 'high'])\n",
    "    \n",
    "    # 2. Strate par longueur (3 niveaux)\n",
    "    df['text_length'] = df['text_cleaned'].str.len()\n",
    "    length_tertiles = df['text_length'].quantile([0.33, 0.67])\n",
    "    df['length_stratum'] = pd.cut(df['text_length'],\n",
    "                                 bins=[0, length_tertiles[0.33], length_tertiles[0.67], float('inf')],\n",
    "                                 labels=['short', 'medium', 'long'])\n",
    "    \n",
    "    # 3. Strate par richesse en entités (3 niveaux)\n",
    "    if df['entities_total'].max() > 0:\n",
    "        entity_tertiles = df['entities_total'].quantile([0.33, 0.67])\n",
    "        df['entity_stratum'] = pd.cut(df['entities_total'],\n",
    "                                     bins=[-1, entity_tertiles[0.33], entity_tertiles[0.67], float('inf')],\n",
    "                                     labels=['sparse', 'moderate', 'rich'])\n",
    "    else:\n",
    "        df['entity_stratum'] = 'sparse'\n",
    "    \n",
    "    # 4. Strate par source (top sources + autres)\n",
    "    source_counts = df['source'].value_counts()\n",
    "    top_sources = source_counts.head(5).index.tolist()\n",
    "    df['source_stratum'] = df['source'].apply(lambda x: x if x in top_sources else 'other')\n",
    "    \n",
    "    print(f\"      Strates créées:\")\n",
    "    print(f\"         Qualité: {df['quality_stratum'].value_counts().to_dict()}\")\n",
    "    print(f\"         Longueur: {df['length_stratum'].value_counts().to_dict()}\")\n",
    "    print(f\"         Entités: {df['entity_stratum'].value_counts().to_dict()}\")\n",
    "    print(f\"         Sources: {len(df['source_stratum'].unique())} catégories\")\n",
    "    \n",
    "    # Échantillonnage stratifié proportionnel\n",
    "    print(\"   Échantillonnage stratifié...\")\n",
    "    \n",
    "    # Groupement par strates multiples\n",
    "    strata_cols = ['quality_stratum', 'length_stratum', 'entity_stratum', 'source_stratum']\n",
    "    grouped = df.groupby(strata_cols, group_keys=False)\n",
    "    \n",
    "    # Calcul des tailles d'échantillon par strate\n",
    "    strata_sizes = grouped.size()\n",
    "    total_size = len(df)\n",
    "    \n",
    "    sample_dfs = []\n",
    "    remaining_samples = n_samples\n",
    "    \n",
    "    for stratum, group in grouped:\n",
    "        if remaining_samples <= 0:\n",
    "            break\n",
    "            \n",
    "        # Taille proportionnelle de l'échantillon pour cette strate\n",
    "        stratum_size = len(group)\n",
    "        proportion = stratum_size / total_size\n",
    "        target_sample_size = max(1, int(proportion * n_samples))\n",
    "        \n",
    "        # Ajustement si on dépasse le nombre d'échantillons restants\n",
    "        actual_sample_size = min(target_sample_size, remaining_samples, stratum_size)\n",
    "        \n",
    "        if actual_sample_size > 0:\n",
    "            # Échantillonnage au sein de la strate\n",
    "            if len(group) >= actual_sample_size:\n",
    "                # Tri par score de qualité pour prendre les meilleurs\n",
    "                group_sorted = group.sort_values('quality_score_advanced', ascending=False)\n",
    "                stratum_sample = group_sorted.head(actual_sample_size)\n",
    "                sample_dfs.append(stratum_sample)\n",
    "                remaining_samples -= actual_sample_size\n",
    "    \n",
    "    # Combinaison des échantillons de toutes les strates\n",
    "    if sample_dfs:\n",
    "        calibration_corpus = pd.concat(sample_dfs, ignore_index=True)\n",
    "    else:\n",
    "        # Fallback: échantillonnage simple par qualité\n",
    "        calibration_corpus = df.nlargest(n_samples, 'quality_score_advanced')\n",
    "    \n",
    "    # Complément aléatoire si nécessaire\n",
    "    if len(calibration_corpus) < n_samples:\n",
    "        remaining_df = df[~df.index.isin(calibration_corpus.index)]\n",
    "        if len(remaining_df) > 0:\n",
    "            additional_samples = min(n_samples - len(calibration_corpus), len(remaining_df))\n",
    "            additional = remaining_df.sample(n=additional_samples, random_state=42)\n",
    "            calibration_corpus = pd.concat([calibration_corpus, additional], ignore_index=True)\n",
    "    \n",
    "    return calibration_corpus.head(n_samples)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du corpus de calibration\n",
    "calibration_corpus = create_stratified_calibration_corpus(df_clean_dedup, n_samples=300)\n",
    "\n",
    "print(f\"\\n    CORPUS DE CALIBRATION CRÉÉ:\")\n",
    "print(f\"      Taille finale: {len(calibration_corpus)} articles\")\n",
    "print(f\"      Score qualité moyen: {calibration_corpus['quality_score_advanced'].mean():.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effda8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la représentativité\n",
    "print(f\"\\n    REPRÉSENTATIVITÉ DU CORPUS:\")\n",
    "print(f\"      Sources: {calibration_corpus['source'].nunique()} uniques\")\n",
    "print(f\"      Langues: {calibration_corpus['language'].value_counts().to_dict()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top sources dans le corpus\n",
    "source_dist = calibration_corpus['source'].value_counts().head(5)\n",
    "print(f\"      Top sources:\")\n",
    "for source, count in source_dist.items():\n",
    "    pct = count / len(calibration_corpus) * 100\n",
    "    source_name = source.split('/')[-1] if '/' in source else source\n",
    "    print(f\"         {source_name}: {count} articles ({pct:.1f}%)\")\n",
    "\n",
    "# Distribution qualité\n",
    "quality_dist = calibration_corpus['quality_stratum'].value_counts()\n",
    "print(f\"      Distribution qualité: {quality_dist.to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d23313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métriques de qualité finales\n",
    "quality_metrics = {\n",
    "    'source_file': str(source_file.name),\n",
    "    'enriched_mode': ENRICHED_MODE,\n",
    "    'total_articles_input': len(articles_data),\n",
    "    'articles_after_deduplication': len(df_clean_dedup),\n",
    "    'calibration_corpus_size': len(calibration_corpus),\n",
    "    'deduplication_rate': ((len(df) - len(df_clean_dedup)) / len(df)) if len(df) > 0 else 0,\n",
    "    'avg_quality_score': df_clean_dedup['quality_score_advanced'].mean(),\n",
    "    'language_distribution': df_clean_dedup['language'].value_counts().to_dict(),\n",
    "    'entities_avg_per_article': df_clean_dedup['entities_total'].mean(),\n",
    "    'temporal_bias_detected': df_clean_dedup['temporal_bias_hour'].iloc[0] if len(df_clean_dedup) > 0 else 0,\n",
    "    'geographic_bias_detected': df_clean_dedup['geographic_bias'].iloc[0] if len(df_clean_dedup) > 0 else 0,\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du DataFrame principal (format optimisé)\n",
    "output_file = PROCESSED_DIR / \"articles_preprocessed_advanced.pkl\"\n",
    "df_clean_dedup.to_pickle(output_file)\n",
    "print(f\"    Dataset principal sauvegardé: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du corpus de calibration\n",
    "calibration_file = PROCESSED_DIR / \"calibration_corpus_stratified.pkl\"\n",
    "calibration_corpus.to_pickle(calibration_file)\n",
    "print(f\"    Corpus de calibration sauvegardé: {calibration_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa272eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des métriques\n",
    "metrics_file = PROCESSED_DIR / \"advanced_preprocessing_metrics.json\"\n",
    "with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(quality_metrics, f, indent=2, ensure_ascii=False, default=str)\n",
    "print(f\"    Métriques sauvegardées: {metrics_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV léger pour analyse externe\n",
    "csv_file = PROCESSED_DIR / \"articles_preprocessed_summary.csv\"\n",
    "df_export = df_clean_dedup[[\n",
    "    'title', 'source', 'published', 'language', 'quality_score_advanced',\n",
    "    'entities_total', 'sentence_count', 'readability_score', 'text_complexity'\n",
    "]].copy()\n",
    "df_export.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "print(f\"    Export CSV résumé: {csv_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export JSON du corpus de calibration \n",
    "calibration_json = PROCESSED_DIR / \"calibration_corpus_300.json\"\n",
    "calibration_export = calibration_corpus[[\n",
    "    'id', 'title', 'text_cleaned', 'source', 'published', 'language',\n",
    "    'quality_score_advanced', 'entities_advanced'\n",
    "]].to_dict('records')\n",
    "\n",
    "with open(calibration_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(calibration_export, f, ensure_ascii=False, indent=2, default=str)\n",
    "print(f\"    Corpus calibration JSON: {calibration_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé des performances\n",
    "processing_time = (datetime.now() - datetime.fromisoformat(quality_metrics['processing_timestamp'])).total_seconds()\n",
    "\n",
    "print(f\" RÉSULTATS FINAUX:\")\n",
    "print(f\"   Source: {quality_metrics['source_file']}\")\n",
    "print(f\"   Mode: {'Enrichissement complémentaire' if ENRICHED_MODE else 'Pipeline complet'}\")\n",
    "print(f\"   Articles traités: {quality_metrics['total_articles_input']}\")\n",
    "print(f\"   Articles finaux: {quality_metrics['articles_after_deduplication']}\")\n",
    "print(f\"   Corpus de calibration: {quality_metrics['calibration_corpus_size']}\")\n",
    "print(f\"   Taux de déduplication: {quality_metrics['deduplication_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n MÉTRIQUES DE QUALITÉ:\")\n",
    "print(f\"   Score qualité moyen: {quality_metrics['avg_quality_score']:.3f}\")\n",
    "print(f\"   Entités par article: {quality_metrics['entities_avg_per_article']:.1f}\")\n",
    "print(f\"   Biais temporel détecté: {quality_metrics['temporal_bias_detected']:.2f}\")\n",
    "print(f\"   Biais géographique: {quality_metrics['geographic_bias_detected']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n DISTRIBUTION LINGUISTIQUE:\")\n",
    "for lang, count in quality_metrics['language_distribution'].items():\n",
    "    pct = count / quality_metrics['articles_after_deduplication'] * 100\n",
    "    print(f\"   {lang}: {count} articles ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c386cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFICHIERS GÉNÉRÉS:\")\n",
    "print(f\"   1. {output_file.name} - Dataset principal avec preprocessing avancé\")\n",
    "print(f\"   2. {calibration_file.name} - Corpus stratifié pour calibration\")\n",
    "print(f\"   3. {metrics_file.name} - Métriques détaillées\")\n",
    "print(f\"   4. {csv_file.name} - Export CSV pour analyse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nINSTRUCTIONS D'INSTALLATION SI ERREURS:\")\n",
    "print(f\"   Si erreur spaCy: !python -m spacy download fr_core_news_lg\")\n",
    "print(f\"   Si erreur FAISS: !pip install faiss-cpu\")\n",
    "print(f\"   Si erreur sentence-transformers: !pip install sentence-transformers\")\n",
    "print(f\"   Si erreur langdetect: !pip install langdetect\")\n",
    "print(f\"   Si erreur ftfy: !pip install ftfy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
