{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22379de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Auto-reload pour développement interactif\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP avancé\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Détection de langue\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Preprocessing texte\n",
    "import unicodedata\n",
    "import ftfy  # Pour corriger les encodages\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Similarité et déduplication\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033442b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration optimisée pour ressources limitées:\n",
      "   SpaCy: fr_core_news_sm (modèle léger)\n",
      "   Embeddings: all-MiniLM-L6-v2 (80MB au lieu de 420MB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path().resolve().parent.parent\n",
    "sys.path.append(str(BASE_DIR / \"src\"))\n",
    "\n",
    "# Répertoires\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "EXPORTS_DIR = DATA_DIR / \"exports\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration des modèles\n",
    "NLP_MODEL = \"fr_core_news_sm\"  # Modèle spaCy français LÉGER\n",
    "EMBEDDINGS_MODEL = \"all-MiniLM-L6-v2\"  # Modèle plus léger (~80MB vs 420MB)\n",
    "SIMILARITY_THRESHOLD = 0.85  # Seuil de similarité pour déduplication\n",
    "\n",
    "print(f\"Configuration optimisée pour ressources limitées:\")\n",
    "print(f\"   SpaCy: {NLP_MODEL} (modèle léger)\")\n",
    "print(f\"   Embeddings: {EMBEDDINGS_MODEL} (80MB au lieu de 420MB)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b185b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier JSON enrichi (priorité) ou brut (fallback)\n",
    "enriched_file = EXPORTS_DIR / \"enriched_article.json\"  # MODIFIÉ: sans \"s\"\n",
    "enriched_files_alt = EXPORTS_DIR / \"enriched_articles.json\"  # Alternative\n",
    "raw_file = EXPORTS_DIR / \"raw_articles.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09529af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FICHIER ENRICHI ALTERNATIF DÉTECTÉ: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\exports\\enriched_articles.json\n",
      "   Mode: Preprocessing avancé sur données pré-enrichies\n"
     ]
    }
   ],
   "source": [
    "# Détection automatique du fichier source\n",
    "if enriched_file.exists():\n",
    "    source_file = enriched_file\n",
    "    print(f\"FICHIER ENRICHI DÉTECTÉ: {enriched_file}\")\n",
    "    print(\"   Mode: Preprocessing avancé sur données pré-enrichies\")\n",
    "elif enriched_files_alt.exists():\n",
    "    source_file = enriched_files_alt\n",
    "    print(f\"FICHIER ENRICHI ALTERNATIF DÉTECTÉ: {enriched_files_alt}\")\n",
    "    print(\"   Mode: Preprocessing avancé sur données pré-enrichies\")\n",
    "elif raw_file.exists():\n",
    "    source_file = raw_file\n",
    "    print(f\"FICHIER BRUT DÉTECTÉ: {raw_file}\")\n",
    "    print(\"   → Mode: Preprocessing complet depuis zéro\")\n",
    "else:\n",
    "    print(f\"ERREUR: Aucun fichier source trouvé!\")\n",
    "    print(f\"   Recherche: {enriched_file} OU {raw_file}\")\n",
    "    print(\"   Solution: Exécutez d'abord collect_articles.ipynb ou enrich_articles.ipynb\")\n",
    "    exit(1)\n",
    "\n",
    "with open(source_file, 'r', encoding='utf-8') as f:\n",
    "    articles_data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987311e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 articles chargés depuis enriched_articles.json\n"
     ]
    }
   ],
   "source": [
    "# Mode adaptatif selon la source\n",
    "ENRICHED_MODE = \"enriched_article\" in str(source_file)  # Compatible avec les deux formats\n",
    "\n",
    "print(f\"{len(articles_data)} articles chargés depuis {source_file.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17eb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame pour manipulation\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9949d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Harmonisation: cleaned_text → text_cleaned\n",
      "   Toutes les colonnes essentielles présentes\n",
      "   Structure: 200 articles, 14 colonnes\n"
     ]
    }
   ],
   "source": [
    "# Harmonisation text_cleaned vs cleaned_text\n",
    "if 'cleaned_text' in df.columns and 'text_cleaned' not in df.columns:\n",
    "    df['text_cleaned'] = df['cleaned_text']\n",
    "    print(\"   Harmonisation: cleaned_text → text_cleaned\")\n",
    "elif 'text' in df.columns and 'text_cleaned' not in df.columns:\n",
    "    df['text_cleaned'] = df['text']\n",
    "    print(\"   Création: text → text_cleaned\")\n",
    "\n",
    "# Vérification des colonnes essentielles\n",
    "required_columns = ['title', 'text', 'source']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"   Colonnes manquantes: {missing_columns}\")\n",
    "else:\n",
    "    print(\"   Toutes les colonnes essentielles présentes\")\n",
    "\n",
    "print(f\"   Structure: {len(df)} articles, {len(df.columns)} colonnes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586148c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection des colonnes d'enrichissement déjà présentes\n",
    "enrichment_columns = {\n",
    "    'language': 'language' in df.columns and df['language'].notna().sum() > 0,\n",
    "    'entities': 'entities' in df.columns and df['entities'].notna().sum() > 0,\n",
    "    'quality_score': 'quality_score' in df.columns and df['quality_score'].notna().sum() > 0,\n",
    "    'embedding': 'embedding' in df.columns and df['embedding'].notna().sum() > 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb77985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   language: Présent (200 articles)\n",
      "   entities: Présent (200 articles)\n",
      "   quality_score: Présent (200 articles)\n",
      "   embedding: Présent (200 articles)\n",
      "\n",
      "MODE DÉTECTÉ: Preprocessing complémentaire avancé\n",
      "   → Focus: Déduplication, biais, corpus calibration, métriques avancées\n"
     ]
    }
   ],
   "source": [
    "for col, present in enrichment_columns.items():\n",
    "    status = \"Présent\" if present else \"Absent\"\n",
    "    count = df[col].notna().sum() if present else 0\n",
    "    print(f\"   {col}: {status} ({count} articles)\")\n",
    "\n",
    "# Adaptation de la stratégie\n",
    "if enrichment_columns['language'] and enrichment_columns['entities']:\n",
    "    print(f\"\\nMODE DÉTECTÉ: Preprocessing complémentaire avancé\")\n",
    "    print(f\"   → Focus: Déduplication, biais, corpus calibration, métriques avancées\")\n",
    "    SKIP_BASIC_ENRICHMENT = True\n",
    "else:\n",
    "    print(f\"\\nMODE DÉTECTÉ: Preprocessing complet depuis zéro\")\n",
    "    print(f\"   → Pipeline: Enrichissement + Analyses avancées\")\n",
    "    SKIP_BASIC_ENRICHMENT = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f12352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Textes déjà nettoyés détectés\n",
      "   Filtrage longueur minimum (100 chars): 200 → 200 articles\n"
     ]
    }
   ],
   "source": [
    "def clean_text_advanced(text):\n",
    "    \"\"\"Nettoyage robuste et avancé du texte\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Correction encodage\n",
    "    text = ftfy.fix_text(text)\n",
    "    # Suppression HTML résiduel\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Normalisation Unicode\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    # Suppression caractères de contrôle\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    # Normalisation espaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Suppression URLs et emails\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Suppression patterns RSS spécifiques\n",
    "    text = re.sub(r'#xtor=RSS-\\d+.*', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]$', '', text)  # Crédits en fin d'article\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Application du nettoyage si nécessaire\n",
    "if 'text_cleaned' not in df.columns or df['text_cleaned'].isna().any():\n",
    "    print(\"   Application du nettoyage avancé...\")\n",
    "    df['text_cleaned'] = df['text'].apply(clean_text_advanced)\n",
    "    print(f\"      {len(df)} textes nettoyés\")\n",
    "else:\n",
    "    print(\"   Textes déjà nettoyés détectés\")\n",
    "\n",
    "# Filtrage des articles trop courts ou vides\n",
    "min_length = 100  # caractères minimum\n",
    "df_clean = df[df['text_cleaned'].str.len() >= min_length].copy()\n",
    "print(f\"   Filtrage longueur minimum ({min_length} chars): {len(df)} → {len(df_clean)} articles\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eccbd9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Langues déjà détectées, validation des données...\n",
      "   Focus français: 130 articles sélectionnés\n"
     ]
    }
   ],
   "source": [
    "if not enrichment_columns['language'] or not SKIP_BASIC_ENRICHMENT:\n",
    "    print(\"   Détection de langue en cours...\")\n",
    "    def detect_language_robust(text):\n",
    "        \"\"\"Détection de langue avec fallback\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return 'unknown', 0.0\n",
    "        \n",
    "        try:\n",
    "            # langdetect avec probabilités\n",
    "            langs = detect_langs(text)\n",
    "            primary_lang = langs[0]\n",
    "            return primary_lang.lang, primary_lang.prob\n",
    "        except LangDetectException:\n",
    "            # Fallback: détection basique\n",
    "            try:\n",
    "                return detect(text), 0.5\n",
    "            except:\n",
    "                return 'unknown', 0.0\n",
    "\n",
    "    # Application de la détection\n",
    "    language_results = df_clean['text_cleaned'].apply(detect_language_robust)\n",
    "    df_clean['language'] = [result[0] for result in language_results]\n",
    "    df_clean['language_confidence'] = [result[1] for result in language_results]\n",
    "    \n",
    "    print(f\"   Détection de langue terminée\")\n",
    "else:\n",
    "    print(\"   Langues déjà détectées, validation des données...\")\n",
    "    if 'language_confidence' not in df_clean.columns:\n",
    "        df_clean['language_confidence'] = df_clean['language'].apply(lambda x: 0.9 if x == 'fr' else 0.7)\n",
    "\n",
    "# Sélection intelligente selon la distribution\n",
    "lang_counts = df_clean['language'].value_counts()\n",
    "if lang_counts.get('fr', 0) > len(df_clean) * 0.3:  # Si >30% en français\n",
    "    df_filtered = df_clean[df_clean['language'] == 'fr'].copy()\n",
    "    print(f\"   Focus français: {len(df_filtered)} articles sélectionnés\")\n",
    "else:\n",
    "    # Garder top 2 langues si pas assez de français\n",
    "    top_langs = lang_counts.head(2).index.tolist()\n",
    "    df_filtered = df_clean[df_clean['language'].isin(top_langs)].copy()\n",
    "    print(f\"   Multi-langues: {len(df_filtered)} articles ({top_langs})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68357aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Chargement du modèle sentence-transformers...\n",
      "      Modèle all-MiniLM-L6-v2 chargé avec succès\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle d'embeddings avec gestion d'erreur\n",
    "print(\"   Chargement du modèle sentence-transformers...\")\n",
    "try:\n",
    "    embeddings_model = SentenceTransformer(EMBEDDINGS_MODEL)\n",
    "    print(f\"      Modèle {EMBEDDINGS_MODEL} chargé avec succès\")\n",
    "except OSError as e:\n",
    "    if \"pagination\" in str(e) or \"1455\" in str(e):\n",
    "        print(f\"      Erreur mémoire détectée, utilisation d'un modèle plus léger...\")\n",
    "        # Fallback vers un modèle encore plus petit\n",
    "        EMBEDDINGS_MODEL_FALLBACK = \"all-MiniLM-L6-v2\"\n",
    "        embeddings_model = SentenceTransformer(EMBEDDINGS_MODEL_FALLBACK)\n",
    "        print(f\"      Modèle fallback {EMBEDDINGS_MODEL_FALLBACK} chargé\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "def deduplicate_semantic(df, threshold=0.85):\n",
    "    \"\"\"Déduplication sémantique avancée avec FAISS et correction du type\"\"\"\n",
    "    print(f\"   Génération des embeddings pour {len(df)} articles...\")\n",
    "    \n",
    "    # Utilisation des embeddings existants ou génération\n",
    "    if 'embedding' in df.columns and df['embedding'].notna().sum() > 0:\n",
    "        print(\"      Utilisation des embeddings existants\")\n",
    "        embeddings = []\n",
    "        for idx, emb in df['embedding'].items():\n",
    "            if isinstance(emb, (list, np.ndarray)) and len(emb) > 0:\n",
    "                embeddings.append(np.array(emb, dtype=np.float32))  # CORRECTION: forcer float32\n",
    "            else:\n",
    "                # Génération pour les embeddings manquants\n",
    "                text = df.loc[idx, 'text_cleaned']\n",
    "                new_emb = embeddings_model.encode(text)\n",
    "                embeddings.append(np.array(new_emb, dtype=np.float32))  # CORRECTION: forcer float32\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)  # CORRECTION: forcer float32\n",
    "    else:\n",
    "        print(\"      Génération des embeddings...\")\n",
    "        texts = df['text_cleaned'].tolist()\n",
    "        embeddings = embeddings_model.encode(texts, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)  # CORRECTION: forcer float32\n",
    "\n",
    "    # Configuration FAISS\n",
    "    print(\"   Configuration de l'index FAISS...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "    # CORRECTION: Normalisation avec vérification du type et copie\n",
    "    embeddings_normalized = embeddings.copy()  # Créer une copie\n",
    "    if not embeddings_normalized.flags['C_CONTIGUOUS']:\n",
    "        embeddings_normalized = np.ascontiguousarray(embeddings_normalized)  # Assurer la contiguïté\n",
    "\n",
    "    faiss.normalize_L2(embeddings_normalized)  # Normaliser la copie\n",
    "    index.add(embeddings_normalized)\n",
    "    \n",
    "    # Recherche des doublons\n",
    "    print(\"   Recherche des doublons sémantiques...\")\n",
    "    similarities, indices = index.search(embeddings_normalized, k=5)\n",
    "    \n",
    "    to_remove = set()\n",
    "    duplicate_pairs = []\n",
    "    \n",
    "    for i, (sim_scores, sim_indices) in enumerate(zip(similarities, indices)):\n",
    "        for j, (score, idx) in enumerate(zip(sim_scores, sim_indices)):\n",
    "            if j > 0 and score > threshold and idx not in to_remove and i not in to_remove:\n",
    "                # Garde le plus récent ou le mieux noté\n",
    "                if df.iloc[i].get('quality_score', 0) >= df.iloc[idx].get('quality_score', 0):\n",
    "                    to_remove.add(idx)\n",
    "                else:\n",
    "                    to_remove.add(i)\n",
    "                \n",
    "                duplicate_pairs.append((i, idx, score))\n",
    "    \n",
    "    # Suppression des doublons\n",
    "    df_dedup = df.drop(df.index[list(to_remove)]).copy()\n",
    "    \n",
    "    print(f\"   Résultats déduplication:\")\n",
    "    print(f\"      Articles originaux: {len(df)}\")\n",
    "    print(f\"      Doublons détectés: {len(to_remove)}\")\n",
    "    print(f\"      Articles finaux: {len(df_dedup)}\")\n",
    "    print(f\"      Taux de déduplication: {len(to_remove)/len(df)*100:.1f}%\")\n",
    "\n",
    "    return df_dedup, embeddings_normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdcb16bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Génération des embeddings pour 130 articles...\n",
      "      Utilisation des embeddings existants\n",
      "   Configuration de l'index FAISS...\n",
      "   Recherche des doublons sémantiques...\n",
      "   Résultats déduplication:\n",
      "      Articles originaux: 130\n",
      "      Doublons détectés: 2\n",
      "      Articles finaux: 128\n",
      "      Taux de déduplication: 1.5%\n"
     ]
    }
   ],
   "source": [
    "# Application de la déduplication\n",
    "df_clean_dedup, article_embeddings = deduplicate_semantic(df_filtered, SIMILARITY_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ac4cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Conversion et amélioration des entités existantes...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not enrichment_columns['entities'] or not SKIP_BASIC_ENRICHMENT:\n",
    "    print(\"   Extraction complète des entités avec spaCy...\")\n",
    "    \n",
    "    # Chargement du modèle spaCy français\n",
    "    print(f\"      Chargement du modèle spaCy: {NLP_MODEL}\")\n",
    "    try:\n",
    "        nlp = spacy.load(NLP_MODEL)\n",
    "    except OSError:\n",
    "        print(f\"      Modèle {NLP_MODEL} non trouvé. Installation...\")\n",
    "        import subprocess\n",
    "        subprocess.run(f\"python -m spacy download {NLP_MODEL}\", shell=True)\n",
    "        nlp = spacy.load(NLP_MODEL)\n",
    "\n",
    "    def extract_entities_advanced(text, nlp_model):\n",
    "        \"\"\"Extraction d'entités avec enrichissements\"\"\"\n",
    "        if not text or len(text) < 50:\n",
    "            return {\n",
    "                'persons': [], 'organizations': [], 'locations': [],\n",
    "                'dates': [], 'money': [], 'misc': []\n",
    "            }\n",
    "        \n",
    "        # Traitement avec spaCy (limiter la longueur pour performance)\n",
    "        doc = nlp_model(text[:8000])  # Premier 8k caractères\n",
    "        \n",
    "        entities = {\n",
    "            'persons': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "            'dates': [],\n",
    "            'money': [],\n",
    "            'misc': []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_text = ent.text.strip()\n",
    "            if len(entity_text) < 2:  # Ignorer entités trop courtes\n",
    "                continue\n",
    "                \n",
    "            if ent.label_ in ['PERSON']:\n",
    "                entities['persons'].append(entity_text)\n",
    "            elif ent.label_ in ['ORG']:\n",
    "                entities['organizations'].append(entity_text)\n",
    "            elif ent.label_ in ['GPE', 'LOC']:\n",
    "                entities['locations'].append(entity_text)\n",
    "            elif ent.label_ in ['DATE', 'TIME']:\n",
    "                entities['dates'].append(entity_text)\n",
    "            elif ent.label_ in ['MONEY']:\n",
    "                entities['money'].append(entity_text)\n",
    "            else:\n",
    "                entities['misc'].append(entity_text)\n",
    "        \n",
    "        # Déduplication et nettoyage\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))  # Suppression doublons\n",
    "            entities[key] = [e for e in entities[key] if len(e) > 1]  # Filtrage longueur\n",
    "        \n",
    "        return entities\n",
    "\n",
    "    # Application de l'extraction d'entités\n",
    "    entities_results = []\n",
    "    for text in tqdm(df_clean_dedup['text_cleaned'], desc=\"Extraction NER\"):\n",
    "        entities = extract_entities_advanced(text, nlp)\n",
    "        entities_results.append(entities)\n",
    "\n",
    "    # Ajout des résultats au DataFrame\n",
    "    df_clean_dedup['entities_advanced'] = entities_results\n",
    "    \n",
    "    print(f\"      Extraction NER terminée\")\n",
    "else:\n",
    "    print(\"   Conversion et amélioration des entités existantes...\")\n",
    "    def convert_and_improve_entities(existing_entities):\n",
    "        \"\"\"CONVERSION du format existant + amélioration\"\"\"\n",
    "        if not existing_entities or not isinstance(existing_entities, dict):\n",
    "            return {\n",
    "                'persons': [], 'organizations': [], 'locations': [],\n",
    "                'dates': [], 'money': [], 'misc': []\n",
    "            }\n",
    "        \n",
    "        # CONVERSION DES FORMATS\n",
    "        converted = {\n",
    "            'persons': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "            'dates': [],\n",
    "            'money': [],\n",
    "            'misc': []\n",
    "        }\n",
    "        \n",
    "        # Mapping des anciens noms vers les nouveaux\n",
    "        field_mapping = {\n",
    "            'PER': 'persons',           # PER → persons\n",
    "            'PERSON': 'persons',        # PERSON → persons  \n",
    "            'ORG': 'organizations',     # ORG → organizations\n",
    "            'LOC': 'locations',         # LOC → locations\n",
    "            'GPE': 'locations',         # GPE → locations (entités géopolitiques)\n",
    "            'MISC': 'misc',             # MISC → misc\n",
    "            'DATE': 'dates',            # DATE → dates\n",
    "            'TIME': 'dates',            # TIME → dates\n",
    "            'MONEY': 'money'            # MONEY → money\n",
    "        }\n",
    "        \n",
    "        # Conversion avec mapping\n",
    "        for old_key, entity_list in existing_entities.items():\n",
    "            if old_key in field_mapping:\n",
    "                new_key = field_mapping[old_key]\n",
    "                if isinstance(entity_list, list):\n",
    "                    # Nettoyage des entités\n",
    "                    cleaned = [str(e).strip() for e in entity_list if e and len(str(e)) > 1]\n",
    "                    # Déduplication case-insensitive\n",
    "                    seen = set()\n",
    "                    for entity in cleaned:\n",
    "                        if entity.lower() not in seen:\n",
    "                            converted[new_key].append(entity)\n",
    "                            seen.add(entity.lower())\n",
    "        \n",
    "        return converted\n",
    "\n",
    "    # Application de la conversion\n",
    "    df_clean_dedup['entities_advanced'] = df_clean_dedup['entities'].apply(convert_and_improve_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5af3391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Résumé entités: 23.1 entités/article en moyenne\n"
     ]
    }
   ],
   "source": [
    "# Création de colonnes métriques enrichies\n",
    "df_clean_dedup['persons_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('persons', [])))\n",
    "df_clean_dedup['organizations_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('organizations', [])))\n",
    "df_clean_dedup['locations_count'] = df_clean_dedup['entities_advanced'].apply(lambda x: len(x.get('locations', [])))\n",
    "df_clean_dedup['entities_total'] = (df_clean_dedup['persons_count'] + \n",
    "                                   df_clean_dedup['organizations_count'] +\n",
    "                                   df_clean_dedup['locations_count'])\n",
    "\n",
    "print(f\"   Résumé entités: {df_clean_dedup['entities_total'].mean():.1f} entités/article en moyenne\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27fd3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des ressources NLTK si nécessaire\n",
    "try:\n",
    "    import ssl\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "except:\n",
    "    pass\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def segment_text_advanced(text):\n",
    "    \"\"\"Segmentation en phrases avec analyse sémantique avancée\"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return {\n",
    "            'sentences': [],\n",
    "            'sentence_count': 0,\n",
    "            'avg_sentence_length': 0,\n",
    "            'paragraphs': [],\n",
    "            'paragraph_count': 0,\n",
    "            'text_complexity': 0,\n",
    "            'readability_score': 0\n",
    "        }\n",
    "    \n",
    "    # Segmentation en phrases (multi-langue)\n",
    "    sentences = sent_tokenize(text[:5000], language='french')  # Limiter pour performance\n",
    "    \n",
    "    # Segmentation en paragraphes\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip() and len(p) > 20]\n",
    "\n",
    "    # Métriques avancées\n",
    "    sentence_lengths = [len(s.split()) for s in sentences]\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Score de complexité basé sur la longueur des phrases\n",
    "    complexity = 0\n",
    "    if sentence_lengths:\n",
    "        variance = np.var(sentence_lengths)\n",
    "        long_sentences = sum(1 for length in sentence_lengths if length > 20)\n",
    "        complexity = min((variance / 100) + (long_sentences / len(sentences)), 1.0)\n",
    "\n",
    "    # Score de lisibilité approximatif (Flesch-like)\n",
    "    if sentence_lengths and avg_sentence_length > 0:\n",
    "        readability = max(0, min(1, 1 - (avg_sentence_length - 10) / 20))\n",
    "    else:\n",
    "        readability = 0.5\n",
    "\n",
    "    return {\n",
    "        'sentences': sentences[:50],  # Limiter pour stockage\n",
    "        'sentence_count': len(sentences),\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'paragraphs': paragraphs[:20],  # Limiter pour stockage\n",
    "        'paragraph_count': len(paragraphs),\n",
    "        'text_complexity': complexity,\n",
    "        'readability_score': readability\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59dbaa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Segmentation  en cours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmentation: 100%|██████████| 128/128 [00:00<00:00, 801.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Application de la segmentation\n",
    "print(\"   Segmentation  en cours...\")\n",
    "segmentation_results = []\n",
    "for text in tqdm(df_clean_dedup['text_cleaned'], desc=\"Segmentation\"):\n",
    "    segments = segment_text_advanced(text)\n",
    "    segmentation_results.append(segments)\n",
    "\n",
    "df_clean_dedup['segmentation'] = segmentation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa53e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Résumé segmentation: 29.7 phrases/article\n"
     ]
    }
   ],
   "source": [
    "# Extraction des métriques de segmentation\n",
    "df_clean_dedup['sentence_count'] = df_clean_dedup['segmentation'].apply(lambda x: x['sentence_count'])\n",
    "df_clean_dedup['paragraph_count'] = df_clean_dedup['segmentation'].apply(lambda x: x['paragraph_count'])\n",
    "df_clean_dedup['avg_sentence_length'] = df_clean_dedup['segmentation'].apply(lambda x: x['avg_sentence_length'])\n",
    "df_clean_dedup['text_complexity'] = df_clean_dedup['segmentation'].apply(lambda x: x['text_complexity'])\n",
    "df_clean_dedup['readability_score'] = df_clean_dedup['segmentation'].apply(lambda x: x['readability_score'])\n",
    "\n",
    "print(f\"   Résumé segmentation: {df_clean_dedup['sentence_count'].mean():.1f} phrases/article\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7000f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Analyse temporelle...\n"
     ]
    }
   ],
   "source": [
    "# Conversion et nettoyage des dates\n",
    "print(\"   Analyse temporelle...\")\n",
    "df_clean_dedup['published_clean'] = pd.to_datetime(df_clean_dedup['published'], errors='coerce')\n",
    "\n",
    "# Extraction des composants temporels\n",
    "df_clean_dedup['hour'] = df_clean_dedup['published_clean'].dt.hour\n",
    "df_clean_dedup['day_of_week'] = df_clean_dedup['published_clean'].dt.day_name()\n",
    "df_clean_dedup['month'] = df_clean_dedup['published_clean'].dt.month\n",
    "df_clean_dedup['date_only'] = df_clean_dedup['published_clean'].dt.date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceae490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Biais temporel détecté: 0.81 (horaire), 0.50 (quotidien)\n"
     ]
    }
   ],
   "source": [
    "# Analyse des biais temporels\n",
    "valid_dates = df_clean_dedup[df_clean_dedup['published_clean'].notna()]\n",
    "if len(valid_dates) > 0:\n",
    "    # Distribution horaire\n",
    "    hour_dist = valid_dates['hour'].value_counts().head(3)\n",
    "    # Distribution par jour\n",
    "    day_dist = valid_dates['day_of_week'].value_counts().head(3)\n",
    "    \n",
    "    # Calcul du score de biais temporel\n",
    "    hour_entropy = -sum((p := hour_dist / len(valid_dates)) * np.log2(p + 1e-10))\n",
    "    day_entropy = -sum((p := day_dist / len(valid_dates)) * np.log2(p + 1e-10))\n",
    "    \n",
    "    # Normalisation (entropie max = log2(24) pour heures, log2(7) pour jours)\n",
    "    hour_bias = 1 - (hour_entropy / np.log2(24))  # 0 = uniforme, 1 = très biaisé\n",
    "    day_bias = 1 - (day_entropy / np.log2(7))\n",
    "    \n",
    "    print(f\"      Biais temporel détecté: {hour_bias:.2f} (horaire), {day_bias:.2f} (quotidien)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d483b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Analyse géographique...\n",
      "      Biais géographique détecté: -0.57\n"
     ]
    }
   ],
   "source": [
    "# Analyse géographique via entités lieux\n",
    "print(\"   Analyse géographique...\")\n",
    "all_locations = []\n",
    "location_counts_by_article = []\n",
    "for entities in df_clean_dedup['entities_advanced']:\n",
    "    article_locations = entities.get('locations', [])\n",
    "    location_counts_by_article.append(len(article_locations))\n",
    "    all_locations.extend(article_locations)\n",
    "\n",
    "location_distribution = Counter(all_locations)\n",
    "df_clean_dedup['locations_count'] = location_counts_by_article\n",
    "\n",
    "if location_distribution:\n",
    "    # Score de biais géographique\n",
    "    if len(location_distribution) > 1:\n",
    "        geo_probs = np.array(list(location_distribution.values())) / len(all_locations)\n",
    "        geo_entropy = -sum(geo_probs * np.log2(geo_probs + 1e-10))\n",
    "        max_entropy = np.log2(min(len(location_distribution), 50))  # Entropie max théorique\n",
    "        geo_bias = 1 - (geo_entropy / max_entropy) if max_entropy > 0 else 0\n",
    "        print(f\"      Biais géographique détecté: {geo_bias:.2f}\")\n",
    "    else:\n",
    "        geo_bias = 1.0  # Maximum bias if only one location\n",
    "        print(f\"      Biais géographique maximal détecté\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06de1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des scores de biais au DataFrame\n",
    "df_clean_dedup['temporal_bias_hour'] = hour_bias if 'hour_bias' in locals() else 0\n",
    "df_clean_dedup['temporal_bias_day'] = day_bias if 'day_bias' in locals() else 0\n",
    "df_clean_dedup['geographic_bias'] = geo_bias if 'geo_bias' in locals() else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d2504fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_quality_score(row):\n",
    "    \"\"\"Calcul d'un score de qualité multi-dimensionnel\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # 1. Score de longueur (0-1)\n",
    "    text_len = len(str(row.get('text_cleaned', '')))\n",
    "    scores['length'] = min(text_len / 2000, 1.0)  # Optimal à 2000 caractères\n",
    "    \n",
    "    # 2. Score d'entités (0-1)\n",
    "    entities_count = row.get('entities_total', 0)\n",
    "    scores['entities'] = min(entities_count / 10, 1.0)  # Optimal à 10 entités\n",
    "    \n",
    "    # 3. Score de lisibilité (0-1)\n",
    "    scores['readability'] = row.get('readability_score', 0.5)\n",
    "    \n",
    "    # 4. Score de complexité inversé (0-1)\n",
    "    complexity = row.get('text_complexity', 0.5)\n",
    "    scores['complexity'] = 1 - complexity  # Moins complexe = meilleur\n",
    "    \n",
    "    # 5. Score de structure (0-1)\n",
    "    sentence_count = row.get('sentence_count', 0)\n",
    "    paragraph_count = row.get('paragraph_count', 0)\n",
    "    if sentence_count > 0 and paragraph_count > 0:\n",
    "        structure_ratio = min(sentence_count / paragraph_count, 10) / 10  # Ratio phrases/paragraphes\n",
    "        scores['structure'] = structure_ratio\n",
    "    else:\n",
    "        scores['structure'] = 0.1\n",
    "        \n",
    "    # 6. Score de langue (0-1)\n",
    "    lang_confidence = row.get('language_confidence', 0.5)\n",
    "    scores['language'] = lang_confidence\n",
    "    \n",
    "    # Score global pondéré\n",
    "    weights = {\n",
    "        'length': 0.2,\n",
    "        'entities': 0.25, \n",
    "        'readability': 0.2,\n",
    "        'complexity': 0.15,\n",
    "        'structure': 0.1,\n",
    "        'language': 0.1\n",
    "    }\n",
    "    \n",
    "    final_score = sum(scores[key] * weights[key] for key in scores)\n",
    "    \n",
    "    return {\n",
    "        'quality_score_advanced': final_score,\n",
    "        'quality_breakdown': scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e0eeab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Calcul des scores de qualité...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qualité: 100%|██████████| 128/128 [00:00<00:00, 7533.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score qualité moyen: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Application du calcul de qualité\n",
    "print(\"   Calcul des scores de qualité...\")\n",
    "quality_results = []\n",
    "for _, row in tqdm(df_clean_dedup.iterrows(), total=len(df_clean_dedup), desc=\"Qualité\"):\n",
    "    quality_result = calculate_advanced_quality_score(row)\n",
    "    quality_results.append(quality_result)\n",
    "\n",
    "# Ajout des résultats\n",
    "df_clean_dedup['quality_score_advanced'] = [r['quality_score_advanced'] for r in quality_results]\n",
    "df_clean_dedup['quality_breakdown'] = [r['quality_breakdown'] for r in quality_results]\n",
    "\n",
    "print(f\"   Score qualité moyen: {df_clean_dedup['quality_score_advanced'].mean():.3f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5686e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_calibration_corpus(df, n_samples=300):\n",
    "    \"\"\"Création d'un corpus stratifié pour calibration\"\"\"\n",
    "    \n",
    "    # Définition des strates multi-dimensionnelles\n",
    "    print(\"   Définition des strates...\")\n",
    "    \n",
    "    # 1. Strate par qualité (3 niveaux)\n",
    "    quality_tertiles = df['quality_score_advanced'].quantile([0.33, 0.67])\n",
    "    df['quality_stratum'] = pd.cut(df['quality_score_advanced'], \n",
    "                                  bins=[0, quality_tertiles[0.33], quality_tertiles[0.67], 1],\n",
    "                                  labels=['low', 'medium', 'high'])\n",
    "    \n",
    "    # 2. Strate par longueur (3 niveaux)\n",
    "    df['text_length'] = df['text_cleaned'].str.len()\n",
    "    length_tertiles = df['text_length'].quantile([0.33, 0.67])\n",
    "    df['length_stratum'] = pd.cut(df['text_length'],\n",
    "                                 bins=[0, length_tertiles[0.33], length_tertiles[0.67], float('inf')],\n",
    "                                 labels=['short', 'medium', 'long'])\n",
    "    \n",
    "    # 3. Strate par richesse en entités (3 niveaux)\n",
    "    if df['entities_total'].max() > 0:\n",
    "        entity_tertiles = df['entities_total'].quantile([0.33, 0.67])\n",
    "        df['entity_stratum'] = pd.cut(df['entities_total'],\n",
    "                                     bins=[-1, entity_tertiles[0.33], entity_tertiles[0.67], float('inf')],\n",
    "                                     labels=['sparse', 'moderate', 'rich'])\n",
    "    else:\n",
    "        df['entity_stratum'] = 'sparse'\n",
    "        \n",
    "    # 4. Strate par source (top sources + autres)\n",
    "    source_counts = df['source'].value_counts()\n",
    "    top_sources = source_counts.head(5).index.tolist()\n",
    "    df['source_stratum'] = df['source'].apply(lambda x: x if x in top_sources else 'other')\n",
    "    \n",
    "    # Échantillonnage stratifié proportionnel\n",
    "    print(\"   Échantillonnage stratifié...\")\n",
    "    \n",
    "    # Groupement par strates multiples\n",
    "    strata_cols = ['quality_stratum', 'length_stratum', 'entity_stratum', 'source_stratum']\n",
    "    grouped = df.groupby(strata_cols, group_keys=False)\n",
    "    \n",
    "    # Calcul des tailles d'échantillon par strate\n",
    "    strata_sizes = grouped.size()\n",
    "    total_size = len(df)\n",
    "    \n",
    "    sample_dfs = []\n",
    "    remaining_samples = n_samples\n",
    "    \n",
    "    for stratum, group in grouped:\n",
    "        if remaining_samples <= 0:\n",
    "            break\n",
    "            \n",
    "        # Taille proportionnelle de l'échantillon pour cette strate\n",
    "        stratum_size = len(group)\n",
    "        proportion = stratum_size / total_size\n",
    "        target_sample_size = max(1, int(proportion * n_samples))\n",
    "        \n",
    "        # Ajustement si on dépasse le nombre d'échantillons restants\n",
    "        actual_sample_size = min(target_sample_size, remaining_samples, stratum_size)\n",
    "        \n",
    "        if actual_sample_size > 0:\n",
    "            # Échantillonnage au sein de la strate\n",
    "            if len(group) >= actual_sample_size:\n",
    "                # Tri par score de qualité pour prendre les meilleurs\n",
    "                group_sorted = group.sort_values('quality_score_advanced', ascending=False)\n",
    "                stratum_sample = group_sorted.head(actual_sample_size)\n",
    "                sample_dfs.append(stratum_sample)\n",
    "                remaining_samples -= actual_sample_size\n",
    "                \n",
    "    # Combinaison des échantillons de toutes les strates\n",
    "    if sample_dfs:\n",
    "        calibration_corpus = pd.concat(sample_dfs, ignore_index=True)\n",
    "    else:\n",
    "        # Fallback: échantillonnage simple par qualité\n",
    "        calibration_corpus = df.nlargest(n_samples, 'quality_score_advanced')\n",
    "        \n",
    "    # Complément aléatoire si nécessaire\n",
    "    if len(calibration_corpus) < n_samples:\n",
    "        remaining_df = df[~df.index.isin(calibration_corpus.index)]\n",
    "        if len(remaining_df) > 0:\n",
    "            additional_samples = min(n_samples - len(calibration_corpus), len(remaining_df))\n",
    "            additional = remaining_df.sample(n=additional_samples, random_state=42)\n",
    "            calibration_corpus = pd.concat([calibration_corpus, additional], ignore_index=True)\n",
    "            \n",
    "    return calibration_corpus.head(n_samples)  # S'assurer qu'on ne dépasse pas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d1e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Définition des strates...\n",
      "   Échantillonnage stratifié...\n",
      "   CORPUS DE CALIBRATION CRÉÉ:\n",
      "      Taille finale: 186 articles\n",
      "      Score qualité moyen: 0.679\n"
     ]
    }
   ],
   "source": [
    "# Création du corpus de calibration\n",
    "calibration_corpus = create_stratified_calibration_corpus(df_clean_dedup, n_samples=300)\n",
    "\n",
    "print(f\"   CORPUS DE CALIBRATION CRÉÉ:\")\n",
    "print(f\"      Taille finale: {len(calibration_corpus)} articles\")\n",
    "print(f\"      Score qualité moyen: {calibration_corpus['quality_score_advanced'].mean():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec826d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métriques de qualité finales\n",
    "quality_metrics = {\n",
    "    'source_file': str(source_file.name),\n",
    "    'enriched_mode': ENRICHED_MODE,\n",
    "    'total_articles_input': len(articles_data),\n",
    "    'articles_after_deduplication': len(df_clean_dedup),\n",
    "    'calibration_corpus_size': len(calibration_corpus),\n",
    "    'deduplication_rate': ((len(df) - len(df_clean_dedup)) / len(df)) if len(df) > 0 else 0,\n",
    "    'avg_quality_score': df_clean_dedup['quality_score_advanced'].mean(),\n",
    "    'language_distribution': df_clean_dedup['language'].value_counts().to_dict(),\n",
    "    'entities_avg_per_article': df_clean_dedup['entities_total'].mean(),\n",
    "    'temporal_bias_detected': df_clean_dedup['temporal_bias_hour'].iloc[0] if len(df_clean_dedup) > 0 else 0,\n",
    "    'geographic_bias_detected': df_clean_dedup['geographic_bias'].iloc[0] if len(df_clean_dedup) > 0 else 0,\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfaa244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset principal sauvegardé: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\articles_preprocessed_advanced.pkl\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du DataFrame principal (format optimisé)\n",
    "output_file = PROCESSED_DIR / \"articles_preprocessed_advanced.pkl\"\n",
    "df_clean_dedup.to_pickle(output_file)\n",
    "print(f\"   Dataset principal sauvegardé: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e3705eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Corpus de calibration sauvegardé: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\calibration_corpus_stratified.pkl\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du corpus de calibration\n",
    "calibration_file = PROCESSED_DIR / \"calibration_corpus_stratified.pkl\"\n",
    "calibration_corpus.to_pickle(calibration_file)\n",
    "print(f\"   Corpus de calibration sauvegardé: {calibration_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "335f8dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Métriques sauvegardées: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\advanced_preprocessing_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des métriques\n",
    "metrics_file = PROCESSED_DIR / \"advanced_preprocessing_metrics.json\"\n",
    "with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(quality_metrics, f, indent=2, ensure_ascii=False, default=str)\n",
    "print(f\"   Métriques sauvegardées: {metrics_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "874023e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Export CSV résumé: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\articles_preprocessed_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Export CSV léger pour analyse externe\n",
    "csv_file = PROCESSED_DIR / \"articles_preprocessed_summary.csv\"\n",
    "df_export = df_clean_dedup[[\n",
    "    'title', 'source', 'published', 'language', 'quality_score_advanced',\n",
    "    'entities_total', 'sentence_count', 'readability_score', 'text_complexity'\n",
    "]].copy()\n",
    "df_export.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "print(f\"   Export CSV résumé: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8a73751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Corpus calibration JSON: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\calibration_corpus_300.json\n"
     ]
    }
   ],
   "source": [
    "# Export JSON du corpus de calibration (pour Phase 3)\n",
    "calibration_json = PROCESSED_DIR / \"calibration_corpus_300.json\"\n",
    "calibration_export = calibration_corpus[[\n",
    "    'id', 'title', 'text_cleaned', 'source', 'published', 'language',\n",
    "    'quality_score_advanced', 'entities_advanced'\n",
    "]].to_dict('records')\n",
    "\n",
    "with open(calibration_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(calibration_export, f, ensure_ascii=False, indent=2, default=str)\n",
    "print(f\"   Corpus calibration JSON: {calibration_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "818971e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RÉSULTATS FINAUX:\n",
      "   Source: enriched_articles.json\n",
      "   Mode: Enrichissement complémentaire\n",
      "   Articles traités: 200\n",
      "   Articles finaux: 128\n",
      "   Corpus de calibration: 186\n",
      "   Taux de déduplication: 36.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"RÉSULTATS FINAUX:\")\n",
    "print(f\"   Source: {quality_metrics['source_file']}\")\n",
    "print(f\"   Mode: {'Enrichissement complémentaire' if ENRICHED_MODE else 'Pipeline complet'}\")\n",
    "print(f\"   Articles traités: {quality_metrics['total_articles_input']}\")\n",
    "print(f\"   Articles finaux: {quality_metrics['articles_after_deduplication']}\")\n",
    "print(f\"   Corpus de calibration: {quality_metrics['calibration_corpus_size']}\")\n",
    "print(f\"   Taux de déduplication: {quality_metrics['deduplication_rate']:.1%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e394572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÉTRIQUES DE QUALITÉ:\n",
      "   Score qualité moyen: 0.681\n",
      "   Entités par article: 23.1\n",
      "   Biais temporel détecté: 0.81\n",
      "   Biais géographique: -0.57\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMÉTRIQUES DE QUALITÉ:\")\n",
    "print(f\"   Score qualité moyen: {quality_metrics['avg_quality_score']:.3f}\")\n",
    "print(f\"   Entités par article: {quality_metrics['entities_avg_per_article']:.1f}\")\n",
    "print(f\"   Biais temporel détecté: {quality_metrics['temporal_bias_detected']:.2f}\")\n",
    "print(f\"   Biais géographique: {quality_metrics['geographic_bias_detected']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8e744e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FICHIERS GÉNÉRÉS:\n",
      "   1. articles_preprocessed_advanced.pkl - Dataset principal avec preprocessing avancé\n",
      "   2. calibration_corpus_stratified.pkl - Corpus stratifié pour calibration\n",
      "   3. advanced_preprocessing_metrics.json - Métriques détaillées\n",
      "   4. articles_preprocessed_summary.csv - Export CSV pour analyse\n",
      "   5. calibration_corpus_300.json - Corpus JSON pour Phase 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFICHIERS GÉNÉRÉS:\")\n",
    "print(f\"   1. {output_file.name} - Dataset principal avec preprocessing avancé\")\n",
    "print(f\"   2. {calibration_file.name} - Corpus stratifié pour calibration\")\n",
    "print(f\"   3. {metrics_file.name} - Métriques détaillées\")\n",
    "print(f\"   4. {csv_file.name} - Export CSV pour analyse\")\n",
    "print(f\"   5. {calibration_json.name} - Corpus JSON pour Phase 3\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9d909fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VARIABLES DISPONIBLES POUR EXPORT:\n",
      "   - df_clean_dedup: DataFrame principal preprocessé\n",
      "   - calibration_corpus: Corpus de calibration\n",
      "   - quality_metrics: Métriques de qualité\n",
      "   - PROCESSED_DIR: Répertoire des données traitées\n"
     ]
    }
   ],
   "source": [
    "# Variables exportées pour les autres notebooks\n",
    "print(f\"\\nVARIABLES DISPONIBLES POUR EXPORT:\")\n",
    "print(f\"   - df_clean_dedup: DataFrame principal preprocessé\")\n",
    "print(f\"   - calibration_corpus: Corpus de calibration\")\n",
    "print(f\"   - quality_metrics: Métriques de qualité\")\n",
    "print(f\"   - PROCESSED_DIR: Répertoire des données traitées\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d777d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Variables sauvegardées: preprocessing_variables.pkl\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des variables principales pour les autres notebooks\n",
    "import pickle\n",
    "variables_export = {\n",
    "    'df_clean_dedup': df_clean_dedup,\n",
    "    'calibration_corpus': calibration_corpus,\n",
    "    'quality_metrics': quality_metrics,\n",
    "    'PROCESSED_DIR': PROCESSED_DIR\n",
    "}\n",
    "with open(PROCESSED_DIR / 'preprocessing_variables.pkl', 'wb') as f:\n",
    "    pickle.dump(variables_export, f)\n",
    "print(f\"   Variables sauvegardées: preprocessing_variables.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
