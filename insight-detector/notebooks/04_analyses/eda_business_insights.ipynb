{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76c92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload pour développement interactif\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Analyses avancées\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c9e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path().resolve().parent.parent\n",
    "sys.path.append(str(BASE_DIR / \"src\"))\n",
    "\n",
    "# Répertoires\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f7ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées:\n",
      "   Dataset principal: 128 articles\n",
      "   Corpus calibration: 186 articles\n",
      "   Période: 2025-07-27T10:08:32.511773\n"
     ]
    }
   ],
   "source": [
    "# Chargement des variables du preprocessing\n",
    "try:\n",
    "    with open(PROCESSED_DIR / 'preprocessing_variables.pkl', 'rb') as f:\n",
    "        variables = pickle.load(f)\n",
    "    \n",
    "    df_clean_dedup = variables['df_clean_dedup']\n",
    "    calibration_corpus = variables['calibration_corpus']\n",
    "    quality_metrics = variables['quality_metrics']\n",
    "    \n",
    "    print(f\"Données chargées:\")\n",
    "    print(f\"   Dataset principal: {len(df_clean_dedup)} articles\")\n",
    "    print(f\"   Corpus calibration: {len(calibration_corpus)} articles\")\n",
    "    print(f\"   Période: {quality_metrics.get('processing_timestamp', 'N/A')}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Fichier preprocessing_variables.pkl non trouvé!\")\n",
    "    print(\"   → Exécutez d'abord preprocessing_advanced.ipynb\")\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1575bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_source_reliability(df):\n",
    "    \"\"\"Analyse la fiabilité et qualité des sources\"\"\"\n",
    "    \n",
    "    # Métriques par source\n",
    "    source_metrics = df.groupby('source').agg({\n",
    "        'quality_score_advanced': ['mean', 'std', 'count'],\n",
    "        'entities_total': 'mean',\n",
    "        'text_cleaned': lambda x: x.str.len().mean(),\n",
    "        'readability_score': 'mean',\n",
    "        'text_complexity': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Aplatir les colonnes multi-niveaux\n",
    "    source_metrics.columns = ['_'.join(col).strip() for col in source_metrics.columns]\n",
    "    source_metrics = source_metrics.reset_index()\n",
    "    \n",
    "    # Calcul du score de fiabilité composite\n",
    "    source_metrics['reliability_score'] = (\n",
    "        source_metrics['quality_score_advanced_mean'] * 0.4 +\n",
    "        (1 - source_metrics['text_complexity_mean']) * 0.3 +\n",
    "        source_metrics['readability_score_mean'] * 0.3\n",
    "    )\n",
    "    \n",
    "    # Filtrer les sources avec au moins 3 articles\n",
    "    reliable_sources = source_metrics[source_metrics['quality_score_advanced_count'] >= 3]\n",
    "    reliable_sources = reliable_sources.sort_values('reliability_score', ascending=False)\n",
    "    \n",
    "    return reliable_sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16695a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 SOURCES LES PLUS FIABLES:\n",
      "   titres.rss\n",
      "       Score fiabilité: 0.435\n",
      "       Qualité moyenne: 0.728\n",
      "       Articles: 13\n",
      "       Entités/article: 36.2\n",
      "\n",
      "   actualites.xml\n",
      "       Score fiabilité: 0.403\n",
      "       Qualité moyenne: 0.688\n",
      "       Articles: 21\n",
      "       Entités/article: 16.1\n",
      "\n",
      "   \n",
      "       Score fiabilité: 0.388\n",
      "       Qualité moyenne: 0.679\n",
      "       Articles: 8\n",
      "       Entités/article: 10.6\n",
      "\n",
      "   feed\n",
      "       Score fiabilité: 0.378\n",
      "       Qualité moyenne: 0.679\n",
      "       Articles: 15\n",
      "       Entités/article: 12.9\n",
      "\n",
      "   \n",
      "       Score fiabilité: 0.369\n",
      "       Qualité moyenne: 0.671\n",
      "       Articles: 13\n",
      "       Entités/article: 13.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Application de l'analyse\n",
    "source_analysis = analyze_source_reliability(df_clean_dedup)\n",
    "\n",
    "print(f\"TOP 5 SOURCES LES PLUS FIABLES:\")\n",
    "for idx, row in source_analysis.head().iterrows():\n",
    "    source_name = row['source'].split('/')[-1][:30]\n",
    "    print(f\"   {source_name}\")\n",
    "    print(f\"       Score fiabilité: {row['reliability_score']:.3f}\")\n",
    "    print(f\"       Qualité moyenne: {row['quality_score_advanced_mean']:.3f}\")\n",
    "    print(f\"       Articles: {row['quality_score_advanced_count']:.0f}\")\n",
    "    print(f\"       Entités/article: {row['entities_total_mean']:.1f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81d0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCES À SURVEILLER (faible fiabilité):\n",
      "   index.xml: Score 0.354\n",
      "   rss: Score 0.326\n",
      "   rss.xml: Score 0.301\n"
     ]
    }
   ],
   "source": [
    "print(f\"SOURCES À SURVEILLER (faible fiabilité):\")\n",
    "for idx, row in source_analysis.tail(3).iterrows():\n",
    "    source_name = row['source'].split('/')[-1][:30]\n",
    "    print(f\"   {source_name}: Score {row['reliability_score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eef4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSIGHTS SOURCES:\n",
      "   9 sources actives identifiées\n",
      "   Sources premium (score >0.7): 0\n",
      "   Sources à améliorer (score <0.5): 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nINSIGHTS SOURCES:\")\n",
    "print(f\"   {len(source_analysis)} sources actives identifiées\")\n",
    "print(f\"   Sources premium (score >0.7): {(source_analysis['reliability_score'] > 0.7).sum()}\")\n",
    "print(f\"   Sources à améliorer (score <0.5): {(source_analysis['reliability_score'] < 0.5).sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1827fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_temporal_analysis(df):\n",
    "    \"\"\"Analyse temporelle poussée avec détection de patterns\"\"\"\n",
    "    \n",
    "    # Préparation données temporelles\n",
    "    df_temp = df.copy()\n",
    "    df_temp['published_clean'] = pd.to_datetime(df_temp['published'], errors='coerce')\n",
    "    df_temp = df_temp.dropna(subset=['published_clean'])\n",
    "    \n",
    "    # Extraction composants temporels\n",
    "    df_temp['hour'] = df_temp['published_clean'].dt.hour\n",
    "    df_temp['day_week'] = df_temp['published_clean'].dt.day_name()\n",
    "    df_temp['day_month'] = df_temp['published_clean'].dt.day\n",
    "    df_temp['week_year'] = df_temp['published_clean'].dt.isocalendar().week\n",
    "    \n",
    "    # Détection pics d'actualité\n",
    "    daily_counts = df_temp.groupby(df_temp['published_clean'].dt.date).size()\n",
    "    mean_daily = daily_counts.mean()\n",
    "    std_daily = daily_counts.std()\n",
    "    \n",
    "    # Pics = jours avec +2 écarts-types\n",
    "    peak_threshold = mean_daily + 2 * std_daily\n",
    "    peak_days = daily_counts[daily_counts > peak_threshold]\n",
    "    \n",
    "    # Patterns saisonniers (simulation si plusieurs mois)\n",
    "    if len(df_temp['published_clean'].dt.month.unique()) > 1:\n",
    "        monthly_pattern = df_temp.groupby(df_temp['published_clean'].dt.month).size()\n",
    "        seasonal_variance = monthly_pattern.var()\n",
    "    else:\n",
    "        seasonal_variance = 0\n",
    "    \n",
    "    return {\n",
    "        'df_temporal': df_temp,\n",
    "        'peak_days': peak_days,\n",
    "        'daily_pattern': daily_counts,\n",
    "        'seasonal_variance': seasonal_variance,\n",
    "        'hour_distribution': df_temp['hour'].value_counts().sort_index(),\n",
    "        'day_distribution': df_temp['day_week'].value_counts()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5663a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERNS HORAIRES DÉTECTÉS:\n",
      "   01h: 1 articles (0.8%)\n",
      "   02h: 3 articles (2.3%)\n",
      "   03h: 1 articles (0.8%)\n"
     ]
    }
   ],
   "source": [
    "# Application analyse temporelle\n",
    "temporal_results = advanced_temporal_analysis(df_clean_dedup)\n",
    "\n",
    "print(f\"PATTERNS HORAIRES DÉTECTÉS:\")\n",
    "peak_hours = temporal_results['hour_distribution'].head(3)\n",
    "for hour, count in peak_hours.items():\n",
    "    pct = count / len(temporal_results['df_temporal']) * 100\n",
    "    print(f\"   {hour:02d}h: {count} articles ({pct:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d31aa640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PICS D'ACTUALITÉ IDENTIFIÉS:\n",
      "   2025-07-25: 54 articles (pic d'actualité)\n",
      "       Sujets: État(14), Paris(13), Europe(10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPICS D'ACTUALITÉ IDENTIFIÉS:\")\n",
    "if len(temporal_results['peak_days']) > 0:\n",
    "    for date, count in temporal_results['peak_days'].items():\n",
    "        print(f\"   {date}: {count} articles (pic d'actualité)\")\n",
    "        # Analyser les sujets de ce pic\n",
    "        peak_articles = temporal_results['df_temporal'][\n",
    "            temporal_results['df_temporal']['published_clean'].dt.date == date\n",
    "        ]\n",
    "        top_entities = []\n",
    "        for entities in peak_articles['entities_advanced']:\n",
    "            top_entities.extend(entities.get('locations', []))\n",
    "            top_entities.extend(entities.get('organizations', []))\n",
    "        if top_entities:\n",
    "            top_3 = Counter(top_entities).most_common(3)\n",
    "            print(f\"       Sujets: {', '.join([f'{e}({c})' for e, c in top_3])}\")\n",
    "else:\n",
    "    print(f\"   Aucun pic significatif détecté\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037138d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSIGHTS TEMPORELS:\n",
      "   Période d'analyse: 2025-07-18 → 2025-07-26\n",
      "   Heure de pointe: 4h (10 articles)\n",
      "   Jour le plus actif: Friday (55 articles)\n",
      "   Pics d'actualité: 1 détectés\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nINSIGHTS TEMPORELS:\")\n",
    "print(f\"   Période d'analyse: {temporal_results['df_temporal']['published_clean'].min().date()} → {temporal_results['df_temporal']['published_clean'].max().date()}\")\n",
    "print(f\"   Heure de pointe: {temporal_results['hour_distribution'].idxmax()}h ({temporal_results['hour_distribution'].max()} articles)\")\n",
    "print(f\"   Jour le plus actif: {temporal_results['day_distribution'].idxmax()} ({temporal_results['day_distribution'].max()} articles)\")\n",
    "print(f\"   Pics d'actualité: {len(temporal_results['peak_days'])} détectés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97579e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_topic_analysis(df, n_topics=8):\n",
    "    \"\"\"Analyse sémantique avec LDA et clustering\"\"\"\n",
    "    \n",
    "    # Préparation corpus pour LDA\n",
    "    texts = df['text_cleaned'].fillna('').tolist()\n",
    "    \n",
    "    # Nettoyage pour LDA\n",
    "    texts_clean = []\n",
    "    for text in texts:\n",
    "        if len(text) > 100:  # Minimum 100 caractères\n",
    "            texts_clean.append(text)\n",
    "    \n",
    "    if len(texts_clean) < 10:\n",
    "        print(\"   Corpus trop petit pour l'analyse sémantique\")\n",
    "        return None\n",
    "    \n",
    "    # Vectorisation TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words=['le', 'de', 'et', 'à', 'un', 'il', 'être', 'et', 'en', 'avoir', 'que', 'pour'],\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts_clean)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # LDA Topic Modeling\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=42,\n",
    "            max_iter=20,\n",
    "            doc_topic_prior=0.1,\n",
    "            topic_word_prior=0.01\n",
    "        )\n",
    "        \n",
    "        lda_topics = lda.fit_transform(tfidf_matrix)\n",
    "        \n",
    "        # Extraction topics\n",
    "        topics = []\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words_idx = topic.argsort()[-10:][::-1]\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            topics.append({\n",
    "                'id': topic_idx,\n",
    "                'words': top_words,\n",
    "                'weight': topic[top_words_idx]\n",
    "            })\n",
    "        \n",
    "        # Clustering sémantique\n",
    "        kmeans = KMeans(n_clusters=min(5, len(texts_clean)//10), random_state=42)\n",
    "        clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "        \n",
    "        return {\n",
    "            'topics': topics,\n",
    "            'lda_model': lda,\n",
    "            'doc_topics': lda_topics,\n",
    "            'clusters': clusters,\n",
    "            'vectorizer': vectorizer,\n",
    "            'texts_processed': texts_clean\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Erreur analyse sémantique: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe540b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPICS IDENTIFIÉS:\n",
      "   Topic 0: forum, sites, messages, les données, objets\n",
      "   Topic 1: lire, publicité, gaza, ministre, lire aussi\n",
      "   Topic 2: pro, opération, humanitaire, outre, rapport\n",
      "   Topic 3: trump, 2025, ans, sur les, 2024\n",
      "   Topic 4: pas, avec, qu, vous, on\n",
      "   Topic 5: vos, apple, etats, corse, etat\n",
      "   Topic 6: faille, fondation, openai, désinformation, médaille\n",
      "   Topic 7: paris, étude, décision, meta, mouvement\n",
      "\n",
      "TOPICS DOMINANTS:\n",
      "   Topic 4: 55.2% - pas, avec, qu\n",
      "   Topic 3: 18.1% - trump, 2025, ans\n",
      "   Topic 1: 11.5% - lire, publicité, gaza\n"
     ]
    }
   ],
   "source": [
    "# Application analyse sémantique\n",
    "semantic_results = semantic_topic_analysis(df_clean_dedup)\n",
    "\n",
    "if semantic_results:\n",
    "    print(f\"TOPICS IDENTIFIÉS:\")\n",
    "    for topic in semantic_results['topics']:\n",
    "        print(f\"   Topic {topic['id']}: {', '.join(topic['words'][:5])}\")\n",
    "    \n",
    "    # Analyse distribution topics\n",
    "    topic_distribution = semantic_results['doc_topics'].mean(axis=0)\n",
    "    dominant_topics = topic_distribution.argsort()[-3:][::-1]\n",
    "    \n",
    "    print(f\"\\nTOPICS DOMINANTS:\")\n",
    "    for topic_id in dominant_topics:\n",
    "        topic_strength = topic_distribution[topic_id] * 100\n",
    "        topic_words = semantic_results['topics'][topic_id]['words'][:3]\n",
    "        print(f\"   Topic {topic_id}: {topic_strength:.1f}% - {', '.join(topic_words)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b42cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_quality_metrics(df):\n",
    "    \"\"\"Calcul métriques qualité business\"\"\"\n",
    "    \n",
    "    # Densité informationnelle\n",
    "    df['info_density'] = (\n",
    "        df['entities_total'] / (df['text_cleaned'].str.len() / 1000)\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Score de cohérence (variance entités par type)\n",
    "    entity_variance = []\n",
    "    for _, row in df.iterrows():\n",
    "        entities = row['entities_advanced']\n",
    "        if isinstance(entities, dict):\n",
    "            counts = [\n",
    "                len(entities.get('persons', [])),\n",
    "                len(entities.get('organizations', [])),\n",
    "                len(entities.get('locations', []))\n",
    "            ]\n",
    "            entity_variance.append(np.var(counts))\n",
    "        else:\n",
    "            entity_variance.append(0)\n",
    "    \n",
    "    df['coherence_score'] = 1 / (1 + np.array(entity_variance))\n",
    "    \n",
    "    # Score de nouveauté (basé sur similarité avec articles précédents)\n",
    "    novelty_scores = []\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            novelty_scores.append(1.0)\n",
    "        else:\n",
    "            # Simplicité: basé sur différence entités\n",
    "            current_entities = set()\n",
    "            if isinstance(df.iloc[i]['entities_advanced'], dict):\n",
    "                for ent_list in df.iloc[i]['entities_advanced'].values():\n",
    "                    current_entities.update(ent_list)\n",
    "            \n",
    "            prev_entities = set()\n",
    "            if isinstance(df.iloc[i-1]['entities_advanced'], dict):\n",
    "                for ent_list in df.iloc[i-1]['entities_advanced'].values():\n",
    "                    prev_entities.update(ent_list)\n",
    "            \n",
    "            if len(current_entities) == 0:\n",
    "                novelty_scores.append(0.5)\n",
    "            else:\n",
    "                overlap = len(current_entities & prev_entities) / len(current_entities)\n",
    "                novelty_scores.append(1 - overlap)\n",
    "    \n",
    "    df['novelty_score'] = novelty_scores\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f35d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MÉTRIQUES QUALITÉ BUSINESS:\n",
      "   Densité info moyenne: 4.15 entités/1000 chars\n",
      "   Score cohérence moyen: 0.240\n",
      "   Score nouveauté moyen: 0.974\n"
     ]
    }
   ],
   "source": [
    "# Application métriques avancées\n",
    "df_quality = advanced_quality_metrics(df_clean_dedup.copy())\n",
    "\n",
    "print(f\"MÉTRIQUES QUALITÉ BUSINESS:\")\n",
    "print(f\"   Densité info moyenne: {df_quality['info_density'].mean():.2f} entités/1000 chars\")\n",
    "print(f\"   Score cohérence moyen: {df_quality['coherence_score'].mean():.3f}\")\n",
    "print(f\"   Score nouveauté moyen: {df_quality['novelty_score'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "424ea9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification articles premium\n",
    "premium_threshold = df_quality['quality_score_advanced'].quantile(0.8)\n",
    "premium_articles = df_quality[df_quality['quality_score_advanced'] > premium_threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b27b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARTICLES PREMIUM (top 20%):\n",
      "   26 articles identifiés\n",
      "   Score qualité moyen: 0.762\n",
      "   Densité info moyenne: 3.70\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nARTICLES PREMIUM (top 20%):\")\n",
    "print(f\"   {len(premium_articles)} articles identifiés\")\n",
    "print(f\"   Score qualité moyen: {premium_articles['quality_score_advanced'].mean():.3f}\")\n",
    "print(f\"   Densité info moyenne: {premium_articles['info_density'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0d7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df):\n",
    "    \"\"\"Détection d'articles outliers et contenus suspects\"\"\"\n",
    "    \n",
    "    anomalies = {\n",
    "        'outliers': [],\n",
    "        'suspects': [],\n",
    "        'low_quality': [],\n",
    "        'duplicates_potential': []\n",
    "    }\n",
    "    \n",
    "    # 1. Outliers statistiques\n",
    "    for metric in ['text_cleaned', 'entities_total', 'quality_score_advanced']:\n",
    "        if metric == 'text_cleaned':\n",
    "            values = df[metric].str.len()\n",
    "        else:\n",
    "            values = df[metric]\n",
    "        \n",
    "        Q1 = values.quantile(0.25)\n",
    "        Q3 = values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        outliers_idx = df[\n",
    "            (values < (Q1 - 1.5 * IQR)) | \n",
    "            (values > (Q3 + 1.5 * IQR))\n",
    "        ].index.tolist()\n",
    "        \n",
    "        anomalies['outliers'].extend(outliers_idx)\n",
    "    \n",
    "    # 2. Contenus suspects (peu d'entités + court)\n",
    "    suspects_idx = df[\n",
    "        (df['entities_total'] == 0) & \n",
    "        (df['text_cleaned'].str.len() < 300)\n",
    "    ].index.tolist()\n",
    "    anomalies['suspects'] = suspects_idx\n",
    "    \n",
    "    # 3. Qualité très faible\n",
    "    low_quality_threshold = df['quality_score_advanced'].quantile(0.1)\n",
    "    low_quality_idx = df[\n",
    "        df['quality_score_advanced'] < low_quality_threshold\n",
    "    ].index.tolist()\n",
    "    anomalies['low_quality'] = low_quality_idx\n",
    "    \n",
    "    # 4. Doublons potentiels (titres très similaires)\n",
    "    potential_duplicates = []\n",
    "    titles = df['title'].fillna('').tolist()\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        for j in range(i+1, min(i+50, len(titles))):  # Limiter pour performance\n",
    "            similarity = SequenceMatcher(None, titles[i], titles[j]).ratio()\n",
    "            if similarity > 0.8:\n",
    "                potential_duplicates.append((i, j, similarity))\n",
    "    \n",
    "    anomalies['duplicates_potential'] = potential_duplicates\n",
    "    \n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58131d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOMALIES DÉTECTÉES:\n",
      "   Outliers statistiques: 13 articles\n",
      "   Contenus suspects: 0 articles\n",
      "   Qualité très faible: 12 articles\n",
      "   Doublons potentiels: 0 paires\n"
     ]
    }
   ],
   "source": [
    "# Application détection anomalies\n",
    "anomalies = detect_anomalies(df_quality)\n",
    "\n",
    "print(f\"ANOMALIES DÉTECTÉES:\")\n",
    "print(f\"   Outliers statistiques: {len(set(anomalies['outliers']))} articles\")\n",
    "print(f\"   Contenus suspects: {len(anomalies['suspects'])} articles\")\n",
    "print(f\"   Qualité très faible: {len(anomalies['low_quality'])} articles\")\n",
    "print(f\"   Doublons potentiels: {len(anomalies['duplicates_potential'])} paires\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a470886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if anomalies['suspects']:\n",
    "    print(f\"\\nEXEMPLES CONTENUS SUSPECTS:\")\n",
    "    for idx in anomalies['suspects'][:3]:\n",
    "        article = df_quality.iloc[idx]\n",
    "        print(f\"   \\\"{article['title'][:50]}...\\\"\")\n",
    "        print(f\"       Entités: {article['entities_total']}\")\n",
    "        print(f\"       Longueur: {len(article['text_cleaned'])} chars\")\n",
    "        print(f\"       Score: {article['quality_score_advanced']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2117866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if anomalies['duplicates_potential']:\n",
    "    print(f\"\\nEXEMPLES DOUBLONS POTENTIELS:\")\n",
    "    for i, j, sim in anomalies['duplicates_potential'][:3]:\n",
    "        print(f\"   Similarité {sim:.3f}:\")\n",
    "        print(f\"       A: \\\"{df_quality.iloc[i]['title'][:40]}...\\\"\")\n",
    "        print(f\"       B: \\\"{df_quality.iloc[j]['title'][:40]}...\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188a2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCES:\n",
      "   Sources fiables identifiées: 0\n",
      "   Source top: titres.rss\n",
      "   Score fiabilité max: 0.435\n"
     ]
    }
   ],
   "source": [
    "print(f\"SOURCES:\")\n",
    "print(f\"   Sources fiables identifiées: {len(source_analysis[source_analysis['reliability_score'] > 0.6])}\")\n",
    "print(f\"   Source top: {source_analysis.iloc[0]['source'].split('/')[-1]}\")\n",
    "print(f\"   Score fiabilité max: {source_analysis.iloc[0]['reliability_score']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf747c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEMPOREL:\n",
      "   Pics d'actualité: 1\n",
      "   Heure de pointe: 4h\n",
      "   Distribution: 3.1 (écart-type)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTEMPOREL:\")\n",
    "print(f\"   Pics d'actualité: {len(temporal_results['peak_days'])}\")\n",
    "print(f\"   Heure de pointe: {temporal_results['hour_distribution'].idxmax()}h\")\n",
    "print(f\"   Distribution: {temporal_results['hour_distribution'].std():.1f} (écart-type)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e2732b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SÉMANTIQUE:\n",
      "   Topics identifiés: 8\n",
      "   Topic dominant: pas, avec, qu\n"
     ]
    }
   ],
   "source": [
    "if semantic_results:\n",
    "    print(f\"\\nSÉMANTIQUE:\")\n",
    "    print(f\"   Topics identifiés: {len(semantic_results['topics'])}\")\n",
    "    dominant_topic_id = semantic_results['doc_topics'].mean(axis=0).argmax()\n",
    "    print(f\"   Topic dominant: {', '.join(semantic_results['topics'][dominant_topic_id]['words'][:3])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9768ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUALITÉ:\n",
      "   Articles premium: 26 (20.3%)\n",
      "   Densité info moyenne: 4.15\n",
      "   Score cohérence: 0.240\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nQUALITÉ:\")\n",
    "print(f\"   Articles premium: {len(premium_articles)} ({len(premium_articles)/len(df_quality)*100:.1f}%)\")\n",
    "print(f\"   Densité info moyenne: {df_quality['info_density'].mean():.2f}\")\n",
    "print(f\"   Score cohérence: {df_quality['coherence_score'].mean():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75fe28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANOMALIES:\n",
      "   Contenus suspects: 0\n",
      "   Articles faible qualité: 12\n",
      "   Doublons potentiels: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nANOMALIES:\")\n",
    "print(f\"   Contenus suspects: {len(anomalies['suspects'])}\")\n",
    "print(f\"   Articles faible qualité: {len(anomalies['low_quality'])}\")\n",
    "print(f\"   Doublons potentiels: {len(anomalies['duplicates_potential'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d534601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECOMMANDATIONS BUSINESS:\n",
      "   1. Prioriser les 3 sources les plus fiables\n",
      "   2. Collecter davantage pendant les heures de pointe identifiées\n",
      "   3. Nettoyer 0 articles suspects\n",
      "   4. Exploiter les 26 articles premium pour l'entraînement\n",
      "   5. Équilibrer la couverture des 8 topics identifiés\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRECOMMANDATIONS BUSINESS:\")\n",
    "print(f\"   1. Prioriser les {len(source_analysis.head(3))} sources les plus fiables\")\n",
    "print(f\"   2. Collecter davantage pendant les heures de pointe identifiées\")\n",
    "print(f\"   3. Nettoyer {len(anomalies['suspects'])} articles suspects\")\n",
    "print(f\"   4. Exploiter les {len(premium_articles)} articles premium pour l'entraînement\")\n",
    "if semantic_results:\n",
    "    print(f\"   5. Équilibrer la couverture des {len(semantic_results['topics'])} topics identifiés\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adfccdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des résultats EDA\n",
    "eda_results = {\n",
    "    'source_analysis': source_analysis,\n",
    "    'temporal_results': temporal_results,\n",
    "    'semantic_results': semantic_results,\n",
    "    'df_quality': df_quality,\n",
    "    'premium_articles': premium_articles,\n",
    "    'anomalies': anomalies\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DIR / 'eda_business_results.pkl', 'wb') as f:\n",
    "    pickle.dump(eda_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
