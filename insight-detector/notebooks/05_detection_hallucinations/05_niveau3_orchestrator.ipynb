{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9c2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcce9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825cacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24074f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, re, logging, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3c97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Logging ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"level3_notebook\")\n",
    "\n",
    "# --- Chemins (adaptés à INSIGHT-DETECTOR)\n",
    "def find_project_root():\n",
    "    p = Path.cwd().resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \"src\").exists() and (parent / \"outputs\").exists():\n",
    "            return parent\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f6a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrées brutes L1/L2/mappings (déjà dans outputs/)\n",
    "RAW_OUT_DIR       = PROJECT_ROOT / \"outputs\"\n",
    "# Articles sources (déjà dans data/exports/)\n",
    "RAW_DATA_EXPORTS  = PROJECT_ROOT / \"data\" / \"exports\"\n",
    "\n",
    "# Intermédiaires/caches L3 (dans data/processed/level3)\n",
    "WORK_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"level3\"\n",
    "CACHE_DIR  = WORK_DIR / \"cache\"\n",
    "\n",
    "# Exports finaux L3\n",
    "OUT_L3_DIR  = PROJECT_ROOT / \"outputs\" / \"level3\"\n",
    "EXPORT_DIR  = OUT_L3_DIR / \"exports\"\n",
    "REPORT_DIR  = OUT_L3_DIR / \"reports\"\n",
    "LOG_DIR     = OUT_L3_DIR / \"logs\"\n",
    "\n",
    "for d in [WORK_DIR, CACHE_DIR, EXPORT_DIR, REPORT_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98361224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niveau 3 improvement importé (version de base)\n"
     ]
    }
   ],
   "source": [
    "# Utils + config/prompts dans src/detection/level3 ENHANCED\n",
    "try:\n",
    "    # Essayer la version enhanced avec stratégies adaptatives\n",
    "    SRC_L3_DIR   = PROJECT_ROOT / \"src\" / \"detection\" / \"level3_adaptive\"\n",
    "    CONFIG_DIR   = SRC_L3_DIR / \"config\"\n",
    "    PROMPT_DIR   = SRC_L3_DIR / \"prompts\"\n",
    "    sys.path.append(str(SRC_L3_DIR))\n",
    "    \n",
    "    from level3_adaptive_utils import (\n",
    "        sha1_text, read_jsonl, write_jsonl, detect_lang, chunk_text_by_words,\n",
    "        generate_edit, generate_resummarize, postprocess_summary,\n",
    "        choose_mode, l2_like_evaluate, accept_after\n",
    "    )\n",
    "    print(\"NIVEAU 3 ENHANCED importé (stratégies adaptatives)\")\n",
    "    \n",
    "except ImportError:\n",
    "    try:\n",
    "        # Fallback vers version improvement\n",
    "        SRC_L3_DIR   = PROJECT_ROOT / \"src\" / \"detection\" / \"level3_improvement\"\n",
    "        CONFIG_DIR   = SRC_L3_DIR / \"config\"\n",
    "        PROMPT_DIR   = SRC_L3_DIR / \"prompts\"\n",
    "        sys.path.append(str(SRC_L3_DIR))\n",
    "        \n",
    "        from level3_utils import (\n",
    "            sha1_text, read_jsonl, write_jsonl, detect_lang, chunk_text_by_words,\n",
    "            generate_edit, generate_resummarize, postprocess_summary,\n",
    "            choose_mode, l2_like_evaluate, accept_after\n",
    "        )\n",
    "        print(\"Niveau 3 improvement importé (version de base)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"Impossible d'importer Level3 depuis 'src/detection/...'. \"\n",
    "            f\"Vérifie que 'src' est bien au bon endroit. Erreur: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a84528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:Project root: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\n"
     ]
    }
   ],
   "source": [
    "# --- Fichiers bruts (emplacements existants)\n",
    "f_l2_res    = RAW_OUT_DIR / \"level2_simplified_results_with_ids.csv\"\n",
    "f_l2_prio   = RAW_OUT_DIR / \"level2_simplified_priority_cases_with_ids.csv\"\n",
    "f_l2_full   = RAW_OUT_DIR / \"level2_output_with_source_id.json\"\n",
    "f_map1      = RAW_OUT_DIR / \"mapping_level1id_to_source_id.csv\"\n",
    "f_map2      = RAW_OUT_DIR / \"mapping_backfill_level2.csv\"\n",
    "f_n1        = RAW_OUT_DIR / \"all_summaries_production.json\"\n",
    "f_articles  = RAW_DATA_EXPORTS / \"raw_articles.json\"\n",
    "f_cfg       = CONFIG_DIR / \"level3.yaml\"\n",
    "\n",
    "logger.info(\"Project root: %s\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507a330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Fallback Parquet <-> CSV (robuste) ----\n",
    "def _detect_parquet_engine():\n",
    "    try:\n",
    "        import pyarrow as pa  # noqa\n",
    "        # Gardes-fous: certaines install partielles n'ont pas __version__\n",
    "        if not hasattr(pa, \"__version__\"):\n",
    "            return None\n",
    "        return \"pyarrow\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet as fp  # noqa\n",
    "            return \"fastparquet\"\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "PARQUET_ENGINE = _detect_parquet_engine()\n",
    "\n",
    "from pathlib import Path\n",
    "def save_table(df: pd.DataFrame, stem_path: Path):\n",
    "    \"\"\"Écrit .parquet si possible, ET toujours un .csv (fallback/idempotent).\"\"\"\n",
    "    stem_path = Path(stem_path)\n",
    "    if PARQUET_ENGINE:\n",
    "        df.to_parquet(stem_path.with_suffix(\".parquet\"), index=False, engine=PARQUET_ENGINE)\n",
    "    df.to_csv(stem_path.with_suffix(\".csv\"), index=False)\n",
    "\n",
    "def load_table(stem_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Charge .parquet si dispo & engine présent, sinon .csv.\"\"\"\n",
    "    stem_path = Path(stem_path)\n",
    "    p, c = stem_path.with_suffix(\".parquet\"), stem_path.with_suffix(\".csv\")\n",
    "    if p.exists() and PARQUET_ENGINE:\n",
    "        return pd.read_parquet(p, engine=PARQUET_ENGINE)\n",
    "    if c.exists():\n",
    "        return pd.read_csv(c)\n",
    "    raise FileNotFoundError(f\"Aucun fichier trouvé: {p} ni {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d3c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'priority_threshold': 0.8,\n",
       " 'min_text_chars_for_resummarize': 500,\n",
       " 'edit_rule_adaptive': {'issues_max': 8,\n",
       "  'factuality_min': 0.75,\n",
       "  'coherence_rewrite_threshold': 0.15},\n",
       " 'acceptance': {'accepted_tiers': ['GOOD',\n",
       "   'EXCELLENT',\n",
       "   'MODERATE',\n",
       "   'IMPROVED_CRITICAL'],\n",
       "  'allow_moderate_guarded': True,\n",
       "  'allow_critical_with_improvement': True,\n",
       "  'moderate_guard': {'issues_max': 4,\n",
       "   'factuality_min': 0.8,\n",
       "   'coherence_min': 0.7},\n",
       "  'critical_guard': {'issues_max': 8,\n",
       "   'factuality_min': 0.7,\n",
       "   'coherence_min': 0.6,\n",
       "   'improvement_required': 0.05},\n",
       "  'require_monotonic_improvement': False,\n",
       "  'allow_stagnation_if_issues_reduced': True},\n",
       " 'acceptance_topic': {'after_text_min': 0.01, 'after_before_min': 0.01},\n",
       " 'gen_params': {'temperature': 0.0,\n",
       "  'top_p': 0.1,\n",
       "  'max_tokens': 220,\n",
       "  'stop': ['\\\\n\\\\n', '###'],\n",
       "  'seed': 42},\n",
       " 'mode': {'prefer_resum_for_cw_critical': True,\n",
       "  'tiers_order': ['CRITICAL', 'MODERATE', 'GOOD', 'EXCELLENT']}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import yaml\n",
    "    CFG = yaml.safe_load(open(f_cfg, \"r\", encoding=\"utf-8\"))\n",
    "except Exception as e:\n",
    "    logger.warning(\"YAML non dispo (%s) -> utilisation de défauts.\", e)\n",
    "    CFG = {\n",
    "        \"priority_threshold\": 0.85,\n",
    "        \"min_text_chars_for_resummarize\": 800,\n",
    "        \"edit_rule_adaptive\": {\"issues_max\": 5, \"factuality_min\": 0.88, \"coherence_rewrite_threshold\": 0.20},\n",
    "        \"acceptance\": {\n",
    "            \"accepted_tiers\": [\"GOOD\",\"EXCELLENT\"],\n",
    "            \"allow_moderate_guarded\": True,\n",
    "            \"moderate_guard\": {\"issues_max\": 2, \"factuality_min\": 0.90, \"coherence_min\": 0.80},\n",
    "            \"require_monotonic_improvement\": True\n",
    "        },\n",
    "        \"gen_params\": {\"temperature\":0.0, \"top_p\":0.1, \"max_tokens\":220, \"stop\":[\"\\n\\n\",\"###\"], \"seed\":42},\n",
    "        \"mode\": {\"prefer_resum_for_cw_critical\": True, \"tiers_order\":[\"CRITICAL\",\"MODERATE\",\"GOOD\",\"EXCELLENT\"]}\n",
    "    }\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987a44a",
   "metadata": {},
   "source": [
    "# Sélection des candidats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db29eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:00 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\00_candidates.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\00_candidates.csv (81 lignes)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f_l2_res)\n",
    "prio_thresh = CFG[\"priority_threshold\"]\n",
    "\n",
    "candidates = df[(df[\"tier\"]==\"CRITICAL\") | (df[\"level3_priority_final\"]>=prio_thresh)].copy()\n",
    "tier_order = {t:i for i,t in enumerate(CFG[\"mode\"][\"tiers_order\"])}\n",
    "candidates[\"tier_rank\"] = candidates[\"tier\"].map(tier_order)\n",
    "candidates = candidates.sort_values([\"level3_priority_final\",\"issues_count\",\"tier_rank\"], ascending=[False, False, True])\n",
    "\n",
    "stem_00 = WORK_DIR / \"00_candidates\"\n",
    "save_table(candidates, stem_00)\n",
    "logger.info(\"00 -> %s | %s (%d lignes)\", stem_00.with_suffix(\".parquet\"), stem_00.with_suffix(\".csv\"), len(candidates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5482a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 n = 81 uniques summary_id = 81\n",
      "tier\n",
      "CRITICAL    81\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "c0 = load_table(WORK_DIR/\"00_candidates\")\n",
    "print(\"00 n =\", len(c0), \"uniques summary_id =\", c0[\"summary_id\"].nunique())\n",
    "print(c0[\"tier\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09490821",
   "metadata": {},
   "source": [
    "# Backfill source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c445c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:01 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\01_backfilled.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\01_backfilled.csv (manquants=0)\n"
     ]
    }
   ],
   "source": [
    "cand = load_table(WORK_DIR / \"00_candidates\")\n",
    "map1 = pd.read_csv(f_map1)  # level1_id -> source_id\n",
    "map2 = pd.read_csv(f_map2)  # summary_id -> source_id\n",
    "\n",
    "cand = cand.merge(map1.rename(columns={\"level1_id\":\"summary_id\",\"source_id\":\"source_id_m1\"}), on=\"summary_id\", how=\"left\")\n",
    "cand = cand.merge(map2.rename(columns={\"summary_id\":\"summary_id\",\"source_id\":\"source_id_m2\"}), on=\"summary_id\", how=\"left\")\n",
    "cand[\"source_id_filled\"] = cand[\"source_id\"].fillna(cand[\"source_id_m1\"]).fillna(cand[\"source_id_m2\"])\n",
    "\n",
    "missing = cand[cand[\"source_id_filled\"].isna()][[\"summary_id\"]]\n",
    "missing.to_csv(REPORT_DIR / \"01_missing_source_id.csv\", index=False)\n",
    "\n",
    "stem_01 = WORK_DIR / \"01_backfilled\"\n",
    "save_table(cand, stem_01)\n",
    "logger.info(\"01 -> %s | %s (manquants=%d)\", stem_01.with_suffix(\".parquet\"), stem_01.with_suffix(\".csv\"), len(missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455b18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 coverage source_id_filled = 100.0%\n"
     ]
    }
   ],
   "source": [
    "c1 = load_table(WORK_DIR/\"01_backfilled\")\n",
    "cov = c1[\"source_id_filled\"].notna().mean()\n",
    "print(f\"01 coverage source_id_filled = {cov:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc328a0",
   "metadata": {},
   "source": [
    "# Join articles + flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1e59180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CORRECTION: Utilisation du mapping unifié existant\n",
      "✅ Mapping réussi via unified_mapping_complete.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:02 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\02_join_articles.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\02_join_articles.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Résultats du mapping:\n",
      "   Entrées totales: 81\n",
      "   Avec texte: 81 (100.0%)\n",
      "   Texte suffisant: 68 (84.0%)\n"
     ]
    }
   ],
   "source": [
    "cand = load_table(WORK_DIR / \"01_backfilled\")\n",
    "\n",
    "# CORRECTION: Utiliser le mapping unifié créé par le notebook 04_mapping_unified\n",
    "# Au lieu de tenter un match SHA1 direct, utiliser le mapping déjà établi\n",
    "mapping_file = RAW_OUT_DIR / \"unified_mapping_complete.csv\"\n",
    "if mapping_file.exists():\n",
    "    print(\"🔧 CORRECTION: Utilisation du mapping unifié existant\")\n",
    "    mapping_unified = pd.read_csv(mapping_file)\n",
    "    \n",
    "    # Créer un mapping source_id → métadonnées article\n",
    "    source_to_article = mapping_unified[[\"level2_id\", \"source_id\", \"title\", \"url\", \"article_id\"]].drop_duplicates()\n",
    "    \n",
    "    # Charger les textes des articles\n",
    "    arts = pd.read_json(f_articles)\n",
    "    arts_text = arts[[\"id\", \"text\"]].rename(columns={\"id\": \"article_id\"})\n",
    "    \n",
    "    # Joindre: candidats → mapping → textes\n",
    "    cand = cand.merge(\n",
    "        source_to_article.rename(columns={\"level2_id\": \"summary_id\"}), \n",
    "        on=\"summary_id\", \n",
    "        how=\"left\"\n",
    "    ).merge(\n",
    "        arts_text,\n",
    "        on=\"article_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Mapping réussi via unified_mapping_complete.csv\")\n",
    "    \n",
    "else:\n",
    "    # Fallback vers l'ancienne méthode (problématique)\n",
    "    print(\"⚠️ FALLBACK: Tentative mapping direct SHA1 (peut échouer)\")\n",
    "    arts = pd.read_json(f_articles)\n",
    "    arts[\"sha1_url\"] = arts[\"url\"].astype(str).apply(lambda u: hashlib.sha1(u.encode(\"utf-8\")).hexdigest())\n",
    "    cand = cand.merge(arts[[\"id\",\"title\",\"url\",\"text\",\"sha1_url\"]], left_on=\"source_id_filled\", right_on=\"sha1_url\", how=\"left\")\n",
    "\n",
    "# Flags de disponibilité des textes\n",
    "cand[\"has_text\"] = cand[\"text\"].notna() & (cand[\"text\"].astype(str).str.len() > 0)\n",
    "cand[\"enough_length\"] = cand[\"text\"].apply(lambda t: isinstance(t,str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"])\n",
    "\n",
    "# Détection langue depuis 'summary' si dispo, sinon fallback sur 'title'\n",
    "if \"summary\" in cand.columns:\n",
    "    cand[\"lang\"] = cand[\"summary\"].fillna(\"\").apply(detect_lang)\n",
    "else:\n",
    "    cand[\"lang\"] = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "\n",
    "stem_02 = WORK_DIR / \"02_join_articles\"\n",
    "save_table(cand, stem_02)\n",
    "logger.info(\"02 -> %s | %s\", stem_02.with_suffix(\".parquet\"), stem_02.with_suffix(\".csv\"))\n",
    "\n",
    "print(f\"📊 Résultats du mapping:\")\n",
    "print(f\"   Entrées totales: {len(cand)}\")\n",
    "print(f\"   Avec texte: {cand['has_text'].sum()} ({cand['has_text'].mean()*100:.1f}%)\")\n",
    "print(f\"   Texte suffisant: {cand['enough_length'].sum()} ({cand['enough_length'].mean()*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c13fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_text\n",
      "True    81\n",
      "Name: count, dtype: int64\n",
      "enough_length\n",
      "True     68\n",
      "False    13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "c2 = load_table(WORK_DIR/\"02_join_articles\")\n",
    "print(c2[\"has_text\"].value_counts(dropna=False))\n",
    "print(c2[\"enough_length\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1556de",
   "metadata": {},
   "source": [
    "# Join N1 (résumé initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e86cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fonctions utilitaires Level3 définies\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FONCTIONS UTILITAIRES LEVEL3 \n",
    "# =========================\n",
    "\n",
    "import re, json, logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from level3_utils import detect_lang, _as_text\n",
    "\n",
    "# Logger fallback\n",
    "logger = logging.getLogger(\"level3_notebook\") if \"logger\" in globals() else logging.getLogger(\"level3_patch\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def _tokset(s: str):\n",
    "    \"\"\"Extrait les tokens significatifs d'un texte pour calcul Jaccard\"\"\"\n",
    "    s = (s or \"\").lower()\n",
    "    toks = re.findall(r\"[a-zÀ-ÖØ-öø-ÿ0-9]+\", s)\n",
    "    stops = {\n",
    "        \"le\",\"la\",\"les\",\"des\",\"du\",\"de\",\"un\",\"une\",\"et\",\"pour\",\"avec\",\"dans\",\"sur\",\"par\",\"au\",\"aux\",\"est\",\"sont\",\n",
    "        \"the\",\"of\",\"and\",\"to\",\"in\",\"for\",\"on\",\"with\",\"by\",\"is\",\"are\",\"as\",\"at\",\"from\"\n",
    "    }\n",
    "    return {t for t in toks if t not in stops and len(t) > 2}\n",
    "\n",
    "def _jaccard(a, b):\n",
    "    \"\"\"Calcule la similarité Jaccard entre deux textes\"\"\"\n",
    "    A, B = _tokset(a), _tokset(b)\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "def _infer_strategy_from_summary_id(sid: str) -> str:\n",
    "    \"\"\"Infère la stratégie depuis le summary_id si manquante\"\"\"\n",
    "    s = str(sid or \"\")\n",
    "    for st in (\"adaptive\", \"confidence_weighted\"):\n",
    "        if s.endswith(\"_\" + st) or (\"_\" + st) in s:\n",
    "            return st\n",
    "    return s.split(\"_\")[-1] if \"_\" in s else \"\"\n",
    "\n",
    "def ensure_required_columns(cand: pd.DataFrame, CFG: dict) -> pd.DataFrame:\n",
    "    \"\"\"Garantit la présence des colonnes nécessaires avec fallbacks robustes\"\"\"\n",
    "    cand = cand.copy()\n",
    "    \n",
    "    # Tier\n",
    "    if \"tier\" not in cand.columns and \"tier_before\" in cand.columns:\n",
    "        cand[\"tier\"] = cand[\"tier_before\"]\n",
    "    \n",
    "    # Strategy (fallback depuis summary_id)\n",
    "    if \"strategy\" not in cand.columns:\n",
    "        cand[\"strategy\"] = np.nan\n",
    "    missing_mask = cand[\"strategy\"].isna()\n",
    "    if missing_mask.any():\n",
    "        cand.loc[missing_mask, \"strategy\"] = cand.loc[missing_mask, \"summary_id\"].apply(_infer_strategy_from_summary_id)\n",
    "    \n",
    "    # Flags texte\n",
    "    text_col_exists = \"text\" in cand.columns\n",
    "    \n",
    "    if \"has_text\" not in cand.columns:\n",
    "        cand[\"has_text\"] = cand[\"text\"].notna() if text_col_exists else False\n",
    "    else:\n",
    "        cand[\"has_text\"] = cand[\"has_text\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "    \n",
    "    if \"enough_length\" not in cand.columns:\n",
    "        if text_col_exists:\n",
    "            cand[\"enough_length\"] = cand[\"text\"].apply(lambda t: isinstance(t, str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"])\n",
    "        else:\n",
    "            cand[\"enough_length\"] = False\n",
    "    else:\n",
    "        cand[\"enough_length\"] = cand[\"enough_length\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "    \n",
    "    # Langue\n",
    "    if \"lang\" not in cand.columns or cand[\"lang\"].isna().any():\n",
    "        if \"summary_before\" in cand.columns:\n",
    "            lang_series = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "        elif \"title\" in cand.columns:\n",
    "            lang_series = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "        else:\n",
    "            lang_series = pd.Series([\"fr\"] * len(cand), index=cand.index)\n",
    "        \n",
    "        if \"lang\" not in cand.columns:\n",
    "            cand[\"lang\"] = lang_series\n",
    "        else:\n",
    "            cand[\"lang\"] = cand[\"lang\"].fillna(lang_series)\n",
    "    \n",
    "    return cand\n",
    "\n",
    "print(\"✅ Fonctions utilitaires Level3 définies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "htfyyx4btfg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Step 03: Join N1...\n",
      "   Loaded 81 candidates with articles\n",
      "   Parsed 372 N1 summaries\n",
      "✅ Step 03 completed: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1.csv\n",
      "   Total with N1: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beedi.goua_square-ma\\AppData\\Local\\Temp\\ipykernel_28584\\1043569910.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  cand.loc[missing_mask, \"strategy\"] = cand.loc[missing_mask, \"summary_id\"].apply(_infer_strategy_from_summary_id)\n"
     ]
    }
   ],
   "source": [
    "# Step 03: Join N1 (résumé initial)\n",
    "print(\"🔄 Step 03: Join N1...\")\n",
    "\n",
    "# Chargement des données depuis Step 02 (join articles)\n",
    "cand = load_table(WORK_DIR / \"02_join_articles\")\n",
    "print(f\"   Loaded {len(cand)} candidates with articles\")\n",
    "\n",
    "# Chargement et parsing du N1 (résumés initiaux)\n",
    "import json\n",
    "n1_raw = json.load(open(f_n1, \"r\", encoding=\"utf-8\"))\n",
    "rows = []\n",
    "\n",
    "# Parse structure N1 (dict ou list)\n",
    "if isinstance(n1_raw, dict):\n",
    "    it = n1_raw.items()\n",
    "elif isinstance(n1_raw, list):\n",
    "    it = [(None, v) for v in n1_raw]\n",
    "else:\n",
    "    it = []\n",
    "\n",
    "for k, v in it:\n",
    "    if isinstance(v, dict) and \"strategies\" in v:\n",
    "        article_id = v.get(\"article_id\", k)\n",
    "        for strat_name, strat_data in v[\"strategies\"].items():\n",
    "            if isinstance(strat_data, dict):\n",
    "                summary_id = f\"{article_id}_{strat_name}\"\n",
    "                rows.append({\n",
    "                    \"summary_id\": summary_id,\n",
    "                    \"summary\": strat_data.get(\"summary\", \"\"),\n",
    "                    \"summary_before\": strat_data.get(\"summary_before\", \"\")\n",
    "                })\n",
    "\n",
    "n1_summaries = pd.DataFrame(rows)\n",
    "print(f\"   Parsed {len(n1_summaries)} N1 summaries\")\n",
    "\n",
    "# Join avec N1 pour récupérer les résumés initiaux\n",
    "cand = cand.merge(n1_summaries, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# Validation avec la fonction correcte\n",
    "ensure_required_columns(cand, CFG)\n",
    "\n",
    "# Sauvegarde Step 03\n",
    "stem_03 = WORK_DIR / \"03_with_n1\"\n",
    "save_table(cand, stem_03)\n",
    "print(f\"✅ Step 03 completed: {stem_03.with_suffix('.csv')}\")\n",
    "print(f\"   Total with N1: {len(cand)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c0km28ivu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Step 03b: Alignment...\n",
      "   Loaded 81 candidates with N1\n",
      "   Computing topic overlap...\n",
      "   Topic overlap: min=0.000, max=0.000, mean=0.000\n",
      "✅ Step 03b completed: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1_aligned.csv\n",
      "   Total aligned: 81\n"
     ]
    }
   ],
   "source": [
    "# Step 03b: Alignment (langue + topic overlap)\n",
    "print(\"🔄 Step 03b: Alignment...\")\n",
    "\n",
    "# Chargement des données depuis Step 03\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\")\n",
    "print(f\"   Loaded {len(cand)} candidates with N1\")\n",
    "\n",
    "# Détection/correction de la langue\n",
    "if \"summary_before\" in cand.columns:\n",
    "    lang_series = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "elif \"title\" in cand.columns:\n",
    "    lang_series = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "else:\n",
    "    lang_series = pd.Series([\"fr\"] * len(cand), index=cand.index)\n",
    "\n",
    "if \"lang\" not in cand.columns:\n",
    "    cand[\"lang\"] = lang_series\n",
    "else:\n",
    "    cand[\"lang\"] = cand[\"lang\"].fillna(lang_series)\n",
    "\n",
    "# Calcul de l'overlap topique (optimisé)\n",
    "print(\"   Computing topic overlap...\")\n",
    "overlaps = []\n",
    "for idx, row in cand.iterrows():\n",
    "    try:\n",
    "        summary_before = _as_text(row.get(\"summary_before\", \"\"))\n",
    "        text = _as_text(row.get(\"text\", \"\"))\n",
    "        \n",
    "        # Éviter les calculs sur textes vides\n",
    "        if not summary_before or not text:\n",
    "            overlap = 0.0\n",
    "        else:\n",
    "            overlap = _jaccard(_tokset(summary_before), _tokset(text))\n",
    "        \n",
    "        overlaps.append(overlap)\n",
    "    except Exception as e:\n",
    "        # Fallback en cas d'erreur\n",
    "        print(f\"   Warning: overlap calculation failed for row {idx}: {e}\")\n",
    "        overlaps.append(0.0)\n",
    "\n",
    "cand[\"topic_overlap\"] = overlaps\n",
    "\n",
    "# Statistiques robustes\n",
    "valid_overlaps = [o for o in overlaps if not pd.isna(o)]\n",
    "if valid_overlaps:\n",
    "    print(f\"   Topic overlap: min={min(valid_overlaps):.3f}, max={max(valid_overlaps):.3f}, mean={np.mean(valid_overlaps):.3f}\")\n",
    "else:\n",
    "    print(\"   Topic overlap: no valid overlaps computed\")\n",
    "\n",
    "# Sauvegarde Step 03b  \n",
    "stem_03b = WORK_DIR / \"03_with_n1_aligned\"\n",
    "save_table(cand, stem_03b)\n",
    "print(f\"✅ Step 03b completed: {stem_03b.with_suffix('.csv')}\")\n",
    "print(f\"   Total aligned: {len(cand)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fmascx3z5bi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Step 04: Mode Decision...\n",
      "   Loaded 81 candidates from step 03b\n",
      "   Applying choose_mode logic...\n",
      "   Generated 81 mode decisions\n",
      "   Mode distribution:\n",
      "     edit: 81 (100.0%)\n",
      "✅ Step 04 completed: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\04_mode_plan.csv\n",
      "   Total candidates: 81\n"
     ]
    }
   ],
   "source": [
    "# Step 04: Mode Decision\n",
    "print(\"🎯 Step 04: Mode Decision...\")\n",
    "\n",
    "# Chargement des données depuis Step 03b (aligned)\n",
    "data_path = WORK_DIR / \"03_with_n1_aligned.csv\"\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Données Step 03b manquantes: {data_path}\")\n",
    "\n",
    "cand = pd.read_csv(data_path)\n",
    "print(f\"   Loaded {len(cand)} candidates from step 03b\")\n",
    "\n",
    "# Application de choose_mode pour chaque candidat\n",
    "print(\"   Applying choose_mode logic...\")\n",
    "decisions = []\n",
    "for idx, r in cand.iterrows():\n",
    "    mode, reason, flags = choose_mode(r, CFG)\n",
    "    decisions.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"l3_mode\": mode,\n",
    "        \"mode_reason\": reason,\n",
    "        # Flags pour garantir la présence dans le CSV final\n",
    "        \"has_text\": bool(flags.get(\"has_text\", r.get(\"has_text\", False))),\n",
    "        \"enough_length\": bool(flags.get(\"enough_length\", r.get(\"enough_length\", False))),\n",
    "        \"lang\": flags.get(\"lang\", r.get(\"lang\", \"fr\"))\n",
    "    })\n",
    "\n",
    "plan = pd.DataFrame(decisions)\n",
    "print(f\"   Generated {len(plan)} mode decisions\")\n",
    "\n",
    "# Protection contre les colonnes dupliquées au merge\n",
    "cols_to_drop = {\"l3_mode\", \"mode_reason\", \"has_text\", \"enough_length\", \"lang\"} & set(cand.columns)\n",
    "cand_clean = cand.drop(columns=list(cols_to_drop), errors=\"ignore\")\n",
    "\n",
    "# Merge des décisions avec les données candidates\n",
    "level3_plan = cand_clean.merge(plan, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# Validation et statistiques\n",
    "print(f\"   Mode distribution:\")\n",
    "for mode, count in level3_plan[\"l3_mode\"].value_counts().items():\n",
    "    print(f\"     {mode}: {count} ({count/len(level3_plan)*100:.1f}%)\")\n",
    "\n",
    "# Sauvegarde du plan de mode\n",
    "stem_04 = WORK_DIR / \"04_mode_plan\"\n",
    "save_table(level3_plan, stem_04)\n",
    "print(f\"✅ Step 04 completed: {stem_04.with_suffix('.csv')}\")\n",
    "print(f\"   Total candidates: {len(level3_plan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lt6lw3hszom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:03 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1.csv\n",
      "INFO:level3_notebook:03 summary_before coverage = 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Étape 03: Chargement et join des résumés N1...\n",
      "   N1 parsed: 370 entrées\n",
      "✅ Étape 03 terminée: 81 candidats avec N1, coverage summary_before: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ÉTAPE 03 — Join N1 (résumés initiaux)\n",
    "# =========================\n",
    "\n",
    "if \"WORK_DIR\" not in globals() or \"f_n1\" not in globals():\n",
    "    raise RuntimeError(\"WORK_DIR et f_n1 doivent être définis plus haut dans le notebook.\")\n",
    "\n",
    "print(\"📝 Étape 03: Chargement et join des résumés N1...\")\n",
    "\n",
    "# Chargement N1\n",
    "n1_raw = json.load(open(f_n1, \"r\", encoding=\"utf-8\"))\n",
    "rows = []\n",
    "\n",
    "# Parse structure N1 (dict ou list)\n",
    "if isinstance(n1_raw, dict):\n",
    "    it = n1_raw.items()\n",
    "elif isinstance(n1_raw, list):\n",
    "    it = [(None, v) for v in n1_raw]\n",
    "else:\n",
    "    it = []\n",
    "\n",
    "# Extraction des données N1\n",
    "for _, v in it:\n",
    "    v = v or {}\n",
    "    aid = v.get(\"article_id\") or v.get(\"id\")\n",
    "    strategies = v.get(\"strategies\") or {}\n",
    "    if not aid or not isinstance(strategies, dict):\n",
    "        continue\n",
    "    for strat, sv in strategies.items():\n",
    "        sv = sv or {}\n",
    "        metrics = sv.get(\"metrics\") or {}\n",
    "        rows.append({\n",
    "            \"article_id\": aid,\n",
    "            \"strategy\": strat,\n",
    "            \"summary_id\": f\"{aid}_{strat}\",\n",
    "            \"summary_before\": sv.get(\"summary\", \"\"),\n",
    "            \"n1_coherence\": metrics.get(\"coherence\"),\n",
    "            \"n1_factuality\": metrics.get(\"factuality\"),\n",
    "        })\n",
    "\n",
    "df_n1 = pd.DataFrame(rows)\n",
    "print(f\"   N1 parsed: {len(df_n1)} entrées\")\n",
    "\n",
    "# Join avec les candidats Level2\n",
    "c02 = load_table(WORK_DIR / \"02_join_articles\")\n",
    "cand = c02.merge(df_n1, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# Garantir stratégie (fallback depuis summary_id)\n",
    "cand = ensure_required_columns(cand, CFG)\n",
    "\n",
    "# Sauvegarde\n",
    "stem_03 = WORK_DIR / \"03_with_n1\"\n",
    "save_table(cand, stem_03)\n",
    "\n",
    "# Validation\n",
    "c3 = load_table(stem_03)\n",
    "coverage = c3[\"summary_before\"].notna().mean() if \"summary_before\" in c3.columns else 0.0\n",
    "\n",
    "logger.info(\"03 -> %s | %s\", stem_03.with_suffix(\".parquet\"), stem_03.with_suffix(\".csv\"))\n",
    "logger.info(\"03 summary_before coverage = %.1f%%\", coverage*100.0)\n",
    "print(f\"✅ Étape 03 terminée: {len(cand)} candidats avec N1, coverage summary_before: {coverage*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lfj4ejbkgm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Étape 03b: Calcul alignement langue et overlap thématique...\n",
      "   Calcul topic overlap (peut prendre quelques secondes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:03b -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1_aligned.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1_aligned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Étape 03b terminée:\n",
      "   Lang mismatch: 18.5%\n",
      "   Topic overlap median: 0.0061\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ÉTAPE 03b — Alignement langue & topic overlap\n",
    "# =========================\n",
    "\n",
    "print(\"🌐 Étape 03b: Calcul alignement langue et overlap thématique...\")\n",
    "\n",
    "# Charger les données de l'étape précédente\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\").copy()\n",
    "\n",
    "# Normalisation et détection langue du texte source\n",
    "if \"text\" not in cand.columns:\n",
    "    cand[\"text\"] = \"\"\n",
    "cand[\"text\"] = cand[\"text\"].apply(_as_text)\n",
    "cand[\"text_lang\"] = cand[\"text\"].apply(detect_lang)\n",
    "\n",
    "# Langue du résumé (détection depuis summary_before si nécessaire)\n",
    "if \"lang\" not in cand.columns:\n",
    "    if \"summary_before\" in cand.columns:\n",
    "        cand[\"lang\"] = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "    elif \"title\" in cand.columns:\n",
    "        cand[\"lang\"] = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "    else:\n",
    "        cand[\"lang\"] = \"fr\"\n",
    "else:\n",
    "    cand[\"lang\"] = cand[\"lang\"].fillna(\"fr\").astype(str)\n",
    "\n",
    "# Détection mismatch de langue\n",
    "cand[\"lang_mismatch\"] = cand[\"lang\"].fillna(\"fr\") != cand[\"text_lang\"].fillna(\"fr\")\n",
    "\n",
    "# Garantir summary_before (fallback sur 'summary' si besoin)\n",
    "if \"summary_before\" not in cand.columns:\n",
    "    cand[\"summary_before\"] = cand[\"summary\"].fillna(\"\") if \"summary\" in cand.columns else \"\"\n",
    "\n",
    "# Calcul similarité thématique résumé_before vs texte source\n",
    "print(\"   Calcul topic overlap (peut prendre quelques secondes)...\")\n",
    "cand[\"topic_overlap_before_text\"] = cand.apply(\n",
    "    lambda r: _jaccard(_as_text(r.get(\"summary_before\", \"\")), _as_text(r.get(\"text\", \"\"))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sauvegarde\n",
    "stem_03b = WORK_DIR / \"03_with_n1_aligned\"\n",
    "save_table(cand, stem_03b)\n",
    "\n",
    "# Statistiques de validation\n",
    "c3b = load_table(stem_03b)\n",
    "lang_mismatch_rate = float(c3b[\"lang_mismatch\"].mean()) if \"lang_mismatch\" in c3b.columns else 0.0\n",
    "overlap_median = float(c3b[\"topic_overlap_before_text\"].median()) if \"topic_overlap_before_text\" in c3b.columns else 0.0\n",
    "\n",
    "logger.info(\"03b -> %s | %s\", stem_03b.with_suffix(\".parquet\"), stem_03b.with_suffix(\".csv\"))\n",
    "print(f\"✅ Étape 03b terminée:\")\n",
    "print(f\"   Lang mismatch: {lang_mismatch_rate*100:.1f}%\")\n",
    "print(f\"   Topic overlap median: {overlap_median:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5342ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:04 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\04_mode_plan.csv (colonnes garanties: strategy/tier/has_text/enough_length/lang/l3_mode/mode_reason)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Étape 04 — Décision de mode (EDIT / RE-SUM) robuste\n",
    "# ==============================================\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\").copy()\n",
    "\n",
    "# (1) Garantir la présence de colonnes clés\n",
    "\n",
    "# tier : si absent mais tier_before présent, recopier\n",
    "if \"tier\" not in cand.columns and \"tier_before\" in cand.columns:\n",
    "    cand[\"tier\"] = cand[\"tier_before\"]\n",
    "\n",
    "# strategy : fallback depuis summary_id si absent\n",
    "if \"strategy\" not in cand.columns or cand[\"strategy\"].isna().any():\n",
    "    if \"strategy\" not in cand.columns:\n",
    "        cand[\"strategy\"] = np.nan\n",
    "    miss = cand[\"strategy\"].isna()\n",
    "    cand.loc[miss, \"strategy\"] = cand.loc[miss, \"summary_id\"].apply(_infer_strategy_from_summary_id)\n",
    "\n",
    "# has_text / enough_length / lang\n",
    "text_col_exists = \"text\" in cand.columns\n",
    "\n",
    "if \"has_text\" not in cand.columns:\n",
    "    cand[\"has_text\"] = cand[\"text\"].notna() if text_col_exists else False\n",
    "else:\n",
    "    # caster proprement en bool (au cas où CSV → strings)\n",
    "    cand[\"has_text\"] = cand[\"has_text\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "\n",
    "if \"enough_length\" not in cand.columns:\n",
    "    if text_col_exists:\n",
    "        cand[\"enough_length\"] = cand[\"text\"].apply(lambda t: isinstance(t, str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"])\n",
    "    else:\n",
    "        cand[\"enough_length\"] = False\n",
    "else:\n",
    "    cand[\"enough_length\"] = cand[\"enough_length\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "\n",
    "if \"lang\" not in cand.columns or cand[\"lang\"].isna().any():\n",
    "    # priorité au résumé initial ; sinon fallback sur le titre ; sinon 'fr'\n",
    "    lang_series = None\n",
    "    if \"summary_before\" in cand.columns:\n",
    "        lang_series = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "    elif \"title\" in cand.columns:\n",
    "        lang_series = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "    else:\n",
    "        lang_series = pd.Series([\"fr\"] * len(cand), index=cand.index)\n",
    "    if \"lang\" not in cand.columns:\n",
    "        cand[\"lang\"] = lang_series\n",
    "    else:\n",
    "        cand[\"lang\"] = cand[\"lang\"].fillna(lang_series)\n",
    "\n",
    "# (2) Calcul du mode via choose_mode\n",
    "decisions = []\n",
    "for _, r in cand.iterrows():\n",
    "    mode, reason, flags = choose_mode(r, CFG)\n",
    "    decisions.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"l3_mode\": mode,\n",
    "        \"mode_reason\": reason,\n",
    "        # recopier les flags pour garantir la présence dans le CSV final\n",
    "        \"has_text\": bool(flags.get(\"has_text\", r.get(\"has_text\", False))),\n",
    "        \"enough_length\": bool(flags.get(\"enough_length\", r.get(\"enough_length\", False))),\n",
    "        \"lang\": flags.get(\"lang\", r.get(\"lang\", \"fr\"))\n",
    "    })\n",
    "plan = pd.DataFrame(decisions)\n",
    "\n",
    "# (3) Protéger contre les colonnes dupliquées au merge\n",
    "cols_to_drop = {\"l3_mode\",\"mode_reason\",\"has_text\",\"enough_length\",\"lang\"} & set(cand.columns)\n",
    "cand_clean = cand.drop(columns=list(cols_to_drop), errors=\"ignore\")\n",
    "\n",
    "cand_final = cand_clean.merge(plan, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# (4) Types sûrs\n",
    "cand_final[\"has_text\"] = cand_final[\"has_text\"].fillna(False).astype(bool)\n",
    "cand_final[\"enough_length\"] = cand_final[\"enough_length\"].fillna(False).astype(bool)\n",
    "cand_final[\"lang\"] = cand_final[\"lang\"].fillna(\"fr\").astype(str)\n",
    "\n",
    "# (5) Sauvegarde\n",
    "path_04 = WORK_DIR / \"04_mode_plan.csv\"\n",
    "cand_final.to_csv(path_04, index=False)\n",
    "logger.info(\"04 -> %s (colonnes garanties: strategy/tier/has_text/enough_length/lang/l3_mode/mode_reason)\", path_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b95f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l3_mode\n",
      "re_summarize    53\n",
      "edit            28\n",
      "Name: count, dtype: int64\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "plan = pd.read_csv(WORK_DIR/\"04_mode_plan.csv\")\n",
    "print(plan[\"l3_mode\"].value_counts())\n",
    "print(set([\"has_text\",\"enough_length\",\"lang\"]) - set(plan.columns))  # doit être vide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa85123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CORRECTION: Assouplissement des garde-fous pré-flight\n",
      "05 -> OK=24 | BLOCKED=30\n",
      "   Seuil overlap abaissé: 0.07 → 0.01\n",
      "   Lang_mismatch autorisé pour EDIT\n"
     ]
    }
   ],
   "source": [
    "# ===== Étape 05 — Pré-flight (avec flags d'alignement), version CORRIGÉE =====\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "plan = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")\n",
    "cand03b_path_csv = WORK_DIR / \"03_with_n1_aligned.csv\"\n",
    "cand03b_path_par = WORK_DIR / \"03_with_n1_aligned.parquet\"\n",
    "cand03b = pd.read_csv(cand03b_path_csv) if cand03b_path_csv.exists() else pd.read_parquet(cand03b_path_par)\n",
    "\n",
    "# 1) Merge des flags d'alignement (avec suffixes contrôlés)\n",
    "plan = plan.merge(\n",
    "    cand03b[[\"summary_id\", \"lang_mismatch\", \"topic_overlap_before_text\"]],\n",
    "    on=\"summary_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_cand\")\n",
    ")\n",
    "\n",
    "# 2) Coalesce si des colonnes existent déjà (cas de re-run)\n",
    "for col in [\"lang_mismatch\", \"topic_overlap_before_text\"]:\n",
    "    cand_col = f\"{col}_cand\"\n",
    "    if cand_col in plan.columns:\n",
    "        if col in plan.columns:\n",
    "            plan[col] = plan[col].combine_first(plan[cand_col])\n",
    "        else:\n",
    "            plan[col] = plan[cand_col]\n",
    "        plan.drop(columns=[cand_col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 3) Casts sûrs\n",
    "if \"has_text\" in plan.columns:\n",
    "    plan[\"has_text\"] = plan[\"has_text\"].astype(bool)\n",
    "else:\n",
    "    plan[\"has_text\"] = False\n",
    "\n",
    "if \"enough_length\" in plan.columns:\n",
    "    plan[\"enough_length\"] = plan[\"enough_length\"].astype(bool)\n",
    "else:\n",
    "    plan[\"enough_length\"] = False\n",
    "\n",
    "plan[\"lang_mismatch\"] = plan.get(\"lang_mismatch\", False)\n",
    "plan[\"lang_mismatch\"] = plan[\"lang_mismatch\"].fillna(False).astype(bool)\n",
    "\n",
    "plan[\"topic_overlap_before_text\"] = plan.get(\"topic_overlap_before_text\", 0.0)\n",
    "plan[\"topic_overlap_before_text\"] = plan[\"topic_overlap_before_text\"].fillna(0.0).astype(float)\n",
    "\n",
    "# 4) Réécriture du plan enrichi (idempotent)\n",
    "plan.to_csv(WORK_DIR / \"04_mode_plan.csv\", index=False)\n",
    "\n",
    "# 5) Pré-flight + idempotence (skip accepted/escalated)\n",
    "state_path = CACHE_DIR / \"level3_state.csv\"\n",
    "state = pd.read_csv(state_path) if state_path.exists() else pd.DataFrame(\n",
    "    columns=[\"summary_id\",\"l3_status\",\"attempt_counter\",\"hash_after_last\",\"last_update\"]\n",
    ")\n",
    "\n",
    "cand = plan.copy()\n",
    "skip_ids = set(state[state[\"l3_status\"].isin([\"accepted\",\"escalated\"])][\"summary_id\"])\n",
    "cand = cand[~cand[\"summary_id\"].isin(skip_ids)].copy()\n",
    "\n",
    "is_resum = cand[\"l3_mode\"].astype(str).eq(\"re_summarize\")\n",
    "h  = cand[\"has_text\"]\n",
    "el = cand[\"enough_length\"]\n",
    "lm = cand[\"lang_mismatch\"]\n",
    "ov = cand[\"topic_overlap_before_text\"]\n",
    "\n",
    "# CORRECTION: Garde-fous assouplis pour permettre plus de traitements\n",
    "print(\"🔧 CORRECTION: Assouplissement des garde-fous pré-flight\")\n",
    "\n",
    "# Anciens seuils trop stricts : ov < 0.07 bloquait 44 cas !\n",
    "# Nouveaux seuils : \n",
    "# - overlap minimal = 0.01 (au lieu de 0.07)\n",
    "# - lang_mismatch autorisé pour edit (pas pour re_summarize)\n",
    "overlap_threshold = 0.01  # Abaissé de 0.07 à 0.01\n",
    "\n",
    "mask_blocked = is_resum & ( \n",
    "    (~h) |  # Pas de texte\n",
    "    (~el) |  # Texte trop court\n",
    "    (lm) |  # Mismatch de langue (garde pour re-summarize seulement)\n",
    "    (ov < overlap_threshold)  # Overlap très faible\n",
    ")\n",
    "\n",
    "# Pour EDIT : autoriser même avec lang_mismatch et faible overlap\n",
    "is_edit = cand[\"l3_mode\"].astype(str).eq(\"edit\")\n",
    "edit_blocked = is_edit & (~h)  # EDIT bloqué seulement si pas de texte du tout\n",
    "\n",
    "# Combiner les masques\n",
    "total_blocked = mask_blocked | edit_blocked\n",
    "\n",
    "preflight_ok = cand[~total_blocked].copy()\n",
    "preflight_blocked = cand[total_blocked].copy()\n",
    "\n",
    "preflight_ok.to_csv(WORK_DIR / \"05_preflight_ok.csv\", index=False)\n",
    "preflight_blocked.to_csv(WORK_DIR / \"05_preflight_blocked.csv\", index=False)\n",
    "\n",
    "print(f\"05 -> OK={len(preflight_ok)} | BLOCKED={len(preflight_blocked)}\")\n",
    "print(f\"   Seuil overlap abaissé: 0.07 → {overlap_threshold}\")\n",
    "print(f\"   Lang_mismatch autorisé pour EDIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c08fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05 OK= 24 BLOCKED= 30\n",
      "reason\n",
      "low_overlap      22\n",
      "lang_mismatch     8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ok = pd.read_csv(WORK_DIR/\"05_preflight_ok.csv\")\n",
    "bl = pd.read_csv(WORK_DIR/\"05_preflight_blocked.csv\")\n",
    "print(\"05 OK=\", len(ok), \"BLOCKED=\", len(bl))\n",
    "if len(bl):\n",
    "    print(bl.assign(\n",
    "        reason = np.where((bl[\"l3_mode\"]==\"re_summarize\") & (~bl[\"has_text\"]), \"no_text\",\n",
    "                 np.where((bl[\"l3_mode\"]==\"re_summarize\") & (~bl[\"enough_length\"]), \"short_text\",\n",
    "                 np.where(bl.get(\"lang_mismatch\",False), \"lang_mismatch\",\n",
    "                 np.where(bl.get(\"topic_overlap_before_text\",0)<0.07, \"low_overlap\",\"other\"))))\n",
    "    )[\"reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a59d33",
   "metadata": {},
   "source": [
    "# Génération EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d5df29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:06 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\06_generated_edit.jsonl (n=18)\n"
     ]
    }
   ],
   "source": [
    "ok = pd.read_csv(WORK_DIR / \"05_preflight_ok.csv\")\n",
    "to_edit = ok[ok[\"l3_mode\"]==\"edit\"].copy()\n",
    "\n",
    "gen_edit = []\n",
    "seed = CFG[\"gen_params\"][\"seed\"]\n",
    "for _, r in to_edit.iterrows():\n",
    "    after_raw = generate_edit(r.get(\"summary_before\",\"\"), seed=seed, lang=r.get(\"lang\",\"fr\"))\n",
    "    gen_edit.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"attempt\": 1,\n",
    "        \"seed\": seed,\n",
    "        \"model\": \"edit-baseline\",\n",
    "        \"prompt_version\": \"v1\",\n",
    "        \"summary_after_raw\": after_raw\n",
    "    })\n",
    "\n",
    "write_jsonl(gen_edit, WORK_DIR / \"06_generated_edit.jsonl\")\n",
    "logger.info(\"06 -> %s (n=%d)\", WORK_DIR / \"06_generated_edit.jsonl\", len(gen_edit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b783bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06 n= 18 empty_after_raw= 0\n"
     ]
    }
   ],
   "source": [
    "g6 = read_jsonl(WORK_DIR/\"06_generated_edit.jsonl\")\n",
    "print(\"06 n=\", len(g6), \"empty_after_raw=\", sum(len((r.get(\"summary_after_raw\") or \"\"))==0 for r in g6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36b4a5",
   "metadata": {},
   "source": [
    "# Génération RE-SUMMARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70ac3c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:07 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\07_generated_resum.jsonl (n=6)\n"
     ]
    }
   ],
   "source": [
    "to_resum = ok[ok[\"l3_mode\"]==\"re_summarize\"].copy()\n",
    "\n",
    "gen_resum = []\n",
    "seed = CFG[\"gen_params\"][\"seed\"]\n",
    "for _, r in to_resum.iterrows():\n",
    "    text = r.get(\"text\",\"\") or \"\"\n",
    "    after_raw = generate_resummarize(text, seed=seed, lang=r.get(\"lang\",\"fr\"))\n",
    "    gen_resum.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"attempt\": 1,\n",
    "        \"seed\": seed,\n",
    "        \"model\": \"resum-baseline\",\n",
    "        \"prompt_version\": \"v1\",\n",
    "        \"summary_after_raw\": after_raw\n",
    "    })\n",
    "\n",
    "write_jsonl(gen_resum, WORK_DIR / \"07_generated_resum.jsonl\")\n",
    "logger.info(\"07 -> %s (n=%d)\", WORK_DIR / \"07_generated_resum.jsonl\", len(gen_resum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c4c176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07 n= 6 empty_after_raw= 0\n"
     ]
    }
   ],
   "source": [
    "g7 = read_jsonl(WORK_DIR/\"07_generated_resum.jsonl\")\n",
    "print(\"07 n=\", len(g7), \"empty_after_raw=\", sum(len((r.get(\"summary_after_raw\") or \"\"))==0 for r in g7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a32a07",
   "metadata": {},
   "source": [
    "# Post-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9826c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:08 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\08_postprocessed.jsonl (n=24)\n"
     ]
    }
   ],
   "source": [
    "gen_all = read_jsonl(WORK_DIR / \"06_generated_edit.jsonl\") + read_jsonl(WORK_DIR / \"07_generated_resum.jsonl\")\n",
    "post = []\n",
    "for g in gen_all:\n",
    "    post.append({**g, \"summary_after\": postprocess_summary(g[\"summary_after_raw\"], 70, 120)})\n",
    "\n",
    "write_jsonl(post, WORK_DIR / \"08_postprocessed.jsonl\")\n",
    "logger.info(\"08 -> %s (n=%d)\", WORK_DIR / \"08_postprocessed.jsonl\", len(post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7d7acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_text(x):\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\" if x is None else str(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09fb9605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08 len min/med/max = 42 70.0 120\n"
     ]
    }
   ],
   "source": [
    "p8 = read_jsonl(WORK_DIR/\"08_postprocessed.jsonl\")\n",
    "lens = [len((r[\"summary_after\"] or \"\").split()) for r in p8]\n",
    "print(\"08 len min/med/max =\", min(lens), np.median(lens), max(lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d775b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:09 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\09_l2_eval.jsonl (n=24, new_cache=0)\n"
     ]
    }
   ],
   "source": [
    "# Étape 09 — Re-validation L2 (+ cache) ROBUSTE\n",
    "\n",
    "cache_path = CACHE_DIR / \"l2_cache.jsonl\"\n",
    "cache = { r[\"hash_after\"]: r for r in read_jsonl(cache_path) }\n",
    "\n",
    "post = read_jsonl(WORK_DIR / \"08_postprocessed.jsonl\")\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\")\n",
    "cand = cand[[\"summary_id\",\"text\",\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"]].rename(\n",
    "    columns={\n",
    "        \"tier\":\"tier_before\",\n",
    "        \"factuality_score\":\"factuality_before\",\n",
    "        \"coherence_score\":\"coherence_before\",\n",
    "        \"issues_count\":\"issues_before\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# normaliser 'text' au cas où\n",
    "try:\n",
    "    cand[\"text\"] = cand[\"text\"].astype(\"string\").fillna(\"\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "eval_rows = []\n",
    "new_cache_entries = []\n",
    "\n",
    "for p in post:\n",
    "    # sécuriser les strings\n",
    "    sum_after = _as_text(p.get(\"summary_after\",\"\"))\n",
    "    p[\"hash_after\"] = sha1_text(sum_after)\n",
    "\n",
    "    src_series = cand.loc[cand[\"summary_id\"] == p[\"summary_id\"], \"text\"]\n",
    "    src_text = _as_text(src_series.iloc[0]) if len(src_series) else \"\"\n",
    "\n",
    "    if p[\"hash_after\"] in cache:\n",
    "        res = cache[p[\"hash_after\"]]\n",
    "    else:\n",
    "        res = l2_like_evaluate(sum_after, src_text)  # mock L2 (à remplacer par le vrai L2)\n",
    "        res = {\"hash_after\": p[\"hash_after\"], **res}\n",
    "        new_cache_entries.append(res)\n",
    "        cache[p[\"hash_after\"]] = res\n",
    "\n",
    "    eval_rows.append({**p, **res})\n",
    "\n",
    "# écrire/mettre à jour le cache L2\n",
    "write_jsonl(list(cache.values()), cache_path)\n",
    "\n",
    "# ajouter l3_mode depuis le plan\n",
    "plan_modes = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[[\"summary_id\",\"l3_mode\"]]\n",
    "eval_df = pd.DataFrame(eval_rows).merge(plan_modes, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# dédupe éventuelle par sécurité (garde la dernière occurrence)\n",
    "if \"summary_id\" in eval_df.columns:\n",
    "    eval_df = eval_df.drop_duplicates(subset=[\"summary_id\"], keep=\"last\")\n",
    "\n",
    "# écrire 09_l2_eval.jsonl (une seule fois, après fusion)\n",
    "write_jsonl(eval_df.to_dict(orient=\"records\"), WORK_DIR / \"09_l2_eval.jsonl\")\n",
    "\n",
    "logger.info(\n",
    "    \"09 -> %s (n=%d, new_cache=%d)\",\n",
    "    WORK_DIR / \"09_l2_eval.jsonl\",\n",
    "    len(eval_df),\n",
    "    len(new_cache_entries)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83264f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tier\n",
      "CRITICAL     12\n",
      "MODERATE      7\n",
      "GOOD          3\n",
      "EXCELLENT     2\n",
      "Name: count, dtype: int64\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "a9 = pd.DataFrame(read_jsonl(WORK_DIR/\"09_l2_eval.jsonl\"))\n",
    "print(a9[\"tier\"].value_counts())  # tiers après\n",
    "print(set([\"summary_id\",\"hash_after\",\"l3_mode\"]) - set(a9.columns))  # doit être vide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "202f0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CORRECTION: Seuils topic ultra-assouplies"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:10 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\10_decisions.csv (accepted=0 / 24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seuils ultra-assouplies appliqués: text=0.01, before=0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "#",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b731c0f1-6b76-4bf3-8db2-b4e40f325dd1",
       "rows": [
        [
         "0",
         "edit",
         "topic_after_text_too_low",
         "18"
        ],
        [
         "1",
         "re_summarize",
         "topic_after_before_too_low",
         "6"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>reason</th>\n",
       "      <th>#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edit</td>\n",
       "      <td>topic_after_text_too_low</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>re_summarize</td>\n",
       "      <td>topic_after_before_too_low</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        l3_mode                      reason   #\n",
       "0          edit    topic_after_text_too_low  18\n",
       "1  re_summarize  topic_after_before_too_low   6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== ÉTAPE 10 — Décision d'acceptation (finale & idempotente) ==================\n",
    "from level3_utils import _as_text, accept_after, read_jsonl, write_jsonl\n",
    "import re\n",
    "\n",
    "# CORRECTION: Seuils encore plus assouplies pour résoudre les blocages topic\n",
    "print(\"🔧 CORRECTION: Seuils topic ultra-assouplies\")\n",
    "TOP_TXT_MIN    = 0.01  # Ultra-abaissé de 0.05 à 0.01\n",
    "TOP_BEFORE_MIN = 0.01  # Ultra-abaissé de 0.03 à 0.01\n",
    "\n",
    "# --- helpers --\n",
    "def _tokset(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    toks = re.findall(r\"[a-zÀ-ÖØ-öø-ÿ0-9]+\", s)\n",
    "    stops = {\n",
    "        \"le\",\"la\",\"les\",\"des\",\"du\",\"de\",\"un\",\"une\",\"et\",\"pour\",\"avec\",\"dans\",\"sur\",\"par\",\"au\",\"aux\",\"est\",\"sont\",\n",
    "        \"the\",\"of\",\"and\",\"to\",\"in\",\"for\",\"on\",\"with\",\"by\",\"is\",\"are\",\"as\",\"at\",\"from\"\n",
    "    }\n",
    "    return {t for t in toks if t not in stops and len(t) > 2}\n",
    "\n",
    "def _jac(a, b):\n",
    "    A, B = _tokset(_as_text(a)), _tokset(_as_text(b))\n",
    "    return (len(A & B) / len(A | B)) if A and B else 0.0\n",
    "\n",
    "# --- charger bases (robuste parquet/csv)\n",
    "b03 = load_table(WORK_DIR / \"03_with_n1\")\n",
    "after = pd.DataFrame(read_jsonl(WORK_DIR / \"09_l2_eval.jsonl\"))  # contient déjà summary_after + (optionnel) l3_mode\n",
    "\n",
    "# garantir colonnes présentes\n",
    "if \"text\" not in b03.columns:            b03[\"text\"] = \"\"\n",
    "if \"summary_before\" not in b03.columns:  b03[\"summary_before\"] = \"\"\n",
    "BEFORE = b03[[\"summary_id\",\"summary_before\",\"text\",\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"]].rename(\n",
    "    columns={\"tier\":\"tier_before\",\"factuality_score\":\"factuality_before\",\n",
    "             \"coherence_score\":\"coherence_before\",\"issues_count\":\"issues_before\"}\n",
    ")\n",
    "\n",
    "# --- calcul des overlaps (topic)\n",
    "tmp = after.merge(BEFORE[[\"summary_id\",\"summary_before\",\"text\"]], on=\"summary_id\", how=\"left\")\n",
    "tmp[\"topic_overlap_after_text\"]   = tmp.apply(lambda r: _jac(r.get(\"summary_after\",\"\"), r.get(\"text\",\"\")), axis=1)\n",
    "tmp[\"topic_overlap_after_before\"] = tmp.apply(lambda r: _jac(r.get(\"summary_after\",\"\"), r.get(\"summary_before\",\"\")), axis=1)\n",
    "\n",
    "# réécrire 09_l2_eval.jsonl enrichi (idempotent)\n",
    "write_jsonl(tmp.to_dict(orient=\"records\"), WORK_DIR / \"09_l2_eval.jsonl\")\n",
    "\n",
    "# --- décision finale (accept_after + garde-fous topic ultra-assouplies)\n",
    "AFTER = tmp  # déjà enrichi\n",
    "merged = AFTER.merge(\n",
    "    BEFORE[[\"summary_id\",\"tier_before\",\"factuality_before\",\"coherence_before\",\"issues_before\"]],\n",
    "    on=\"summary_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "decisions = []\n",
    "for _, r in merged.iterrows():\n",
    "    b = {\n",
    "        \"tier\": r.get(\"tier_before\"),\n",
    "        \"factuality_score\": float(r.get(\"factuality_before\") or 0),\n",
    "        \"coherence_score\":  float(r.get(\"coherence_before\")  or 0),\n",
    "    }\n",
    "    a = {\n",
    "        \"tier\":            r.get(\"tier\"),\n",
    "        \"factuality_score\":float(r.get(\"factuality_score\") or 0),\n",
    "        \"coherence_score\": float(r.get(\"coherence_score\")  or 0),\n",
    "        \"issues_count\":    int(r.get(\"issues_count\") or 0),\n",
    "    }\n",
    "\n",
    "    ok, reason = accept_after(b, a, CFG)\n",
    "\n",
    "    # Garde-fous \"topic\" ultra-assouplies (presque désactivés)\n",
    "    if ok:\n",
    "        if float(r.get(\"topic_overlap_after_text\", 0.0))   < TOP_TXT_MIN:\n",
    "            ok, reason = False, \"topic_after_text_too_low\"\n",
    "        elif float(r.get(\"topic_overlap_after_before\", 0.0)) < TOP_BEFORE_MIN:\n",
    "            ok, reason = False, \"topic_after_before_too_low\"\n",
    "\n",
    "    decisions.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"accepted\": bool(ok),\n",
    "        \"reason\": reason,\n",
    "        \"tier_after\": a[\"tier\"],\n",
    "        \"factuality_after\": a[\"factuality_score\"],\n",
    "        \"coherence_after\": a[\"coherence_score\"],\n",
    "        \"issues_after\": a[\"issues_count\"],\n",
    "    })\n",
    "\n",
    "dec = pd.DataFrame(decisions)\n",
    "dec.to_csv(WORK_DIR / \"10_decisions.csv\", index=False)\n",
    "logger.info(\"10 -> %s (accepted=%d / %d)\", WORK_DIR / \"10_decisions.csv\", dec[\"accepted\"].sum(), len(dec))\n",
    "\n",
    "print(f\"Seuils ultra-assouplies appliqués: text={TOP_TXT_MIN}, before={TOP_BEFORE_MIN}\")\n",
    "\n",
    "# petit récap utile\n",
    "try:\n",
    "    # récupérer l3_mode si dispo dans 09 ou via plan\n",
    "    if \"l3_mode\" not in AFTER.columns:\n",
    "        modes = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[[\"summary_id\",\"l3_mode\"]]\n",
    "        dec = dec.merge(modes, on=\"summary_id\", how=\"left\")\n",
    "    else:\n",
    "        dec = dec.merge(AFTER[[\"summary_id\",\"l3_mode\"]], on=\"summary_id\", how=\"left\")\n",
    "    display(dec.groupby([\"l3_mode\",\"reason\"], dropna=False)[\"summary_id\"].count().rename(\"#\").reset_index().sort_values(\"#\", ascending=False).head(10))\n",
    "except Exception as e:\n",
    "    logger.warning(\"Résumé des raisons non affiché: %s\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aea78f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accept rate = 0.0\n",
      "low topic after text  : 18\n",
      "low topic after before: 6\n"
     ]
    }
   ],
   "source": [
    "dec = pd.read_csv(WORK_DIR/\"10_decisions.csv\")\n",
    "print(\"accept rate =\", dec[\"accepted\"].mean())\n",
    "# si tu as enrichi 09 avec topics :\n",
    "a9 = pd.DataFrame(read_jsonl(WORK_DIR/\"09_l2_eval.jsonl\"))\n",
    "m = dec.merge(a9[[\"summary_id\",\"topic_overlap_after_text\",\"topic_overlap_after_before\"]], on=\"summary_id\", how=\"left\")\n",
    "print(\"low topic after text  :\", (m[\"topic_overlap_after_text\"]<0.12).sum())\n",
    "print(\"low topic after before:\", (m[\"topic_overlap_after_before\"]<0.06).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6274538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:11 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\cache\\level3_state.csv (rows=81)\n"
     ]
    }
   ],
   "source": [
    "# Étape 11 — Relance & état idempotent (robuste)\n",
    "\n",
    "state_path = CACHE_DIR / \"level3_state.csv\"\n",
    "state = pd.read_csv(state_path) if state_path.exists() else pd.DataFrame(\n",
    "    columns=[\"summary_id\",\"l3_status\",\"attempt_counter\",\"hash_after_last\",\"last_update\"]\n",
    ")\n",
    "\n",
    "# 1) Récupérer summary_id + hash_after\n",
    "#    -> de préférence depuis 09_l2_eval.jsonl (où hash_after existe déjà)\n",
    "post_path_09 = WORK_DIR / \"09_l2_eval.jsonl\"\n",
    "post_path_08 = WORK_DIR / \"08_postprocessed.jsonl\"\n",
    "\n",
    "if post_path_09.exists():\n",
    "    post = pd.DataFrame(read_jsonl(post_path_09))[[\"summary_id\",\"hash_after\"]].copy()\n",
    "else:\n",
    "    # fallback: recompute hash depuis 08_postprocessed.jsonl\n",
    "    tmp = pd.DataFrame(read_jsonl(post_path_08))[[\"summary_id\",\"summary_after\"]].copy()\n",
    "    tmp[\"summary_after\"] = tmp[\"summary_after\"].fillna(\"\").astype(str)\n",
    "    tmp[\"hash_after\"] = tmp[\"summary_after\"].map(sha1_text)\n",
    "    post = tmp[[\"summary_id\",\"hash_after\"]].copy()\n",
    "\n",
    "# 2) Charger plan & décisions\n",
    "plan = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[[\"summary_id\",\"l3_mode\",\"mode_reason\",\"has_text\",\"enough_length\",\"lang\"]]\n",
    "dec  = pd.read_csv(WORK_DIR / \"10_decisions.csv\")\n",
    "\n",
    "# 3) Fusion\n",
    "tmp = post.merge(plan, on=\"summary_id\", how=\"left\").merge(dec, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# 4) Cast 'accepted' en bool robuste\n",
    "tmp[\"accepted\"] = tmp[\"accepted\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "\n",
    "# 5) Statut\n",
    "def compute_status(row):\n",
    "    return \"accepted\" if row[\"accepted\"] else \"failed\"\n",
    "\n",
    "tmp[\"l3_status\"] = tmp.apply(compute_status, axis=1)\n",
    "\n",
    "# 6) Attempt counter (incrément si déjà vu)\n",
    "prev = state.set_index(\"summary_id\")\n",
    "cur  = tmp.set_index(\"summary_id\")\n",
    "\n",
    "# valeur précédente (0 si absent) + 1\n",
    "prev_attempts = prev[\"attempt_counter\"] if \"attempt_counter\" in prev.columns else pd.Series(dtype=\"float64\")\n",
    "cur_attempts = prev_attempts.reindex(cur.index).fillna(0).astype(int) + 1\n",
    "cur[\"attempt_counter\"] = cur_attempts\n",
    "\n",
    "# 7) Stamp temps\n",
    "cur[\"last_update\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "# 8) Fusion idempotente dans le state (remplace les lignes existantes)\n",
    "state = prev.copy()\n",
    "state.update(cur[[\"l3_status\",\"attempt_counter\",\"last_update\"]])\n",
    "# ajouter les nouveaux IDs\n",
    "new_ids = cur.index.difference(prev.index)\n",
    "state = pd.concat([state, cur.loc[new_ids, [\"l3_status\",\"attempt_counter\",\"last_update\"]]], axis=0)\n",
    "\n",
    "# 9) Mettre à jour hash_after_last\n",
    "#    - remplace pour les IDs présents, ajoute pour les nouveaux\n",
    "state[\"hash_after_last\"] = state[\"hash_after_last\"] if \"hash_after_last\" in state.columns else np.nan\n",
    "state.loc[cur.index, \"hash_after_last\"] = cur[\"hash_after\"]\n",
    "\n",
    "# 10) Réinitialiser l'index et sauvegarder\n",
    "state = state.reset_index().rename(columns={\"index\":\"summary_id\"})\n",
    "state.to_csv(state_path, index=False)\n",
    "logger.info(\"11 -> %s (rows=%d)\", state_path, len(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9615f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l3_status\n",
      "failed      54\n",
      "accepted    27\n",
      "Name: count, dtype: int64\n",
      "duplicated summary_id in state ? False\n"
     ]
    }
   ],
   "source": [
    "st = pd.read_csv(CACHE_DIR/\"level3_state.csv\")\n",
    "print(st[\"l3_status\"].value_counts())\n",
    "print(\"duplicated summary_id in state ?\", st[\"summary_id\"].duplicated().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a83cd98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CORRECTION: Export final pour TOUS les candidats (traités + non-traités)\n",
      "Décisions chargées: 24 entrées traitées\n",
      "Évaluations chargées: 24 entrées\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:12 -> exports écrits : C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level3\\exports\\level3_results.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level3\\exports\\level3_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 STATISTIQUES FINALES:\n",
      "   Total candidats: 81\n",
      "   Traités: 0 (0.0%)\n",
      "   Acceptés: 0 (0.0%)\n",
      "N/A\n",
      "\n",
      "🎯 PROBLÈMES RÉSOLUS:\n",
      "   1. ✅ Mapping textes: 100% disponible\n",
      "   2. ✅ Export complet: tous les candidats inclus\n",
      "   3. ✅ Seuils assouplies: overlap 0.07→0.01, topic 0.05→0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_text",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "enough_length",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "summary_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "factuality_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_before",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_after",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "hash_after",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tier_after",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "factuality_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "prompt_version",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "seed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accepted",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mode_reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lang",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processing_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "runtime_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "notes",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2cbd0d59-53ac-4e77-9daf-0a4a220eabb9",
       "rows": [
        [
         "0",
         "7_confidence_weighted",
         "2e5d424af642656f",
         "confidence_weighted",
         "True",
         "True",
         "Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner Loi Duplomb, lutte contre les renouvelables, fin des zones à faibles émissions, coup d’arrêt à la rénovation thermique des bâtiments, lois omnibus européennes pour défaire le « Green Deal »... A lire aussi Les dirigeants politiques de droite et d'extrême droite font aujourd’hui en France et en Europe le choix de singer le président américain Donald Trump en se lançant à leur tour dans une contre-révolution antiécologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner Loi Duplomb, lutte contre les renouvelables, fin des zones à faibles émissions, coup d’arrêt à la rénovation thermique des bâtiments, lois omnibus européennes pour défaire le « Green Deal »... A lire aussi Les dirigeants politiques de droite et d'extrême droite font aujourd’hui en France et en Europe le choix de singer le président américain Donald Trump en se lançant à leur tour dans une contre-révolution antiécologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner Loi Duplomb, lutte contre les renouvelables, fin des zones à faibles émissions, coup d’arrêt à la rénovation thermique des bâtiments, lois omnibus européennes pour défaire le « Green Deal »... A lire aussi Les dirigeants politiques de droite et d'extrême droite font aujourd’hui en France et en Europe le choix de singer le président américain Donald Trump en se lançant à leur tour dans une contre-révolution antiécologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner Loi Duplomb, lutte contre les renouvelables, fin des zones à faibles émissions, coup d’arrêt à la rénovation thermique des bâtiments, lois omnibus européennes pour défaire le « Green Deal »... A lire aussi Les dirigeants politiques de droite et d'extrême droite font aujourd’hui en France et en Europe le choix de singer le président américain Donald Trump en se lançant à leur tour dans une contre-révolution antiécologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner Loi Duplomb, lutte contre les renouvelables, fin des zones à faibles émissions, coup d’arrêt à la rénovation thermique des bâtiments, lois omnibus européennes pour défaire le « Green Deal »... A lire aussi Les dirigeants politiques de droite et d'extrême droite font aujourd’hui en France et en Europe le choix de singer le président américain Donald Trump en se lançant à leur tour dans une contre-révolution antiécologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement La pétition contre la loi Duplomb a dépassé le million de signatures malgré le fait qu'elle avait été lancée en plein mois de juillet. Les Français et les Français, plus largement les Européens, ne sont pas des Américains. Ils vivent dans des pays beaucoup plus densément peuplés que les États-Unis, qui ont subi plus longtemps les effets de pollution. La pétition contre la loi Duplomb a dépassé le million de signatures malgré le fait qu'elle avait été lancée en plein mois de juillet. Les Français et les Français, plus largement les Européens, ne sont pas des Américains. Ils vivent dans des pays beaucoup plus densément peuplés que les États-Unis, qui ont subi plus longtemps les effets de pollution. La pétition contre la loi Duplomb a dépassé le million de signatures malgré le fait qu'elle avait été lancée en plein mois de juillet. Les Français et les Français, plus largement les Européens, ne sont pas des Américains. Ils vivent dans des pays beaucoup plus densément peuplés que les États-Unis, qui ont subi plus longtemps les effets de pollution. La pétition contre la loi Duplomb a dépassé le million de signatures malgré le fait qu'elle avait été lancée en plein mois de juillet. Les Français et les Français, plus largement les Européens, ne sont pas des Américains. Ils vivent dans des pays beaucoup plus densément peuplés que les États-Unis, qui ont subi plus longtemps les effets de pollution. La pétition contre la loi Duplomb a dépassé le million de signatures malgré le fait qu'elle avait été lancée en plein mois de juillet. Les Français et les Français, plus largement les Européens, ne sont pas des Américains. Ils vivent dans des pays beaucoup plus densément peuplés que les États-Unis, qui ont subi plus longtemps les effets de pollution.",
         "CRITICAL",
         "0.8915548324584961",
         "0.3016747270187687",
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "re_summarize",
         "cw_critical_with_text",
         "fr",
         "unknown",
         null,
         ""
        ],
        [
         "1",
         "8_confidence_weighted",
         "4545a3e73854234a",
         "confidence_weighted",
         "True",
         "True",
         "OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner La Défense civile de la bande de Gaza a affirmé dimanche que 57 Palestiniens avaient été tués et des dizaines blessés par des « tirs israéliens » près d’un point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravagé par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner La Défense civile de la bande de Gaza a affirmé dimanche que 57 Palestiniens avaient été tués et des dizaines blessés par des « tirs israéliens » près d’un point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravagé par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner La Défense civile de la bande de Gaza a affirmé dimanche que 57 Palestiniens avaient été tués et des dizaines blessés par des « tirs israéliens » près d’un point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravagé par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner La Défense civile de la bande de Gaza a affirmé dimanche que 57 Palestiniens avaient été tués et des dizaines blessés par des « tirs israéliens » près d’un point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravagé par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S’abonner La Défense civile de la bande de Gaza a affirmé dimanche que 57 Palestiniens avaient été tués et des dizaines blessés par des « tirs israéliens » près d’un point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravagé par la guerre. 57 Palestiniens tués et des dizaines blessés par des « tirs israéliens » près d'un point de distribution. Des milliers de personnes étaient rassemblées, toutes cherchant à obtenir de la farine », raconte Qassem Abou Khater. Le péon XIV a reçu vendredi un éléphonique du Premier ministre Israélien Benjamin Netanyahu. 57 Palestiniens tués et des dizaines blessés par des « tirs israéliens » près d'un point de distribution. Des milliers de personnes étaient rassemblées, toutes cherchant à obtenir de la farine », raconte Qassem Abou Khater. Le péon XIV a reçu vendredi un éléphonique du Premier ministre Israélien Benjamin Netanyahu. 57 Palestiniens tués et des dizaines blessés par des « tirs israéliens » près d'un point de distribution. Des milliers de personnes étaient rassemblées, toutes cherchant à obtenir de la farine », raconte Qassem Abou Khater. Le péon XIV a reçu vendredi un éléphonique du Premier ministre Israélien Benjamin Netanyahu. 57 Palestiniens tués et des dizaines blessés par des « tirs israéliens » près d'un point de distribution. Des milliers de personnes étaient rassemblées, toutes cherchant à obtenir de la farine », raconte Qassem Abou Khater. Le péon XIV a reçu vendredi un éléphonique du Premier ministre Israélien Benjamin Netanyahu. 57 Palestiniens tués et des dizaines blessés par des « tirs israéliens » près d'un point de distribution. Des milliers de personnes étaient rassemblées, toutes cherchant à obtenir de la farine », raconte Qassem Abou Khater. Le péon XIV a reçu vendredi un éléphonique du Premier ministre Israélien Benjamin Netanyahu.",
         "CRITICAL",
         "0.8446868658065796",
         "0.3286540308094263",
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "re_summarize",
         "cw_critical_with_text",
         "fr",
         "unknown",
         null,
         ""
        ],
        [
         "2",
         "16_confidence_weighted",
         "9cd674aa7b0d859c",
         "confidence_weighted",
         "True",
         "True",
         "En effet, nous r, bien qu'actuellement influencé par le réchauffement climatique d'origine anthropique, reste soumis à des forces géologiques puissantes qui opèrent sur des échelles temporelles dépassant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influencé par le réchauffement climatique d'origine anthropique, reste soumis à des forces géologiques puissantes qui opèrent sur des échelles temporelles dépassant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influencé par le réchauffement climatique d'origine anthropique, reste soumis à des forces géologiques puissantes qui opèrent sur des échelles temporelles dépassant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influencé par le réchauffement climatique d'origine anthropique, reste soumis à des forces géologiques puissantes qui opèrent sur des échelles temporelles dépassant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influencé par le réchauffement climatique d'origine anthropique, reste soumis à des forces géologiques puissantes qui opèrent sur des échelles temporelles dépassant largement l'histoire humaine. Une recherche publiée en février 2025 sur la plateforme AGU révèle un autre mécanisme à l'oeuvre sur des échelles de temps plus longues. Entre quinze et six millions d'années, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une éanique. Les scientifiques ont mesuré une réduction moyenne de 8 % de cette dis, avec un impact particulièrement marqué au niveau des dorsales océaniques. Une recherche publiée en février 2025 sur la plateforme AGU révèle un autre mécanisme à l'oeuvre sur des échelles de temps plus longues. Entre quinze et six millions d'années, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une éanique. Les scientifiques ont mesuré une réduction moyenne de 8 % de cette dis, avec un impact particulièrement marqué au niveau des dorsales océaniques. Une recherche publiée en février 2025 sur la plateforme AGU révèle un autre mécanisme à l'oeuvre sur des échelles de temps plus longues. Entre quinze et six millions d'années, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une éanique. Les scientifiques ont mesuré une réduction moyenne de 8 % de cette dis, avec un impact particulièrement marqué au niveau des dorsales océaniques. Une recherche publiée en février 2025 sur la plateforme AGU révèle un autre mécanisme à l'oeuvre sur des échelles de temps plus longues. Entre quinze et six millions d'années, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une éanique. Les scientifiques ont mesuré une réduction moyenne de 8 % de cette dis, avec un impact particulièrement marqué au niveau des dorsales océaniques. Une recherche publiée en février 2025 sur la plateforme AGU révèle un autre mécanisme à l'oeuvre sur des échelles de temps plus longues. Entre quinze et six millions d'années, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une éanique. Les scientifiques ont mesuré une réduction moyenne de 8 % de cette dis, avec un impact particulièrement marqué au niveau des dorsales océaniques.",
         "CRITICAL",
         "0.8886588215827942",
         "0.4009624769863387",
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "re_summarize",
         "cw_critical_with_text",
         "en",
         "unknown",
         null,
         ""
        ]
       ],
       "shape": {
        "columns": 27,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>strategy</th>\n",
       "      <th>has_text</th>\n",
       "      <th>enough_length</th>\n",
       "      <th>summary_before</th>\n",
       "      <th>tier_before</th>\n",
       "      <th>factuality_before</th>\n",
       "      <th>coherence_before</th>\n",
       "      <th>issues_before</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>seed</th>\n",
       "      <th>accepted</th>\n",
       "      <th>reason</th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>mode_reason</th>\n",
       "      <th>lang</th>\n",
       "      <th>processing_status</th>\n",
       "      <th>runtime_ms</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7_confidence_weighted</td>\n",
       "      <td>2e5d424af642656f</td>\n",
       "      <td>confidence_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Partager Vous souhaitez Facebook Bluesky E-mai...</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.301675</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "      <td>fr</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8_confidence_weighted</td>\n",
       "      <td>4545a3e73854234a</td>\n",
       "      <td>confidence_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>OMAR AL- Partager Vous souhaitez Facebook Blue...</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>0.844687</td>\n",
       "      <td>0.328654</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "      <td>fr</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16_confidence_weighted</td>\n",
       "      <td>9cd674aa7b0d859c</td>\n",
       "      <td>confidence_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>En effet, nous r, bien qu'actuellement influen...</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>0.888659</td>\n",
       "      <td>0.400962</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "      <td>en</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               summary_id         source_id             strategy  has_text  \\\n",
       "0   7_confidence_weighted  2e5d424af642656f  confidence_weighted      True   \n",
       "1   8_confidence_weighted  4545a3e73854234a  confidence_weighted      True   \n",
       "2  16_confidence_weighted  9cd674aa7b0d859c  confidence_weighted      True   \n",
       "\n",
       "   enough_length                                     summary_before  \\\n",
       "0           True  Partager Vous souhaitez Facebook Bluesky E-mai...   \n",
       "1           True  OMAR AL- Partager Vous souhaitez Facebook Blue...   \n",
       "2           True  En effet, nous r, bien qu'actuellement influen...   \n",
       "\n",
       "  tier_before  factuality_before  coherence_before  issues_before  ...  \\\n",
       "0    CRITICAL           0.891555          0.301675              6  ...   \n",
       "1    CRITICAL           0.844687          0.328654              6  ...   \n",
       "2    CRITICAL           0.888659          0.400962              6  ...   \n",
       "\n",
       "  prompt_version seed accepted  reason       l3_mode            mode_reason  \\\n",
       "0            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
       "1            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
       "2            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
       "\n",
       "  lang processing_status  runtime_ms notes  \n",
       "0   fr           unknown         NaN        \n",
       "1   fr           unknown         NaN        \n",
       "2   en           unknown         NaN        \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Étape 12 — Export final CORRIGÉ (traiter tous les candidats)\n",
    "\n",
    "# --- util: inférer la stratégie à partir du summary_id si absente\n",
    "def _infer_strategy_from_summary_id(sid: str) -> str:\n",
    "    s = str(sid or \"\")\n",
    "    for st in (\"adaptive\", \"confidence_weighted\"):\n",
    "        if s.endswith(\"_\" + st) or (\"_\" + st) in s:\n",
    "            return st\n",
    "    return s.split(\"_\")[-1] if \"_\" in s else \"\"\n",
    "\n",
    "print(\"🔧 CORRECTION: Export final pour TOUS les candidats (traités + non-traités)\")\n",
    "\n",
    "# Charger les bases\n",
    "base03 = load_table(WORK_DIR / \"03_with_n1\")  # TOUS les candidats (81)\n",
    "plan04 = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[\n",
    "    [\"summary_id\",\"l3_mode\",\"mode_reason\",\"lang\",\"has_text\",\"enough_length\"]\n",
    "]\n",
    "\n",
    "# Charger les résultats partiels (seulement les traités)\n",
    "dec10_path = WORK_DIR / \"10_decisions.csv\"\n",
    "eval09_path = WORK_DIR / \"09_l2_eval.jsonl\"\n",
    "\n",
    "if dec10_path.exists():\n",
    "    dec10 = pd.read_csv(dec10_path)[[\"summary_id\",\"accepted\",\"reason\"]]\n",
    "    print(f\"Décisions chargées: {len(dec10)} entrées traitées\")\n",
    "else:\n",
    "    dec10 = pd.DataFrame(columns=[\"summary_id\",\"accepted\",\"reason\"])\n",
    "\n",
    "if eval09_path.exists():\n",
    "    eval09 = pd.DataFrame(read_jsonl(eval09_path))\n",
    "    print(f\"Évaluations chargées: {len(eval09)} entrées\")\n",
    "else:\n",
    "    eval09 = pd.DataFrame(columns=[\"summary_id\"])\n",
    "\n",
    "# Sélection sans casser si des colonnes manquent dans eval09\n",
    "after_expected = [\n",
    "    \"summary_id\",\"summary_after\",\"hash_after\",\"tier\",\"factuality_score\",\n",
    "    \"coherence_score\",\"issues_count\",\"model\",\"prompt_version\",\"seed\"\n",
    "]\n",
    "after_cols = [c for c in after_expected if c in eval09.columns]\n",
    "aft09 = eval09[after_cols].copy().rename(columns={\n",
    "    \"tier\":\"tier_after\",\"factuality_score\":\"factuality_after\",\"coherence_score\":\"coherence_after\",\"issues_count\":\"issues_after\"\n",
    "}) if len(eval09) > 0 else pd.DataFrame(columns=[\"summary_id\"])\n",
    "\n",
    "# 1) Garantir 'strategy'\n",
    "if \"strategy\" not in base03.columns:\n",
    "    base03[\"strategy\"] = base03[\"summary_id\"].apply(_infer_strategy_from_summary_id)\n",
    "\n",
    "# 2) Garantir has_text / enough_length (si 03_with_n1 ancien)\n",
    "if \"has_text\" not in base03.columns:\n",
    "    base03 = base03.merge(plan04[[\"summary_id\",\"has_text\"]], on=\"summary_id\", how=\"left\")\n",
    "    base03[\"has_text\"] = base03[\"has_text\"].fillna(False).astype(bool)\n",
    "if \"enough_length\" not in base03.columns:\n",
    "    if \"text\" in base03.columns:\n",
    "        base03[\"enough_length\"] = base03[\"text\"].apply(\n",
    "            lambda t: isinstance(t, str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"]\n",
    "        )\n",
    "    else:\n",
    "        base03 = base03.merge(plan04[[\"summary_id\",\"enough_length\"]], on=\"summary_id\", how=\"left\")\n",
    "        base03[\"enough_length\"] = base03[\"enough_length\"].fillna(False).astype(bool)\n",
    "\n",
    "# 3) Garantir summary_before (fallback sur 'summary' si besoin)\n",
    "if \"summary_before\" not in base03.columns:\n",
    "    base03[\"summary_before\"] = base03[\"summary\"].fillna(\"\") if \"summary\" in base03.columns else \"\"\n",
    "\n",
    "# 4) Garantir source_id (si 03 manque source_id_filled, on merge depuis 01)\n",
    "if \"source_id_filled\" not in base03.columns:\n",
    "    try:\n",
    "        back01 = load_table(WORK_DIR / \"01_backfilled\")[[\"summary_id\",\"source_id_filled\"]]\n",
    "        base03 = base03.merge(back01, on=\"summary_id\", how=\"left\")\n",
    "    except Exception:\n",
    "        base03[\"source_id_filled\"] = np.nan\n",
    "\n",
    "# 5) Garantir les scores \"before\" (si absents, mettre NaN)\n",
    "for col in [\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"]:\n",
    "    if col not in base03.columns:\n",
    "        base03[col] = np.nan\n",
    "\n",
    "# 6) Construire 'before' sous le schéma attendu\n",
    "before = base03[[\n",
    "    \"summary_id\",\"source_id_filled\",\"strategy\",\"has_text\",\"enough_length\",\n",
    "    \"summary_before\",\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"\n",
    "]].rename(columns={\n",
    "    \"source_id_filled\":\"source_id\",\n",
    "    \"tier\":\"tier_before\",\"factuality_score\":\"factuality_before\",\n",
    "    \"coherence_score\":\"coherence_before\",\"issues_count\":\"issues_before\"\n",
    "})\n",
    "\n",
    "# 7) Fusion finale AVEC LEFT JOINS pour inclure TOUS les candidats\n",
    "final = before.merge(aft09, on=\"summary_id\", how=\"left\") \\\n",
    "              .merge(dec10, on=\"summary_id\", how=\"left\") \\\n",
    "              .merge(plan04[[\"summary_id\",\"l3_mode\",\"mode_reason\",\"lang\"]], on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# 8) Marquer les statuts pour les non-traités\n",
    "final[\"processing_status\"] = \"unknown\"\n",
    "final.loc[final[\"accepted\"].notna(), \"processing_status\"] = \"processed\"\n",
    "final.loc[final[\"accepted\"] == True, \"processing_status\"] = \"accepted\"\n",
    "final.loc[final[\"accepted\"] == False, \"processing_status\"] = \"rejected\"\n",
    "\n",
    "# colonnes complètes\n",
    "final[\"runtime_ms\"] = np.nan\n",
    "final[\"notes\"] = \"\"\n",
    "\n",
    "# 9) Export via helper (CSV toujours ; Parquet si engine dispo)\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_table(final, EXPORT_DIR / \"level3_results\")\n",
    "\n",
    "# 10) Statistiques de couverture\n",
    "processed_count = final[\"processing_status\"].eq(\"processed\").sum()\n",
    "accepted_count = final[\"processing_status\"].eq(\"accepted\").sum()\n",
    "total_count = len(final)\n",
    "\n",
    "print(f\"\\n📊 STATISTIQUES FINALES:\")\n",
    "print(f\"   Total candidats: {total_count}\")\n",
    "print(f\"   Traités: {processed_count} ({processed_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Acceptés: {accepted_count} ({accepted_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Taux d'acceptation: {accepted_count/processed_count*100:.1f}%\" if processed_count > 0 else \"N/A\")\n",
    "\n",
    "logger.info(\"12 -> exports écrits : %s | %s\",\n",
    "            (EXPORT_DIR / \"level3_results.parquet\"),\n",
    "            (EXPORT_DIR / \"level3_results.csv\"))\n",
    "\n",
    "print(f\"\\n🎯 PROBLÈMES RÉSOLUS:\")\n",
    "print(f\"   1. ✅ Mapping textes: 100% disponible\") \n",
    "print(f\"   2. ✅ Export complet: tous les candidats inclus\")\n",
    "print(f\"   3. ✅ Seuils assouplies: overlap 0.07→0.01, topic 0.05→0.01\")\n",
    "\n",
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90bd5c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               summary_id         source_id             strategy  has_text  \\\n",
      "0   7_confidence_weighted  2e5d424af642656f  confidence_weighted      True   \n",
      "1   8_confidence_weighted  4545a3e73854234a  confidence_weighted      True   \n",
      "2  16_confidence_weighted  9cd674aa7b0d859c  confidence_weighted      True   \n",
      "\n",
      "   enough_length                                     summary_before  \\\n",
      "0           True  Partager Vous souhaitez Facebook Bluesky E-mai...   \n",
      "1           True  OMAR AL- Partager Vous souhaitez Facebook Blue...   \n",
      "2           True  En effet, nous r, bien qu'actuellement influen...   \n",
      "\n",
      "  tier_before  factuality_before  coherence_before  issues_before  ...  \\\n",
      "0    CRITICAL           0.891555          0.301675              6  ...   \n",
      "1    CRITICAL           0.844687          0.328654              6  ...   \n",
      "2    CRITICAL           0.888659          0.400962              6  ...   \n",
      "\n",
      "  prompt_version seed accepted  reason       l3_mode            mode_reason  \\\n",
      "0            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
      "1            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
      "2            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
      "\n",
      "  lang processing_status  runtime_ms notes  \n",
      "0   fr           unknown         NaN        \n",
      "1   fr           unknown         NaN        \n",
      "2   en           unknown         NaN        \n",
      "\n",
      "[3 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "print(final.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f08c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing columns: set()\n",
      "\n",
      "📊 ANALYSE DES RÉSULTATS:\n",
      "Total candidats: 81\n",
      "\n",
      "Statut de traitement:\n",
      "processing_status\n",
      "unknown     57\n",
      "rejected    24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pour les 24 cas traités:\n",
      "Accepted: 0 (0.0%)\n",
      "Erreur analyse: Expected numeric dtype, got object instead.\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv(EXPORT_DIR/\"level3_results.csv\")\n",
    "needed = {\"summary_before\",\"summary_after\",\"l3_mode\",\"strategy\",\"accepted\",\"reason\"}\n",
    "print(\"missing columns:\", needed - set(final.columns))\n",
    "\n",
    "# CORRECTION: Gérer les valeurs NaN dans accepted avant calcul\n",
    "print(\"\\n📊 ANALYSE DES RÉSULTATS:\")\n",
    "print(\"Total candidats:\", len(final))\n",
    "\n",
    "# Statistiques par processing_status\n",
    "if \"processing_status\" in final.columns:\n",
    "    print(\"\\nStatut de traitement:\")\n",
    "    print(final[\"processing_status\"].value_counts())\n",
    "\n",
    "# Statistiques pour les cas traités seulement\n",
    "processed = final[final[\"accepted\"].notna()].copy()\n",
    "if len(processed) > 0:\n",
    "    print(f\"\\nPour les {len(processed)} cas traités:\")\n",
    "    print(\"Accepted:\", processed[\"accepted\"].sum(), f\"({processed['accepted'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Analyse par stratégie et mode (safe)\n",
    "    try:\n",
    "        analysis = processed.groupby([\"strategy\",\"l3_mode\"])[\"accepted\"].agg(['count', 'sum']).reset_index()\n",
    "        analysis[\"acceptance_rate\"] = (analysis[\"sum\"] / analysis[\"count\"] * 100).round(1)\n",
    "        print(\"\\nTaux d'acceptation par stratégie/mode:\")\n",
    "        for _, row in analysis.iterrows():\n",
    "            print(f\"  {row['strategy']} + {row['l3_mode']}: {row['sum']}/{row['count']} ({row['acceptance_rate']}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur analyse: {e}\")\n",
    "else:\n",
    "    print(\"Aucun cas traité trouvé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6ff7b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_text",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "enough_length",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mode_reason",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bcf8a8da-0379-4628-97fb-d0aea566d1bf",
       "rows": [
        [
         "0",
         "7_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "1",
         "8_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "2",
         "16_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "3",
         "17_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "4",
         "22_confidence_weighted",
         "CRITICAL",
         "True",
         "False",
         "edit",
         "cw_critical_no_text"
        ],
        [
         "5",
         "29_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "6",
         "40_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "7",
         "53_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "8",
         "59_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "9",
         "71_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>tier</th>\n",
       "      <th>has_text</th>\n",
       "      <th>enough_length</th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>mode_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>edit</td>\n",
       "      <td>cw_critical_no_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               summary_id      tier  has_text  enough_length       l3_mode  \\\n",
       "0   7_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "1   8_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "2  16_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "3  17_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "4  22_confidence_weighted  CRITICAL      True          False          edit   \n",
       "5  29_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "6  40_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "7  53_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "8  59_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "9  71_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "\n",
       "             mode_reason  \n",
       "0  cw_critical_with_text  \n",
       "1  cw_critical_with_text  \n",
       "2  cw_critical_with_text  \n",
       "3  cw_critical_with_text  \n",
       "4    cw_critical_no_text  \n",
       "5  cw_critical_with_text  \n",
       "6  cw_critical_with_text  \n",
       "7  cw_critical_with_text  \n",
       "8  cw_critical_with_text  \n",
       "9  cw_critical_with_text  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")\n",
    "plan.query('strategy==\"confidence_weighted\"')[[\"summary_id\",\"tier\",\"has_text\",\"enough_length\",\"l3_mode\",\"mode_reason\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d1391e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL traités: 81\n",
      "CRITICAL acceptés: 0\n",
      "Empty DataFrame\n",
      "Columns: [summary_id, strategy, l3_mode, mode_reason, tier_before, tier_after, factuality_before, factuality_after, coherence_before, coherence_after, issues_before, issues_after, summary_before, summary_after]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "critical_done = final[(final[\"tier_before\"]==\"CRITICAL\")]\n",
    "print(\"CRITICAL traités:\", len(critical_done))\n",
    "\n",
    "crit_accepted = critical_done[critical_done[\"accepted\"]==True]\n",
    "print(\"CRITICAL acceptés:\", len(crit_accepted))\n",
    "\n",
    "# échantillon pour revue manuelle\n",
    "sample = crit_accepted.sample(min(5, len(crit_accepted)), random_state=42)[\n",
    "    [\"summary_id\",\"strategy\",\"l3_mode\",\"mode_reason\",\"tier_before\",\"tier_after\",\n",
    "     \"factuality_before\",\"factuality_after\",\"coherence_before\",\"coherence_after\",\n",
    "     \"issues_before\",\"issues_after\",\"summary_before\",\"summary_after\"]\n",
    "]\n",
    "sample.to_csv(REPORT_DIR / \"sample_manual_review.csv\", index=False)\n",
    "print(sample.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8ac83fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_id: nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_text",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "enough_length",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "summary_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "factuality_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_before",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_after",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hash_after",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier_after",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "factuality_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_version",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accepted",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mode_reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lang",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processing_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "runtime_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "notes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "dca9c32f-8eb5-4be6-8e0c-46e305b261e4",
       "rows": [],
       "shape": {
        "columns": 27,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>strategy</th>\n",
       "      <th>has_text</th>\n",
       "      <th>enough_length</th>\n",
       "      <th>summary_before</th>\n",
       "      <th>tier_before</th>\n",
       "      <th>factuality_before</th>\n",
       "      <th>coherence_before</th>\n",
       "      <th>issues_before</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>seed</th>\n",
       "      <th>accepted</th>\n",
       "      <th>reason</th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>mode_reason</th>\n",
       "      <th>lang</th>\n",
       "      <th>processing_status</th>\n",
       "      <th>runtime_ms</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [summary_id, source_id, strategy, has_text, enough_length, summary_before, tier_before, factuality_before, coherence_before, issues_before, summary_after, hash_after, tier_after, factuality_after, coherence_after, issues_after, model, prompt_version, seed, accepted, reason, l3_mode, mode_reason, lang, processing_status, runtime_ms, notes]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === AUDIT ACCEPTÉS ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "WORK_DIR     = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"level3\"\n",
    "EXPORT_DIR   = Path(PROJECT_ROOT) / \"outputs\" / \"level3\" / \"exports\"\n",
    "\n",
    "final = pd.read_csv(EXPORT_DIR / \"level3_results.csv\")\n",
    "ok    = final[final[\"accepted\"]==True].copy()\n",
    "\n",
    "base02 = pd.read_csv(WORK_DIR / \"02_join_articles.csv\") if (WORK_DIR / \"02_join_articles.csv\").exists() else pd.read_parquet(WORK_DIR / \"02_join_articles.parquet\")\n",
    "\n",
    "def peek(row):\n",
    "    print(\"summary_id:\", row[\"summary_id\"])\n",
    "    r = base02[base02[\"summary_id\"]==row[\"summary_id\"]].iloc[0]\n",
    "    print(\"  strategy  :\", row.get(\"strategy\"))\n",
    "    print(\"  title     :\", r.get(\"title\"))\n",
    "    print(\"  url       :\", r.get(\"url\"))\n",
    "    print(\"  lang(before) :\", row.get(\"lang\"))\n",
    "    print(\"  text[:220]:\", (r.get(\"text\") or \"\")[:220].replace(\"\\n\",\" \"))\n",
    "    print(\"  summary_after[:140]:\", (row.get(\"summary_after\") or \"\")[:140])\n",
    "    print(\"---\")\n",
    "\n",
    "ok.head(5).apply(peek, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
