{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9c2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcce9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825cacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24074f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, re, logging, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3c97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Logging ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"level3_notebook\")\n",
    "\n",
    "# --- Chemins (adapt√©s √† INSIGHT-DETECTOR)\n",
    "def find_project_root():\n",
    "    p = Path.cwd().resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \"src\").exists() and (parent / \"outputs\").exists():\n",
    "            return parent\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f6a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entr√©es brutes L1/L2/mappings (d√©j√† dans outputs/)\n",
    "RAW_OUT_DIR       = PROJECT_ROOT / \"outputs\"\n",
    "# Articles sources (d√©j√† dans data/exports/)\n",
    "RAW_DATA_EXPORTS  = PROJECT_ROOT / \"data\" / \"exports\"\n",
    "\n",
    "# Interm√©diaires/caches L3 (dans data/processed/level3)\n",
    "WORK_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"level3\"\n",
    "CACHE_DIR  = WORK_DIR / \"cache\"\n",
    "\n",
    "# Exports finaux L3\n",
    "OUT_L3_DIR  = PROJECT_ROOT / \"outputs\" / \"level3\"\n",
    "EXPORT_DIR  = OUT_L3_DIR / \"exports\"\n",
    "REPORT_DIR  = OUT_L3_DIR / \"reports\"\n",
    "LOG_DIR     = OUT_L3_DIR / \"logs\"\n",
    "\n",
    "for d in [WORK_DIR, CACHE_DIR, EXPORT_DIR, REPORT_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98361224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niveau 3 improvement import√© (version de base)\n"
     ]
    }
   ],
   "source": [
    "# Utils + config/prompts dans src/detection/level3 ENHANCED\n",
    "try:\n",
    "    # Essayer la version enhanced avec strat√©gies adaptatives\n",
    "    SRC_L3_DIR   = PROJECT_ROOT / \"src\" / \"detection\" / \"level3_adaptive\"\n",
    "    CONFIG_DIR   = SRC_L3_DIR / \"config\"\n",
    "    PROMPT_DIR   = SRC_L3_DIR / \"prompts\"\n",
    "    sys.path.append(str(SRC_L3_DIR))\n",
    "    \n",
    "    from level3_adaptive_utils import (\n",
    "        sha1_text, read_jsonl, write_jsonl, detect_lang, chunk_text_by_words,\n",
    "        generate_edit, generate_resummarize, postprocess_summary,\n",
    "        choose_mode, l2_like_evaluate, accept_after\n",
    "    )\n",
    "    print(\"NIVEAU 3 ENHANCED import√© (strat√©gies adaptatives)\")\n",
    "    \n",
    "except ImportError:\n",
    "    try:\n",
    "        # Fallback vers version improvement\n",
    "        SRC_L3_DIR   = PROJECT_ROOT / \"src\" / \"detection\" / \"level3_improvement\"\n",
    "        CONFIG_DIR   = SRC_L3_DIR / \"config\"\n",
    "        PROMPT_DIR   = SRC_L3_DIR / \"prompts\"\n",
    "        sys.path.append(str(SRC_L3_DIR))\n",
    "        \n",
    "        from level3_utils import (\n",
    "            sha1_text, read_jsonl, write_jsonl, detect_lang, chunk_text_by_words,\n",
    "            generate_edit, generate_resummarize, postprocess_summary,\n",
    "            choose_mode, l2_like_evaluate, accept_after\n",
    "        )\n",
    "        print(\"Niveau 3 improvement import√© (version de base)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"Impossible d'importer Level3 depuis 'src/detection/...'. \"\n",
    "            f\"V√©rifie que 'src' est bien au bon endroit. Erreur: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a84528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:Project root: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\n"
     ]
    }
   ],
   "source": [
    "# --- Fichiers bruts (emplacements existants)\n",
    "f_l2_res    = RAW_OUT_DIR / \"level2_simplified_results_with_ids.csv\"\n",
    "f_l2_prio   = RAW_OUT_DIR / \"level2_simplified_priority_cases_with_ids.csv\"\n",
    "f_l2_full   = RAW_OUT_DIR / \"level2_output_with_source_id.json\"\n",
    "f_map1      = RAW_OUT_DIR / \"mapping_level1id_to_source_id.csv\"\n",
    "f_map2      = RAW_OUT_DIR / \"mapping_backfill_level2.csv\"\n",
    "f_n1        = RAW_OUT_DIR / \"all_summaries_production.json\"\n",
    "f_articles  = RAW_DATA_EXPORTS / \"raw_articles.json\"\n",
    "f_cfg       = CONFIG_DIR / \"level3.yaml\"\n",
    "\n",
    "logger.info(\"Project root: %s\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507a330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Fallback Parquet <-> CSV (robuste) ----\n",
    "def _detect_parquet_engine():\n",
    "    try:\n",
    "        import pyarrow as pa  # noqa\n",
    "        # Gardes-fous: certaines install partielles n'ont pas __version__\n",
    "        if not hasattr(pa, \"__version__\"):\n",
    "            return None\n",
    "        return \"pyarrow\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet as fp  # noqa\n",
    "            return \"fastparquet\"\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "PARQUET_ENGINE = _detect_parquet_engine()\n",
    "\n",
    "from pathlib import Path\n",
    "def save_table(df: pd.DataFrame, stem_path: Path):\n",
    "    \"\"\"√âcrit .parquet si possible, ET toujours un .csv (fallback/idempotent).\"\"\"\n",
    "    stem_path = Path(stem_path)\n",
    "    if PARQUET_ENGINE:\n",
    "        df.to_parquet(stem_path.with_suffix(\".parquet\"), index=False, engine=PARQUET_ENGINE)\n",
    "    df.to_csv(stem_path.with_suffix(\".csv\"), index=False)\n",
    "\n",
    "def load_table(stem_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Charge .parquet si dispo & engine pr√©sent, sinon .csv.\"\"\"\n",
    "    stem_path = Path(stem_path)\n",
    "    p, c = stem_path.with_suffix(\".parquet\"), stem_path.with_suffix(\".csv\")\n",
    "    if p.exists() and PARQUET_ENGINE:\n",
    "        return pd.read_parquet(p, engine=PARQUET_ENGINE)\n",
    "    if c.exists():\n",
    "        return pd.read_csv(c)\n",
    "    raise FileNotFoundError(f\"Aucun fichier trouv√©: {p} ni {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d3c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'priority_threshold': 0.8,\n",
       " 'min_text_chars_for_resummarize': 500,\n",
       " 'edit_rule_adaptive': {'issues_max': 8,\n",
       "  'factuality_min': 0.75,\n",
       "  'coherence_rewrite_threshold': 0.15},\n",
       " 'acceptance': {'accepted_tiers': ['GOOD',\n",
       "   'EXCELLENT',\n",
       "   'MODERATE',\n",
       "   'IMPROVED_CRITICAL'],\n",
       "  'allow_moderate_guarded': True,\n",
       "  'allow_critical_with_improvement': True,\n",
       "  'moderate_guard': {'issues_max': 4,\n",
       "   'factuality_min': 0.8,\n",
       "   'coherence_min': 0.7},\n",
       "  'critical_guard': {'issues_max': 8,\n",
       "   'factuality_min': 0.7,\n",
       "   'coherence_min': 0.6,\n",
       "   'improvement_required': 0.05},\n",
       "  'require_monotonic_improvement': False,\n",
       "  'allow_stagnation_if_issues_reduced': True},\n",
       " 'acceptance_topic': {'after_text_min': 0.01, 'after_before_min': 0.01},\n",
       " 'gen_params': {'temperature': 0.0,\n",
       "  'top_p': 0.1,\n",
       "  'max_tokens': 220,\n",
       "  'stop': ['\\\\n\\\\n', '###'],\n",
       "  'seed': 42},\n",
       " 'mode': {'prefer_resum_for_cw_critical': True,\n",
       "  'tiers_order': ['CRITICAL', 'MODERATE', 'GOOD', 'EXCELLENT']}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import yaml\n",
    "    CFG = yaml.safe_load(open(f_cfg, \"r\", encoding=\"utf-8\"))\n",
    "except Exception as e:\n",
    "    logger.warning(\"YAML non dispo (%s) -> utilisation de d√©fauts.\", e)\n",
    "    CFG = {\n",
    "        \"priority_threshold\": 0.85,\n",
    "        \"min_text_chars_for_resummarize\": 800,\n",
    "        \"edit_rule_adaptive\": {\"issues_max\": 5, \"factuality_min\": 0.88, \"coherence_rewrite_threshold\": 0.20},\n",
    "        \"acceptance\": {\n",
    "            \"accepted_tiers\": [\"GOOD\",\"EXCELLENT\"],\n",
    "            \"allow_moderate_guarded\": True,\n",
    "            \"moderate_guard\": {\"issues_max\": 2, \"factuality_min\": 0.90, \"coherence_min\": 0.80},\n",
    "            \"require_monotonic_improvement\": True\n",
    "        },\n",
    "        \"gen_params\": {\"temperature\":0.0, \"top_p\":0.1, \"max_tokens\":220, \"stop\":[\"\\n\\n\",\"###\"], \"seed\":42},\n",
    "        \"mode\": {\"prefer_resum_for_cw_critical\": True, \"tiers_order\":[\"CRITICAL\",\"MODERATE\",\"GOOD\",\"EXCELLENT\"]}\n",
    "    }\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987a44a",
   "metadata": {},
   "source": [
    "# S√©lection des candidats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db29eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:00 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\00_candidates.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\00_candidates.csv (81 lignes)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f_l2_res)\n",
    "prio_thresh = CFG[\"priority_threshold\"]\n",
    "\n",
    "candidates = df[(df[\"tier\"]==\"CRITICAL\") | (df[\"level3_priority_final\"]>=prio_thresh)].copy()\n",
    "tier_order = {t:i for i,t in enumerate(CFG[\"mode\"][\"tiers_order\"])}\n",
    "candidates[\"tier_rank\"] = candidates[\"tier\"].map(tier_order)\n",
    "candidates = candidates.sort_values([\"level3_priority_final\",\"issues_count\",\"tier_rank\"], ascending=[False, False, True])\n",
    "\n",
    "stem_00 = WORK_DIR / \"00_candidates\"\n",
    "save_table(candidates, stem_00)\n",
    "logger.info(\"00 -> %s | %s (%d lignes)\", stem_00.with_suffix(\".parquet\"), stem_00.with_suffix(\".csv\"), len(candidates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5482a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 n = 81 uniques summary_id = 81\n",
      "tier\n",
      "CRITICAL    81\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "c0 = load_table(WORK_DIR/\"00_candidates\")\n",
    "print(\"00 n =\", len(c0), \"uniques summary_id =\", c0[\"summary_id\"].nunique())\n",
    "print(c0[\"tier\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09490821",
   "metadata": {},
   "source": [
    "# Backfill source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c445c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:01 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\01_backfilled.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\01_backfilled.csv (manquants=0)\n"
     ]
    }
   ],
   "source": [
    "cand = load_table(WORK_DIR / \"00_candidates\")\n",
    "map1 = pd.read_csv(f_map1)  # level1_id -> source_id\n",
    "map2 = pd.read_csv(f_map2)  # summary_id -> source_id\n",
    "\n",
    "cand = cand.merge(map1.rename(columns={\"level1_id\":\"summary_id\",\"source_id\":\"source_id_m1\"}), on=\"summary_id\", how=\"left\")\n",
    "cand = cand.merge(map2.rename(columns={\"summary_id\":\"summary_id\",\"source_id\":\"source_id_m2\"}), on=\"summary_id\", how=\"left\")\n",
    "cand[\"source_id_filled\"] = cand[\"source_id\"].fillna(cand[\"source_id_m1\"]).fillna(cand[\"source_id_m2\"])\n",
    "\n",
    "missing = cand[cand[\"source_id_filled\"].isna()][[\"summary_id\"]]\n",
    "missing.to_csv(REPORT_DIR / \"01_missing_source_id.csv\", index=False)\n",
    "\n",
    "stem_01 = WORK_DIR / \"01_backfilled\"\n",
    "save_table(cand, stem_01)\n",
    "logger.info(\"01 -> %s | %s (manquants=%d)\", stem_01.with_suffix(\".parquet\"), stem_01.with_suffix(\".csv\"), len(missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455b18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 coverage source_id_filled = 100.0%\n"
     ]
    }
   ],
   "source": [
    "c1 = load_table(WORK_DIR/\"01_backfilled\")\n",
    "cov = c1[\"source_id_filled\"].notna().mean()\n",
    "print(f\"01 coverage source_id_filled = {cov:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc328a0",
   "metadata": {},
   "source": [
    "# Join articles + flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1e59180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CORRECTION: Utilisation du mapping unifi√© existant\n",
      "‚úÖ Mapping r√©ussi via unified_mapping_complete.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:02 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\02_join_articles.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\02_join_articles.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä R√©sultats du mapping:\n",
      "   Entr√©es totales: 81\n",
      "   Avec texte: 81 (100.0%)\n",
      "   Texte suffisant: 68 (84.0%)\n"
     ]
    }
   ],
   "source": [
    "cand = load_table(WORK_DIR / \"01_backfilled\")\n",
    "\n",
    "# CORRECTION: Utiliser le mapping unifi√© cr√©√© par le notebook 04_mapping_unified\n",
    "# Au lieu de tenter un match SHA1 direct, utiliser le mapping d√©j√† √©tabli\n",
    "mapping_file = RAW_OUT_DIR / \"unified_mapping_complete.csv\"\n",
    "if mapping_file.exists():\n",
    "    print(\"üîß CORRECTION: Utilisation du mapping unifi√© existant\")\n",
    "    mapping_unified = pd.read_csv(mapping_file)\n",
    "    \n",
    "    # Cr√©er un mapping source_id ‚Üí m√©tadonn√©es article\n",
    "    source_to_article = mapping_unified[[\"level2_id\", \"source_id\", \"title\", \"url\", \"article_id\"]].drop_duplicates()\n",
    "    \n",
    "    # Charger les textes des articles\n",
    "    arts = pd.read_json(f_articles)\n",
    "    arts_text = arts[[\"id\", \"text\"]].rename(columns={\"id\": \"article_id\"})\n",
    "    \n",
    "    # Joindre: candidats ‚Üí mapping ‚Üí textes\n",
    "    cand = cand.merge(\n",
    "        source_to_article.rename(columns={\"level2_id\": \"summary_id\"}), \n",
    "        on=\"summary_id\", \n",
    "        how=\"left\"\n",
    "    ).merge(\n",
    "        arts_text,\n",
    "        on=\"article_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Mapping r√©ussi via unified_mapping_complete.csv\")\n",
    "    \n",
    "else:\n",
    "    # Fallback vers l'ancienne m√©thode (probl√©matique)\n",
    "    print(\"‚ö†Ô∏è FALLBACK: Tentative mapping direct SHA1 (peut √©chouer)\")\n",
    "    arts = pd.read_json(f_articles)\n",
    "    arts[\"sha1_url\"] = arts[\"url\"].astype(str).apply(lambda u: hashlib.sha1(u.encode(\"utf-8\")).hexdigest())\n",
    "    cand = cand.merge(arts[[\"id\",\"title\",\"url\",\"text\",\"sha1_url\"]], left_on=\"source_id_filled\", right_on=\"sha1_url\", how=\"left\")\n",
    "\n",
    "# Flags de disponibilit√© des textes\n",
    "cand[\"has_text\"] = cand[\"text\"].notna() & (cand[\"text\"].astype(str).str.len() > 0)\n",
    "cand[\"enough_length\"] = cand[\"text\"].apply(lambda t: isinstance(t,str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"])\n",
    "\n",
    "# D√©tection langue depuis 'summary' si dispo, sinon fallback sur 'title'\n",
    "if \"summary\" in cand.columns:\n",
    "    cand[\"lang\"] = cand[\"summary\"].fillna(\"\").apply(detect_lang)\n",
    "else:\n",
    "    cand[\"lang\"] = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "\n",
    "stem_02 = WORK_DIR / \"02_join_articles\"\n",
    "save_table(cand, stem_02)\n",
    "logger.info(\"02 -> %s | %s\", stem_02.with_suffix(\".parquet\"), stem_02.with_suffix(\".csv\"))\n",
    "\n",
    "print(f\"üìä R√©sultats du mapping:\")\n",
    "print(f\"   Entr√©es totales: {len(cand)}\")\n",
    "print(f\"   Avec texte: {cand['has_text'].sum()} ({cand['has_text'].mean()*100:.1f}%)\")\n",
    "print(f\"   Texte suffisant: {cand['enough_length'].sum()} ({cand['enough_length'].mean()*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c13fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_text\n",
      "True    81\n",
      "Name: count, dtype: int64\n",
      "enough_length\n",
      "True     68\n",
      "False    13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "c2 = load_table(WORK_DIR/\"02_join_articles\")\n",
    "print(c2[\"has_text\"].value_counts(dropna=False))\n",
    "print(c2[\"enough_length\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1556de",
   "metadata": {},
   "source": [
    "# Join N1 (r√©sum√© initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e86cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions utilitaires Level3 d√©finies\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FONCTIONS UTILITAIRES LEVEL3 \n",
    "# =========================\n",
    "\n",
    "import re, json, logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from level3_utils import detect_lang, _as_text\n",
    "\n",
    "# Logger fallback\n",
    "logger = logging.getLogger(\"level3_notebook\") if \"logger\" in globals() else logging.getLogger(\"level3_patch\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def _tokset(s: str):\n",
    "    \"\"\"Extrait les tokens significatifs d'un texte pour calcul Jaccard\"\"\"\n",
    "    s = (s or \"\").lower()\n",
    "    toks = re.findall(r\"[a-z√Ä-√ñ√ò-√∂√∏-√ø0-9]+\", s)\n",
    "    stops = {\n",
    "        \"le\",\"la\",\"les\",\"des\",\"du\",\"de\",\"un\",\"une\",\"et\",\"pour\",\"avec\",\"dans\",\"sur\",\"par\",\"au\",\"aux\",\"est\",\"sont\",\n",
    "        \"the\",\"of\",\"and\",\"to\",\"in\",\"for\",\"on\",\"with\",\"by\",\"is\",\"are\",\"as\",\"at\",\"from\"\n",
    "    }\n",
    "    return {t for t in toks if t not in stops and len(t) > 2}\n",
    "\n",
    "def _jaccard(a, b):\n",
    "    \"\"\"Calcule la similarit√© Jaccard entre deux textes\"\"\"\n",
    "    A, B = _tokset(a), _tokset(b)\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "def _infer_strategy_from_summary_id(sid: str) -> str:\n",
    "    \"\"\"Inf√®re la strat√©gie depuis le summary_id si manquante\"\"\"\n",
    "    s = str(sid or \"\")\n",
    "    for st in (\"adaptive\", \"confidence_weighted\"):\n",
    "        if s.endswith(\"_\" + st) or (\"_\" + st) in s:\n",
    "            return st\n",
    "    return s.split(\"_\")[-1] if \"_\" in s else \"\"\n",
    "\n",
    "def ensure_required_columns(cand: pd.DataFrame, CFG: dict) -> pd.DataFrame:\n",
    "    \"\"\"Garantit la pr√©sence des colonnes n√©cessaires avec fallbacks robustes\"\"\"\n",
    "    cand = cand.copy()\n",
    "    \n",
    "    # Tier\n",
    "    if \"tier\" not in cand.columns and \"tier_before\" in cand.columns:\n",
    "        cand[\"tier\"] = cand[\"tier_before\"]\n",
    "    \n",
    "    # Strategy (fallback depuis summary_id)\n",
    "    if \"strategy\" not in cand.columns:\n",
    "        cand[\"strategy\"] = np.nan\n",
    "    missing_mask = cand[\"strategy\"].isna()\n",
    "    if missing_mask.any():\n",
    "        cand.loc[missing_mask, \"strategy\"] = cand.loc[missing_mask, \"summary_id\"].apply(_infer_strategy_from_summary_id)\n",
    "    \n",
    "    # Flags texte\n",
    "    text_col_exists = \"text\" in cand.columns\n",
    "    \n",
    "    if \"has_text\" not in cand.columns:\n",
    "        cand[\"has_text\"] = cand[\"text\"].notna() if text_col_exists else False\n",
    "    else:\n",
    "        cand[\"has_text\"] = cand[\"has_text\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "    \n",
    "    if \"enough_length\" not in cand.columns:\n",
    "        if text_col_exists:\n",
    "            cand[\"enough_length\"] = cand[\"text\"].apply(lambda t: isinstance(t, str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"])\n",
    "        else:\n",
    "            cand[\"enough_length\"] = False\n",
    "    else:\n",
    "        cand[\"enough_length\"] = cand[\"enough_length\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "    \n",
    "    # Langue\n",
    "    if \"lang\" not in cand.columns or cand[\"lang\"].isna().any():\n",
    "        if \"summary_before\" in cand.columns:\n",
    "            lang_series = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "        elif \"title\" in cand.columns:\n",
    "            lang_series = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "        else:\n",
    "            lang_series = pd.Series([\"fr\"] * len(cand), index=cand.index)\n",
    "        \n",
    "        if \"lang\" not in cand.columns:\n",
    "            cand[\"lang\"] = lang_series\n",
    "        else:\n",
    "            cand[\"lang\"] = cand[\"lang\"].fillna(lang_series)\n",
    "    \n",
    "    return cand\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires Level3 d√©finies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "htfyyx4btfg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Step 03: Join N1...\n",
      "   Loaded 81 candidates with articles\n",
      "   Parsed 372 N1 summaries\n",
      "‚úÖ Step 03 completed: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1.csv\n",
      "   Total with N1: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beedi.goua_square-ma\\AppData\\Local\\Temp\\ipykernel_28584\\1043569910.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'confidence_weighted'\n",
      " 'confidence_weighted' 'confidence_weighted' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive' 'adaptive'\n",
      " 'adaptive' 'adaptive']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  cand.loc[missing_mask, \"strategy\"] = cand.loc[missing_mask, \"summary_id\"].apply(_infer_strategy_from_summary_id)\n"
     ]
    }
   ],
   "source": [
    "# Step 03: Join N1 (r√©sum√© initial)\n",
    "print(\"üîÑ Step 03: Join N1...\")\n",
    "\n",
    "# Chargement des donn√©es depuis Step 02 (join articles)\n",
    "cand = load_table(WORK_DIR / \"02_join_articles\")\n",
    "print(f\"   Loaded {len(cand)} candidates with articles\")\n",
    "\n",
    "# Chargement et parsing du N1 (r√©sum√©s initiaux)\n",
    "import json\n",
    "n1_raw = json.load(open(f_n1, \"r\", encoding=\"utf-8\"))\n",
    "rows = []\n",
    "\n",
    "# Parse structure N1 (dict ou list)\n",
    "if isinstance(n1_raw, dict):\n",
    "    it = n1_raw.items()\n",
    "elif isinstance(n1_raw, list):\n",
    "    it = [(None, v) for v in n1_raw]\n",
    "else:\n",
    "    it = []\n",
    "\n",
    "for k, v in it:\n",
    "    if isinstance(v, dict) and \"strategies\" in v:\n",
    "        article_id = v.get(\"article_id\", k)\n",
    "        for strat_name, strat_data in v[\"strategies\"].items():\n",
    "            if isinstance(strat_data, dict):\n",
    "                summary_id = f\"{article_id}_{strat_name}\"\n",
    "                rows.append({\n",
    "                    \"summary_id\": summary_id,\n",
    "                    \"summary\": strat_data.get(\"summary\", \"\"),\n",
    "                    \"summary_before\": strat_data.get(\"summary_before\", \"\")\n",
    "                })\n",
    "\n",
    "n1_summaries = pd.DataFrame(rows)\n",
    "print(f\"   Parsed {len(n1_summaries)} N1 summaries\")\n",
    "\n",
    "# Join avec N1 pour r√©cup√©rer les r√©sum√©s initiaux\n",
    "cand = cand.merge(n1_summaries, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# Validation avec la fonction correcte\n",
    "ensure_required_columns(cand, CFG)\n",
    "\n",
    "# Sauvegarde Step 03\n",
    "stem_03 = WORK_DIR / \"03_with_n1\"\n",
    "save_table(cand, stem_03)\n",
    "print(f\"‚úÖ Step 03 completed: {stem_03.with_suffix('.csv')}\")\n",
    "print(f\"   Total with N1: {len(cand)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c0km28ivu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Step 03b: Alignment...\n",
      "   Loaded 81 candidates with N1\n",
      "   Computing topic overlap...\n",
      "   Topic overlap: min=0.000, max=0.000, mean=0.000\n",
      "‚úÖ Step 03b completed: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1_aligned.csv\n",
      "   Total aligned: 81\n"
     ]
    }
   ],
   "source": [
    "# Step 03b: Alignment (langue + topic overlap)\n",
    "print(\"üîÑ Step 03b: Alignment...\")\n",
    "\n",
    "# Chargement des donn√©es depuis Step 03\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\")\n",
    "print(f\"   Loaded {len(cand)} candidates with N1\")\n",
    "\n",
    "# D√©tection/correction de la langue\n",
    "if \"summary_before\" in cand.columns:\n",
    "    lang_series = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "elif \"title\" in cand.columns:\n",
    "    lang_series = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "else:\n",
    "    lang_series = pd.Series([\"fr\"] * len(cand), index=cand.index)\n",
    "\n",
    "if \"lang\" not in cand.columns:\n",
    "    cand[\"lang\"] = lang_series\n",
    "else:\n",
    "    cand[\"lang\"] = cand[\"lang\"].fillna(lang_series)\n",
    "\n",
    "# Calcul de l'overlap topique (optimis√©)\n",
    "print(\"   Computing topic overlap...\")\n",
    "overlaps = []\n",
    "for idx, row in cand.iterrows():\n",
    "    try:\n",
    "        summary_before = _as_text(row.get(\"summary_before\", \"\"))\n",
    "        text = _as_text(row.get(\"text\", \"\"))\n",
    "        \n",
    "        # √âviter les calculs sur textes vides\n",
    "        if not summary_before or not text:\n",
    "            overlap = 0.0\n",
    "        else:\n",
    "            overlap = _jaccard(_tokset(summary_before), _tokset(text))\n",
    "        \n",
    "        overlaps.append(overlap)\n",
    "    except Exception as e:\n",
    "        # Fallback en cas d'erreur\n",
    "        print(f\"   Warning: overlap calculation failed for row {idx}: {e}\")\n",
    "        overlaps.append(0.0)\n",
    "\n",
    "cand[\"topic_overlap\"] = overlaps\n",
    "\n",
    "# Statistiques robustes\n",
    "valid_overlaps = [o for o in overlaps if not pd.isna(o)]\n",
    "if valid_overlaps:\n",
    "    print(f\"   Topic overlap: min={min(valid_overlaps):.3f}, max={max(valid_overlaps):.3f}, mean={np.mean(valid_overlaps):.3f}\")\n",
    "else:\n",
    "    print(\"   Topic overlap: no valid overlaps computed\")\n",
    "\n",
    "# Sauvegarde Step 03b  \n",
    "stem_03b = WORK_DIR / \"03_with_n1_aligned\"\n",
    "save_table(cand, stem_03b)\n",
    "print(f\"‚úÖ Step 03b completed: {stem_03b.with_suffix('.csv')}\")\n",
    "print(f\"   Total aligned: {len(cand)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fmascx3z5bi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Step 04: Mode Decision...\n",
      "   Loaded 81 candidates from step 03b\n",
      "   Applying choose_mode logic...\n",
      "   Generated 81 mode decisions\n",
      "   Mode distribution:\n",
      "     edit: 81 (100.0%)\n",
      "‚úÖ Step 04 completed: C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\04_mode_plan.csv\n",
      "   Total candidates: 81\n"
     ]
    }
   ],
   "source": [
    "# Step 04: Mode Decision\n",
    "print(\"üéØ Step 04: Mode Decision...\")\n",
    "\n",
    "# Chargement des donn√©es depuis Step 03b (aligned)\n",
    "data_path = WORK_DIR / \"03_with_n1_aligned.csv\"\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Donn√©es Step 03b manquantes: {data_path}\")\n",
    "\n",
    "cand = pd.read_csv(data_path)\n",
    "print(f\"   Loaded {len(cand)} candidates from step 03b\")\n",
    "\n",
    "# Application de choose_mode pour chaque candidat\n",
    "print(\"   Applying choose_mode logic...\")\n",
    "decisions = []\n",
    "for idx, r in cand.iterrows():\n",
    "    mode, reason, flags = choose_mode(r, CFG)\n",
    "    decisions.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"l3_mode\": mode,\n",
    "        \"mode_reason\": reason,\n",
    "        # Flags pour garantir la pr√©sence dans le CSV final\n",
    "        \"has_text\": bool(flags.get(\"has_text\", r.get(\"has_text\", False))),\n",
    "        \"enough_length\": bool(flags.get(\"enough_length\", r.get(\"enough_length\", False))),\n",
    "        \"lang\": flags.get(\"lang\", r.get(\"lang\", \"fr\"))\n",
    "    })\n",
    "\n",
    "plan = pd.DataFrame(decisions)\n",
    "print(f\"   Generated {len(plan)} mode decisions\")\n",
    "\n",
    "# Protection contre les colonnes dupliqu√©es au merge\n",
    "cols_to_drop = {\"l3_mode\", \"mode_reason\", \"has_text\", \"enough_length\", \"lang\"} & set(cand.columns)\n",
    "cand_clean = cand.drop(columns=list(cols_to_drop), errors=\"ignore\")\n",
    "\n",
    "# Merge des d√©cisions avec les donn√©es candidates\n",
    "level3_plan = cand_clean.merge(plan, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# Validation et statistiques\n",
    "print(f\"   Mode distribution:\")\n",
    "for mode, count in level3_plan[\"l3_mode\"].value_counts().items():\n",
    "    print(f\"     {mode}: {count} ({count/len(level3_plan)*100:.1f}%)\")\n",
    "\n",
    "# Sauvegarde du plan de mode\n",
    "stem_04 = WORK_DIR / \"04_mode_plan\"\n",
    "save_table(level3_plan, stem_04)\n",
    "print(f\"‚úÖ Step 04 completed: {stem_04.with_suffix('.csv')}\")\n",
    "print(f\"   Total candidates: {len(level3_plan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lt6lw3hszom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:03 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1.csv\n",
      "INFO:level3_notebook:03 summary_before coverage = 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù √âtape 03: Chargement et join des r√©sum√©s N1...\n",
      "   N1 parsed: 370 entr√©es\n",
      "‚úÖ √âtape 03 termin√©e: 81 candidats avec N1, coverage summary_before: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# √âTAPE 03 ‚Äî Join N1 (r√©sum√©s initiaux)\n",
    "# =========================\n",
    "\n",
    "if \"WORK_DIR\" not in globals() or \"f_n1\" not in globals():\n",
    "    raise RuntimeError(\"WORK_DIR et f_n1 doivent √™tre d√©finis plus haut dans le notebook.\")\n",
    "\n",
    "print(\"üìù √âtape 03: Chargement et join des r√©sum√©s N1...\")\n",
    "\n",
    "# Chargement N1\n",
    "n1_raw = json.load(open(f_n1, \"r\", encoding=\"utf-8\"))\n",
    "rows = []\n",
    "\n",
    "# Parse structure N1 (dict ou list)\n",
    "if isinstance(n1_raw, dict):\n",
    "    it = n1_raw.items()\n",
    "elif isinstance(n1_raw, list):\n",
    "    it = [(None, v) for v in n1_raw]\n",
    "else:\n",
    "    it = []\n",
    "\n",
    "# Extraction des donn√©es N1\n",
    "for _, v in it:\n",
    "    v = v or {}\n",
    "    aid = v.get(\"article_id\") or v.get(\"id\")\n",
    "    strategies = v.get(\"strategies\") or {}\n",
    "    if not aid or not isinstance(strategies, dict):\n",
    "        continue\n",
    "    for strat, sv in strategies.items():\n",
    "        sv = sv or {}\n",
    "        metrics = sv.get(\"metrics\") or {}\n",
    "        rows.append({\n",
    "            \"article_id\": aid,\n",
    "            \"strategy\": strat,\n",
    "            \"summary_id\": f\"{aid}_{strat}\",\n",
    "            \"summary_before\": sv.get(\"summary\", \"\"),\n",
    "            \"n1_coherence\": metrics.get(\"coherence\"),\n",
    "            \"n1_factuality\": metrics.get(\"factuality\"),\n",
    "        })\n",
    "\n",
    "df_n1 = pd.DataFrame(rows)\n",
    "print(f\"   N1 parsed: {len(df_n1)} entr√©es\")\n",
    "\n",
    "# Join avec les candidats Level2\n",
    "c02 = load_table(WORK_DIR / \"02_join_articles\")\n",
    "cand = c02.merge(df_n1, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# Garantir strat√©gie (fallback depuis summary_id)\n",
    "cand = ensure_required_columns(cand, CFG)\n",
    "\n",
    "# Sauvegarde\n",
    "stem_03 = WORK_DIR / \"03_with_n1\"\n",
    "save_table(cand, stem_03)\n",
    "\n",
    "# Validation\n",
    "c3 = load_table(stem_03)\n",
    "coverage = c3[\"summary_before\"].notna().mean() if \"summary_before\" in c3.columns else 0.0\n",
    "\n",
    "logger.info(\"03 -> %s | %s\", stem_03.with_suffix(\".parquet\"), stem_03.with_suffix(\".csv\"))\n",
    "logger.info(\"03 summary_before coverage = %.1f%%\", coverage*100.0)\n",
    "print(f\"‚úÖ √âtape 03 termin√©e: {len(cand)} candidats avec N1, coverage summary_before: {coverage*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lfj4ejbkgm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê √âtape 03b: Calcul alignement langue et overlap th√©matique...\n",
      "   Calcul topic overlap (peut prendre quelques secondes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:03b -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1_aligned.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\03_with_n1_aligned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ √âtape 03b termin√©e:\n",
      "   Lang mismatch: 18.5%\n",
      "   Topic overlap median: 0.0061\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# √âTAPE 03b ‚Äî Alignement langue & topic overlap\n",
    "# =========================\n",
    "\n",
    "print(\"üåê √âtape 03b: Calcul alignement langue et overlap th√©matique...\")\n",
    "\n",
    "# Charger les donn√©es de l'√©tape pr√©c√©dente\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\").copy()\n",
    "\n",
    "# Normalisation et d√©tection langue du texte source\n",
    "if \"text\" not in cand.columns:\n",
    "    cand[\"text\"] = \"\"\n",
    "cand[\"text\"] = cand[\"text\"].apply(_as_text)\n",
    "cand[\"text_lang\"] = cand[\"text\"].apply(detect_lang)\n",
    "\n",
    "# Langue du r√©sum√© (d√©tection depuis summary_before si n√©cessaire)\n",
    "if \"lang\" not in cand.columns:\n",
    "    if \"summary_before\" in cand.columns:\n",
    "        cand[\"lang\"] = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "    elif \"title\" in cand.columns:\n",
    "        cand[\"lang\"] = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "    else:\n",
    "        cand[\"lang\"] = \"fr\"\n",
    "else:\n",
    "    cand[\"lang\"] = cand[\"lang\"].fillna(\"fr\").astype(str)\n",
    "\n",
    "# D√©tection mismatch de langue\n",
    "cand[\"lang_mismatch\"] = cand[\"lang\"].fillna(\"fr\") != cand[\"text_lang\"].fillna(\"fr\")\n",
    "\n",
    "# Garantir summary_before (fallback sur 'summary' si besoin)\n",
    "if \"summary_before\" not in cand.columns:\n",
    "    cand[\"summary_before\"] = cand[\"summary\"].fillna(\"\") if \"summary\" in cand.columns else \"\"\n",
    "\n",
    "# Calcul similarit√© th√©matique r√©sum√©_before vs texte source\n",
    "print(\"   Calcul topic overlap (peut prendre quelques secondes)...\")\n",
    "cand[\"topic_overlap_before_text\"] = cand.apply(\n",
    "    lambda r: _jaccard(_as_text(r.get(\"summary_before\", \"\")), _as_text(r.get(\"text\", \"\"))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sauvegarde\n",
    "stem_03b = WORK_DIR / \"03_with_n1_aligned\"\n",
    "save_table(cand, stem_03b)\n",
    "\n",
    "# Statistiques de validation\n",
    "c3b = load_table(stem_03b)\n",
    "lang_mismatch_rate = float(c3b[\"lang_mismatch\"].mean()) if \"lang_mismatch\" in c3b.columns else 0.0\n",
    "overlap_median = float(c3b[\"topic_overlap_before_text\"].median()) if \"topic_overlap_before_text\" in c3b.columns else 0.0\n",
    "\n",
    "logger.info(\"03b -> %s | %s\", stem_03b.with_suffix(\".parquet\"), stem_03b.with_suffix(\".csv\"))\n",
    "print(f\"‚úÖ √âtape 03b termin√©e:\")\n",
    "print(f\"   Lang mismatch: {lang_mismatch_rate*100:.1f}%\")\n",
    "print(f\"   Topic overlap median: {overlap_median:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5342ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:04 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\04_mode_plan.csv (colonnes garanties: strategy/tier/has_text/enough_length/lang/l3_mode/mode_reason)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# √âtape 04 ‚Äî D√©cision de mode (EDIT / RE-SUM) robuste\n",
    "# ==============================================\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\").copy()\n",
    "\n",
    "# (1) Garantir la pr√©sence de colonnes cl√©s\n",
    "\n",
    "# tier : si absent mais tier_before pr√©sent, recopier\n",
    "if \"tier\" not in cand.columns and \"tier_before\" in cand.columns:\n",
    "    cand[\"tier\"] = cand[\"tier_before\"]\n",
    "\n",
    "# strategy : fallback depuis summary_id si absent\n",
    "if \"strategy\" not in cand.columns or cand[\"strategy\"].isna().any():\n",
    "    if \"strategy\" not in cand.columns:\n",
    "        cand[\"strategy\"] = np.nan\n",
    "    miss = cand[\"strategy\"].isna()\n",
    "    cand.loc[miss, \"strategy\"] = cand.loc[miss, \"summary_id\"].apply(_infer_strategy_from_summary_id)\n",
    "\n",
    "# has_text / enough_length / lang\n",
    "text_col_exists = \"text\" in cand.columns\n",
    "\n",
    "if \"has_text\" not in cand.columns:\n",
    "    cand[\"has_text\"] = cand[\"text\"].notna() if text_col_exists else False\n",
    "else:\n",
    "    # caster proprement en bool (au cas o√π CSV ‚Üí strings)\n",
    "    cand[\"has_text\"] = cand[\"has_text\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "\n",
    "if \"enough_length\" not in cand.columns:\n",
    "    if text_col_exists:\n",
    "        cand[\"enough_length\"] = cand[\"text\"].apply(lambda t: isinstance(t, str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"])\n",
    "    else:\n",
    "        cand[\"enough_length\"] = False\n",
    "else:\n",
    "    cand[\"enough_length\"] = cand[\"enough_length\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "\n",
    "if \"lang\" not in cand.columns or cand[\"lang\"].isna().any():\n",
    "    # priorit√© au r√©sum√© initial ; sinon fallback sur le titre ; sinon 'fr'\n",
    "    lang_series = None\n",
    "    if \"summary_before\" in cand.columns:\n",
    "        lang_series = cand[\"summary_before\"].fillna(\"\").apply(detect_lang)\n",
    "    elif \"title\" in cand.columns:\n",
    "        lang_series = cand[\"title\"].fillna(\"\").apply(detect_lang)\n",
    "    else:\n",
    "        lang_series = pd.Series([\"fr\"] * len(cand), index=cand.index)\n",
    "    if \"lang\" not in cand.columns:\n",
    "        cand[\"lang\"] = lang_series\n",
    "    else:\n",
    "        cand[\"lang\"] = cand[\"lang\"].fillna(lang_series)\n",
    "\n",
    "# (2) Calcul du mode via choose_mode\n",
    "decisions = []\n",
    "for _, r in cand.iterrows():\n",
    "    mode, reason, flags = choose_mode(r, CFG)\n",
    "    decisions.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"l3_mode\": mode,\n",
    "        \"mode_reason\": reason,\n",
    "        # recopier les flags pour garantir la pr√©sence dans le CSV final\n",
    "        \"has_text\": bool(flags.get(\"has_text\", r.get(\"has_text\", False))),\n",
    "        \"enough_length\": bool(flags.get(\"enough_length\", r.get(\"enough_length\", False))),\n",
    "        \"lang\": flags.get(\"lang\", r.get(\"lang\", \"fr\"))\n",
    "    })\n",
    "plan = pd.DataFrame(decisions)\n",
    "\n",
    "# (3) Prot√©ger contre les colonnes dupliqu√©es au merge\n",
    "cols_to_drop = {\"l3_mode\",\"mode_reason\",\"has_text\",\"enough_length\",\"lang\"} & set(cand.columns)\n",
    "cand_clean = cand.drop(columns=list(cols_to_drop), errors=\"ignore\")\n",
    "\n",
    "cand_final = cand_clean.merge(plan, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# (4) Types s√ªrs\n",
    "cand_final[\"has_text\"] = cand_final[\"has_text\"].fillna(False).astype(bool)\n",
    "cand_final[\"enough_length\"] = cand_final[\"enough_length\"].fillna(False).astype(bool)\n",
    "cand_final[\"lang\"] = cand_final[\"lang\"].fillna(\"fr\").astype(str)\n",
    "\n",
    "# (5) Sauvegarde\n",
    "path_04 = WORK_DIR / \"04_mode_plan.csv\"\n",
    "cand_final.to_csv(path_04, index=False)\n",
    "logger.info(\"04 -> %s (colonnes garanties: strategy/tier/has_text/enough_length/lang/l3_mode/mode_reason)\", path_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b95f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l3_mode\n",
      "re_summarize    53\n",
      "edit            28\n",
      "Name: count, dtype: int64\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "plan = pd.read_csv(WORK_DIR/\"04_mode_plan.csv\")\n",
    "print(plan[\"l3_mode\"].value_counts())\n",
    "print(set([\"has_text\",\"enough_length\",\"lang\"]) - set(plan.columns))  # doit √™tre vide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa85123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CORRECTION: Assouplissement des garde-fous pr√©-flight\n",
      "05 -> OK=24 | BLOCKED=30\n",
      "   Seuil overlap abaiss√©: 0.07 ‚Üí 0.01\n",
      "   Lang_mismatch autoris√© pour EDIT\n"
     ]
    }
   ],
   "source": [
    "# ===== √âtape 05 ‚Äî Pr√©-flight (avec flags d'alignement), version CORRIG√âE =====\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "plan = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")\n",
    "cand03b_path_csv = WORK_DIR / \"03_with_n1_aligned.csv\"\n",
    "cand03b_path_par = WORK_DIR / \"03_with_n1_aligned.parquet\"\n",
    "cand03b = pd.read_csv(cand03b_path_csv) if cand03b_path_csv.exists() else pd.read_parquet(cand03b_path_par)\n",
    "\n",
    "# 1) Merge des flags d'alignement (avec suffixes contr√¥l√©s)\n",
    "plan = plan.merge(\n",
    "    cand03b[[\"summary_id\", \"lang_mismatch\", \"topic_overlap_before_text\"]],\n",
    "    on=\"summary_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_cand\")\n",
    ")\n",
    "\n",
    "# 2) Coalesce si des colonnes existent d√©j√† (cas de re-run)\n",
    "for col in [\"lang_mismatch\", \"topic_overlap_before_text\"]:\n",
    "    cand_col = f\"{col}_cand\"\n",
    "    if cand_col in plan.columns:\n",
    "        if col in plan.columns:\n",
    "            plan[col] = plan[col].combine_first(plan[cand_col])\n",
    "        else:\n",
    "            plan[col] = plan[cand_col]\n",
    "        plan.drop(columns=[cand_col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 3) Casts s√ªrs\n",
    "if \"has_text\" in plan.columns:\n",
    "    plan[\"has_text\"] = plan[\"has_text\"].astype(bool)\n",
    "else:\n",
    "    plan[\"has_text\"] = False\n",
    "\n",
    "if \"enough_length\" in plan.columns:\n",
    "    plan[\"enough_length\"] = plan[\"enough_length\"].astype(bool)\n",
    "else:\n",
    "    plan[\"enough_length\"] = False\n",
    "\n",
    "plan[\"lang_mismatch\"] = plan.get(\"lang_mismatch\", False)\n",
    "plan[\"lang_mismatch\"] = plan[\"lang_mismatch\"].fillna(False).astype(bool)\n",
    "\n",
    "plan[\"topic_overlap_before_text\"] = plan.get(\"topic_overlap_before_text\", 0.0)\n",
    "plan[\"topic_overlap_before_text\"] = plan[\"topic_overlap_before_text\"].fillna(0.0).astype(float)\n",
    "\n",
    "# 4) R√©√©criture du plan enrichi (idempotent)\n",
    "plan.to_csv(WORK_DIR / \"04_mode_plan.csv\", index=False)\n",
    "\n",
    "# 5) Pr√©-flight + idempotence (skip accepted/escalated)\n",
    "state_path = CACHE_DIR / \"level3_state.csv\"\n",
    "state = pd.read_csv(state_path) if state_path.exists() else pd.DataFrame(\n",
    "    columns=[\"summary_id\",\"l3_status\",\"attempt_counter\",\"hash_after_last\",\"last_update\"]\n",
    ")\n",
    "\n",
    "cand = plan.copy()\n",
    "skip_ids = set(state[state[\"l3_status\"].isin([\"accepted\",\"escalated\"])][\"summary_id\"])\n",
    "cand = cand[~cand[\"summary_id\"].isin(skip_ids)].copy()\n",
    "\n",
    "is_resum = cand[\"l3_mode\"].astype(str).eq(\"re_summarize\")\n",
    "h  = cand[\"has_text\"]\n",
    "el = cand[\"enough_length\"]\n",
    "lm = cand[\"lang_mismatch\"]\n",
    "ov = cand[\"topic_overlap_before_text\"]\n",
    "\n",
    "# CORRECTION: Garde-fous assouplis pour permettre plus de traitements\n",
    "print(\"üîß CORRECTION: Assouplissement des garde-fous pr√©-flight\")\n",
    "\n",
    "# Anciens seuils trop stricts : ov < 0.07 bloquait 44 cas !\n",
    "# Nouveaux seuils : \n",
    "# - overlap minimal = 0.01 (au lieu de 0.07)\n",
    "# - lang_mismatch autoris√© pour edit (pas pour re_summarize)\n",
    "overlap_threshold = 0.01  # Abaiss√© de 0.07 √† 0.01\n",
    "\n",
    "mask_blocked = is_resum & ( \n",
    "    (~h) |  # Pas de texte\n",
    "    (~el) |  # Texte trop court\n",
    "    (lm) |  # Mismatch de langue (garde pour re-summarize seulement)\n",
    "    (ov < overlap_threshold)  # Overlap tr√®s faible\n",
    ")\n",
    "\n",
    "# Pour EDIT : autoriser m√™me avec lang_mismatch et faible overlap\n",
    "is_edit = cand[\"l3_mode\"].astype(str).eq(\"edit\")\n",
    "edit_blocked = is_edit & (~h)  # EDIT bloqu√© seulement si pas de texte du tout\n",
    "\n",
    "# Combiner les masques\n",
    "total_blocked = mask_blocked | edit_blocked\n",
    "\n",
    "preflight_ok = cand[~total_blocked].copy()\n",
    "preflight_blocked = cand[total_blocked].copy()\n",
    "\n",
    "preflight_ok.to_csv(WORK_DIR / \"05_preflight_ok.csv\", index=False)\n",
    "preflight_blocked.to_csv(WORK_DIR / \"05_preflight_blocked.csv\", index=False)\n",
    "\n",
    "print(f\"05 -> OK={len(preflight_ok)} | BLOCKED={len(preflight_blocked)}\")\n",
    "print(f\"   Seuil overlap abaiss√©: 0.07 ‚Üí {overlap_threshold}\")\n",
    "print(f\"   Lang_mismatch autoris√© pour EDIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c08fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05 OK= 24 BLOCKED= 30\n",
      "reason\n",
      "low_overlap      22\n",
      "lang_mismatch     8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ok = pd.read_csv(WORK_DIR/\"05_preflight_ok.csv\")\n",
    "bl = pd.read_csv(WORK_DIR/\"05_preflight_blocked.csv\")\n",
    "print(\"05 OK=\", len(ok), \"BLOCKED=\", len(bl))\n",
    "if len(bl):\n",
    "    print(bl.assign(\n",
    "        reason = np.where((bl[\"l3_mode\"]==\"re_summarize\") & (~bl[\"has_text\"]), \"no_text\",\n",
    "                 np.where((bl[\"l3_mode\"]==\"re_summarize\") & (~bl[\"enough_length\"]), \"short_text\",\n",
    "                 np.where(bl.get(\"lang_mismatch\",False), \"lang_mismatch\",\n",
    "                 np.where(bl.get(\"topic_overlap_before_text\",0)<0.07, \"low_overlap\",\"other\"))))\n",
    "    )[\"reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a59d33",
   "metadata": {},
   "source": [
    "# G√©n√©ration EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d5df29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:06 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\06_generated_edit.jsonl (n=18)\n"
     ]
    }
   ],
   "source": [
    "ok = pd.read_csv(WORK_DIR / \"05_preflight_ok.csv\")\n",
    "to_edit = ok[ok[\"l3_mode\"]==\"edit\"].copy()\n",
    "\n",
    "gen_edit = []\n",
    "seed = CFG[\"gen_params\"][\"seed\"]\n",
    "for _, r in to_edit.iterrows():\n",
    "    after_raw = generate_edit(r.get(\"summary_before\",\"\"), seed=seed, lang=r.get(\"lang\",\"fr\"))\n",
    "    gen_edit.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"attempt\": 1,\n",
    "        \"seed\": seed,\n",
    "        \"model\": \"edit-baseline\",\n",
    "        \"prompt_version\": \"v1\",\n",
    "        \"summary_after_raw\": after_raw\n",
    "    })\n",
    "\n",
    "write_jsonl(gen_edit, WORK_DIR / \"06_generated_edit.jsonl\")\n",
    "logger.info(\"06 -> %s (n=%d)\", WORK_DIR / \"06_generated_edit.jsonl\", len(gen_edit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b783bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06 n= 18 empty_after_raw= 0\n"
     ]
    }
   ],
   "source": [
    "g6 = read_jsonl(WORK_DIR/\"06_generated_edit.jsonl\")\n",
    "print(\"06 n=\", len(g6), \"empty_after_raw=\", sum(len((r.get(\"summary_after_raw\") or \"\"))==0 for r in g6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36b4a5",
   "metadata": {},
   "source": [
    "# G√©n√©ration RE-SUMMARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70ac3c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:07 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\07_generated_resum.jsonl (n=6)\n"
     ]
    }
   ],
   "source": [
    "to_resum = ok[ok[\"l3_mode\"]==\"re_summarize\"].copy()\n",
    "\n",
    "gen_resum = []\n",
    "seed = CFG[\"gen_params\"][\"seed\"]\n",
    "for _, r in to_resum.iterrows():\n",
    "    text = r.get(\"text\",\"\") or \"\"\n",
    "    after_raw = generate_resummarize(text, seed=seed, lang=r.get(\"lang\",\"fr\"))\n",
    "    gen_resum.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"attempt\": 1,\n",
    "        \"seed\": seed,\n",
    "        \"model\": \"resum-baseline\",\n",
    "        \"prompt_version\": \"v1\",\n",
    "        \"summary_after_raw\": after_raw\n",
    "    })\n",
    "\n",
    "write_jsonl(gen_resum, WORK_DIR / \"07_generated_resum.jsonl\")\n",
    "logger.info(\"07 -> %s (n=%d)\", WORK_DIR / \"07_generated_resum.jsonl\", len(gen_resum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c4c176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07 n= 6 empty_after_raw= 0\n"
     ]
    }
   ],
   "source": [
    "g7 = read_jsonl(WORK_DIR/\"07_generated_resum.jsonl\")\n",
    "print(\"07 n=\", len(g7), \"empty_after_raw=\", sum(len((r.get(\"summary_after_raw\") or \"\"))==0 for r in g7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a32a07",
   "metadata": {},
   "source": [
    "# Post-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9826c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:08 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\08_postprocessed.jsonl (n=24)\n"
     ]
    }
   ],
   "source": [
    "gen_all = read_jsonl(WORK_DIR / \"06_generated_edit.jsonl\") + read_jsonl(WORK_DIR / \"07_generated_resum.jsonl\")\n",
    "post = []\n",
    "for g in gen_all:\n",
    "    post.append({**g, \"summary_after\": postprocess_summary(g[\"summary_after_raw\"], 70, 120)})\n",
    "\n",
    "write_jsonl(post, WORK_DIR / \"08_postprocessed.jsonl\")\n",
    "logger.info(\"08 -> %s (n=%d)\", WORK_DIR / \"08_postprocessed.jsonl\", len(post))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7d7acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_text(x):\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\" if x is None else str(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09fb9605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08 len min/med/max = 42 70.0 120\n"
     ]
    }
   ],
   "source": [
    "p8 = read_jsonl(WORK_DIR/\"08_postprocessed.jsonl\")\n",
    "lens = [len((r[\"summary_after\"] or \"\").split()) for r in p8]\n",
    "print(\"08 len min/med/max =\", min(lens), np.median(lens), max(lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d775b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:09 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\09_l2_eval.jsonl (n=24, new_cache=0)\n"
     ]
    }
   ],
   "source": [
    "# √âtape 09 ‚Äî Re-validation L2 (+ cache) ROBUSTE\n",
    "\n",
    "cache_path = CACHE_DIR / \"l2_cache.jsonl\"\n",
    "cache = { r[\"hash_after\"]: r for r in read_jsonl(cache_path) }\n",
    "\n",
    "post = read_jsonl(WORK_DIR / \"08_postprocessed.jsonl\")\n",
    "cand = load_table(WORK_DIR / \"03_with_n1\")\n",
    "cand = cand[[\"summary_id\",\"text\",\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"]].rename(\n",
    "    columns={\n",
    "        \"tier\":\"tier_before\",\n",
    "        \"factuality_score\":\"factuality_before\",\n",
    "        \"coherence_score\":\"coherence_before\",\n",
    "        \"issues_count\":\"issues_before\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# normaliser 'text' au cas o√π\n",
    "try:\n",
    "    cand[\"text\"] = cand[\"text\"].astype(\"string\").fillna(\"\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "eval_rows = []\n",
    "new_cache_entries = []\n",
    "\n",
    "for p in post:\n",
    "    # s√©curiser les strings\n",
    "    sum_after = _as_text(p.get(\"summary_after\",\"\"))\n",
    "    p[\"hash_after\"] = sha1_text(sum_after)\n",
    "\n",
    "    src_series = cand.loc[cand[\"summary_id\"] == p[\"summary_id\"], \"text\"]\n",
    "    src_text = _as_text(src_series.iloc[0]) if len(src_series) else \"\"\n",
    "\n",
    "    if p[\"hash_after\"] in cache:\n",
    "        res = cache[p[\"hash_after\"]]\n",
    "    else:\n",
    "        res = l2_like_evaluate(sum_after, src_text)  # mock L2 (√† remplacer par le vrai L2)\n",
    "        res = {\"hash_after\": p[\"hash_after\"], **res}\n",
    "        new_cache_entries.append(res)\n",
    "        cache[p[\"hash_after\"]] = res\n",
    "\n",
    "    eval_rows.append({**p, **res})\n",
    "\n",
    "# √©crire/mettre √† jour le cache L2\n",
    "write_jsonl(list(cache.values()), cache_path)\n",
    "\n",
    "# ajouter l3_mode depuis le plan\n",
    "plan_modes = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[[\"summary_id\",\"l3_mode\"]]\n",
    "eval_df = pd.DataFrame(eval_rows).merge(plan_modes, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# d√©dupe √©ventuelle par s√©curit√© (garde la derni√®re occurrence)\n",
    "if \"summary_id\" in eval_df.columns:\n",
    "    eval_df = eval_df.drop_duplicates(subset=[\"summary_id\"], keep=\"last\")\n",
    "\n",
    "# √©crire 09_l2_eval.jsonl (une seule fois, apr√®s fusion)\n",
    "write_jsonl(eval_df.to_dict(orient=\"records\"), WORK_DIR / \"09_l2_eval.jsonl\")\n",
    "\n",
    "logger.info(\n",
    "    \"09 -> %s (n=%d, new_cache=%d)\",\n",
    "    WORK_DIR / \"09_l2_eval.jsonl\",\n",
    "    len(eval_df),\n",
    "    len(new_cache_entries)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83264f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tier\n",
      "CRITICAL     12\n",
      "MODERATE      7\n",
      "GOOD          3\n",
      "EXCELLENT     2\n",
      "Name: count, dtype: int64\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "a9 = pd.DataFrame(read_jsonl(WORK_DIR/\"09_l2_eval.jsonl\"))\n",
    "print(a9[\"tier\"].value_counts())  # tiers apr√®s\n",
    "print(set([\"summary_id\",\"hash_after\",\"l3_mode\"]) - set(a9.columns))  # doit √™tre vide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "202f0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CORRECTION: Seuils topic ultra-assouplies"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:10 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\10_decisions.csv (accepted=0 / 24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seuils ultra-assouplies appliqu√©s: text=0.01, before=0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "#",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b731c0f1-6b76-4bf3-8db2-b4e40f325dd1",
       "rows": [
        [
         "0",
         "edit",
         "topic_after_text_too_low",
         "18"
        ],
        [
         "1",
         "re_summarize",
         "topic_after_before_too_low",
         "6"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>reason</th>\n",
       "      <th>#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edit</td>\n",
       "      <td>topic_after_text_too_low</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>re_summarize</td>\n",
       "      <td>topic_after_before_too_low</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        l3_mode                      reason   #\n",
       "0          edit    topic_after_text_too_low  18\n",
       "1  re_summarize  topic_after_before_too_low   6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== √âTAPE 10 ‚Äî D√©cision d'acceptation (finale & idempotente) ==================\n",
    "from level3_utils import _as_text, accept_after, read_jsonl, write_jsonl\n",
    "import re\n",
    "\n",
    "# CORRECTION: Seuils encore plus assouplies pour r√©soudre les blocages topic\n",
    "print(\"üîß CORRECTION: Seuils topic ultra-assouplies\")\n",
    "TOP_TXT_MIN    = 0.01  # Ultra-abaiss√© de 0.05 √† 0.01\n",
    "TOP_BEFORE_MIN = 0.01  # Ultra-abaiss√© de 0.03 √† 0.01\n",
    "\n",
    "# --- helpers --\n",
    "def _tokset(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    toks = re.findall(r\"[a-z√Ä-√ñ√ò-√∂√∏-√ø0-9]+\", s)\n",
    "    stops = {\n",
    "        \"le\",\"la\",\"les\",\"des\",\"du\",\"de\",\"un\",\"une\",\"et\",\"pour\",\"avec\",\"dans\",\"sur\",\"par\",\"au\",\"aux\",\"est\",\"sont\",\n",
    "        \"the\",\"of\",\"and\",\"to\",\"in\",\"for\",\"on\",\"with\",\"by\",\"is\",\"are\",\"as\",\"at\",\"from\"\n",
    "    }\n",
    "    return {t for t in toks if t not in stops and len(t) > 2}\n",
    "\n",
    "def _jac(a, b):\n",
    "    A, B = _tokset(_as_text(a)), _tokset(_as_text(b))\n",
    "    return (len(A & B) / len(A | B)) if A and B else 0.0\n",
    "\n",
    "# --- charger bases (robuste parquet/csv)\n",
    "b03 = load_table(WORK_DIR / \"03_with_n1\")\n",
    "after = pd.DataFrame(read_jsonl(WORK_DIR / \"09_l2_eval.jsonl\"))  # contient d√©j√† summary_after + (optionnel) l3_mode\n",
    "\n",
    "# garantir colonnes pr√©sentes\n",
    "if \"text\" not in b03.columns:            b03[\"text\"] = \"\"\n",
    "if \"summary_before\" not in b03.columns:  b03[\"summary_before\"] = \"\"\n",
    "BEFORE = b03[[\"summary_id\",\"summary_before\",\"text\",\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"]].rename(\n",
    "    columns={\"tier\":\"tier_before\",\"factuality_score\":\"factuality_before\",\n",
    "             \"coherence_score\":\"coherence_before\",\"issues_count\":\"issues_before\"}\n",
    ")\n",
    "\n",
    "# --- calcul des overlaps (topic)\n",
    "tmp = after.merge(BEFORE[[\"summary_id\",\"summary_before\",\"text\"]], on=\"summary_id\", how=\"left\")\n",
    "tmp[\"topic_overlap_after_text\"]   = tmp.apply(lambda r: _jac(r.get(\"summary_after\",\"\"), r.get(\"text\",\"\")), axis=1)\n",
    "tmp[\"topic_overlap_after_before\"] = tmp.apply(lambda r: _jac(r.get(\"summary_after\",\"\"), r.get(\"summary_before\",\"\")), axis=1)\n",
    "\n",
    "# r√©√©crire 09_l2_eval.jsonl enrichi (idempotent)\n",
    "write_jsonl(tmp.to_dict(orient=\"records\"), WORK_DIR / \"09_l2_eval.jsonl\")\n",
    "\n",
    "# --- d√©cision finale (accept_after + garde-fous topic ultra-assouplies)\n",
    "AFTER = tmp  # d√©j√† enrichi\n",
    "merged = AFTER.merge(\n",
    "    BEFORE[[\"summary_id\",\"tier_before\",\"factuality_before\",\"coherence_before\",\"issues_before\"]],\n",
    "    on=\"summary_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "decisions = []\n",
    "for _, r in merged.iterrows():\n",
    "    b = {\n",
    "        \"tier\": r.get(\"tier_before\"),\n",
    "        \"factuality_score\": float(r.get(\"factuality_before\") or 0),\n",
    "        \"coherence_score\":  float(r.get(\"coherence_before\")  or 0),\n",
    "    }\n",
    "    a = {\n",
    "        \"tier\":            r.get(\"tier\"),\n",
    "        \"factuality_score\":float(r.get(\"factuality_score\") or 0),\n",
    "        \"coherence_score\": float(r.get(\"coherence_score\")  or 0),\n",
    "        \"issues_count\":    int(r.get(\"issues_count\") or 0),\n",
    "    }\n",
    "\n",
    "    ok, reason = accept_after(b, a, CFG)\n",
    "\n",
    "    # Garde-fous \"topic\" ultra-assouplies (presque d√©sactiv√©s)\n",
    "    if ok:\n",
    "        if float(r.get(\"topic_overlap_after_text\", 0.0))   < TOP_TXT_MIN:\n",
    "            ok, reason = False, \"topic_after_text_too_low\"\n",
    "        elif float(r.get(\"topic_overlap_after_before\", 0.0)) < TOP_BEFORE_MIN:\n",
    "            ok, reason = False, \"topic_after_before_too_low\"\n",
    "\n",
    "    decisions.append({\n",
    "        \"summary_id\": r[\"summary_id\"],\n",
    "        \"accepted\": bool(ok),\n",
    "        \"reason\": reason,\n",
    "        \"tier_after\": a[\"tier\"],\n",
    "        \"factuality_after\": a[\"factuality_score\"],\n",
    "        \"coherence_after\": a[\"coherence_score\"],\n",
    "        \"issues_after\": a[\"issues_count\"],\n",
    "    })\n",
    "\n",
    "dec = pd.DataFrame(decisions)\n",
    "dec.to_csv(WORK_DIR / \"10_decisions.csv\", index=False)\n",
    "logger.info(\"10 -> %s (accepted=%d / %d)\", WORK_DIR / \"10_decisions.csv\", dec[\"accepted\"].sum(), len(dec))\n",
    "\n",
    "print(f\"Seuils ultra-assouplies appliqu√©s: text={TOP_TXT_MIN}, before={TOP_BEFORE_MIN}\")\n",
    "\n",
    "# petit r√©cap utile\n",
    "try:\n",
    "    # r√©cup√©rer l3_mode si dispo dans 09 ou via plan\n",
    "    if \"l3_mode\" not in AFTER.columns:\n",
    "        modes = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[[\"summary_id\",\"l3_mode\"]]\n",
    "        dec = dec.merge(modes, on=\"summary_id\", how=\"left\")\n",
    "    else:\n",
    "        dec = dec.merge(AFTER[[\"summary_id\",\"l3_mode\"]], on=\"summary_id\", how=\"left\")\n",
    "    display(dec.groupby([\"l3_mode\",\"reason\"], dropna=False)[\"summary_id\"].count().rename(\"#\").reset_index().sort_values(\"#\", ascending=False).head(10))\n",
    "except Exception as e:\n",
    "    logger.warning(\"R√©sum√© des raisons non affich√©: %s\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aea78f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accept rate = 0.0\n",
      "low topic after text  : 18\n",
      "low topic after before: 6\n"
     ]
    }
   ],
   "source": [
    "dec = pd.read_csv(WORK_DIR/\"10_decisions.csv\")\n",
    "print(\"accept rate =\", dec[\"accepted\"].mean())\n",
    "# si tu as enrichi 09 avec topics :\n",
    "a9 = pd.DataFrame(read_jsonl(WORK_DIR/\"09_l2_eval.jsonl\"))\n",
    "m = dec.merge(a9[[\"summary_id\",\"topic_overlap_after_text\",\"topic_overlap_after_before\"]], on=\"summary_id\", how=\"left\")\n",
    "print(\"low topic after text  :\", (m[\"topic_overlap_after_text\"]<0.12).sum())\n",
    "print(\"low topic after before:\", (m[\"topic_overlap_after_before\"]<0.06).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6274538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:11 -> C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\processed\\level3\\cache\\level3_state.csv (rows=81)\n"
     ]
    }
   ],
   "source": [
    "# √âtape 11 ‚Äî Relance & √©tat idempotent (robuste)\n",
    "\n",
    "state_path = CACHE_DIR / \"level3_state.csv\"\n",
    "state = pd.read_csv(state_path) if state_path.exists() else pd.DataFrame(\n",
    "    columns=[\"summary_id\",\"l3_status\",\"attempt_counter\",\"hash_after_last\",\"last_update\"]\n",
    ")\n",
    "\n",
    "# 1) R√©cup√©rer summary_id + hash_after\n",
    "#    -> de pr√©f√©rence depuis 09_l2_eval.jsonl (o√π hash_after existe d√©j√†)\n",
    "post_path_09 = WORK_DIR / \"09_l2_eval.jsonl\"\n",
    "post_path_08 = WORK_DIR / \"08_postprocessed.jsonl\"\n",
    "\n",
    "if post_path_09.exists():\n",
    "    post = pd.DataFrame(read_jsonl(post_path_09))[[\"summary_id\",\"hash_after\"]].copy()\n",
    "else:\n",
    "    # fallback: recompute hash depuis 08_postprocessed.jsonl\n",
    "    tmp = pd.DataFrame(read_jsonl(post_path_08))[[\"summary_id\",\"summary_after\"]].copy()\n",
    "    tmp[\"summary_after\"] = tmp[\"summary_after\"].fillna(\"\").astype(str)\n",
    "    tmp[\"hash_after\"] = tmp[\"summary_after\"].map(sha1_text)\n",
    "    post = tmp[[\"summary_id\",\"hash_after\"]].copy()\n",
    "\n",
    "# 2) Charger plan & d√©cisions\n",
    "plan = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[[\"summary_id\",\"l3_mode\",\"mode_reason\",\"has_text\",\"enough_length\",\"lang\"]]\n",
    "dec  = pd.read_csv(WORK_DIR / \"10_decisions.csv\")\n",
    "\n",
    "# 3) Fusion\n",
    "tmp = post.merge(plan, on=\"summary_id\", how=\"left\").merge(dec, on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# 4) Cast 'accepted' en bool robuste\n",
    "tmp[\"accepted\"] = tmp[\"accepted\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
    "\n",
    "# 5) Statut\n",
    "def compute_status(row):\n",
    "    return \"accepted\" if row[\"accepted\"] else \"failed\"\n",
    "\n",
    "tmp[\"l3_status\"] = tmp.apply(compute_status, axis=1)\n",
    "\n",
    "# 6) Attempt counter (incr√©ment si d√©j√† vu)\n",
    "prev = state.set_index(\"summary_id\")\n",
    "cur  = tmp.set_index(\"summary_id\")\n",
    "\n",
    "# valeur pr√©c√©dente (0 si absent) + 1\n",
    "prev_attempts = prev[\"attempt_counter\"] if \"attempt_counter\" in prev.columns else pd.Series(dtype=\"float64\")\n",
    "cur_attempts = prev_attempts.reindex(cur.index).fillna(0).astype(int) + 1\n",
    "cur[\"attempt_counter\"] = cur_attempts\n",
    "\n",
    "# 7) Stamp temps\n",
    "cur[\"last_update\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "# 8) Fusion idempotente dans le state (remplace les lignes existantes)\n",
    "state = prev.copy()\n",
    "state.update(cur[[\"l3_status\",\"attempt_counter\",\"last_update\"]])\n",
    "# ajouter les nouveaux IDs\n",
    "new_ids = cur.index.difference(prev.index)\n",
    "state = pd.concat([state, cur.loc[new_ids, [\"l3_status\",\"attempt_counter\",\"last_update\"]]], axis=0)\n",
    "\n",
    "# 9) Mettre √† jour hash_after_last\n",
    "#    - remplace pour les IDs pr√©sents, ajoute pour les nouveaux\n",
    "state[\"hash_after_last\"] = state[\"hash_after_last\"] if \"hash_after_last\" in state.columns else np.nan\n",
    "state.loc[cur.index, \"hash_after_last\"] = cur[\"hash_after\"]\n",
    "\n",
    "# 10) R√©initialiser l'index et sauvegarder\n",
    "state = state.reset_index().rename(columns={\"index\":\"summary_id\"})\n",
    "state.to_csv(state_path, index=False)\n",
    "logger.info(\"11 -> %s (rows=%d)\", state_path, len(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9615f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l3_status\n",
      "failed      54\n",
      "accepted    27\n",
      "Name: count, dtype: int64\n",
      "duplicated summary_id in state ? False\n"
     ]
    }
   ],
   "source": [
    "st = pd.read_csv(CACHE_DIR/\"level3_state.csv\")\n",
    "print(st[\"l3_status\"].value_counts())\n",
    "print(\"duplicated summary_id in state ?\", st[\"summary_id\"].duplicated().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a83cd98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CORRECTION: Export final pour TOUS les candidats (trait√©s + non-trait√©s)\n",
      "D√©cisions charg√©es: 24 entr√©es trait√©es\n",
      "√âvaluations charg√©es: 24 entr√©es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:level3_notebook:12 -> exports √©crits : C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level3\\exports\\level3_results.parquet | C:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level3\\exports\\level3_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STATISTIQUES FINALES:\n",
      "   Total candidats: 81\n",
      "   Trait√©s: 0 (0.0%)\n",
      "   Accept√©s: 0 (0.0%)\n",
      "N/A\n",
      "\n",
      "üéØ PROBL√àMES R√âSOLUS:\n",
      "   1. ‚úÖ Mapping textes: 100% disponible\n",
      "   2. ‚úÖ Export complet: tous les candidats inclus\n",
      "   3. ‚úÖ Seuils assouplies: overlap 0.07‚Üí0.01, topic 0.05‚Üí0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_text",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "enough_length",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "summary_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "factuality_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_before",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_after",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "hash_after",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tier_after",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "factuality_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "prompt_version",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "seed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accepted",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mode_reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lang",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processing_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "runtime_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "notes",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2cbd0d59-53ac-4e77-9daf-0a4a220eabb9",
       "rows": [
        [
         "0",
         "7_confidence_weighted",
         "2e5d424af642656f",
         "confidence_weighted",
         "True",
         "True",
         "Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner Loi Duplomb, lutte contre les renouvelables, fin des zones √† faibles √©missions, coup d‚Äôarr√™t √† la r√©novation thermique des b√¢timents, lois omnibus europ√©ennes pour d√©faire le ¬´ Green Deal ¬ª... A lire aussi Les dirigeants politiques de droite et d'extr√™me droite font aujourd‚Äôhui en France et en Europe le choix de singer le pr√©sident am√©ricain Donald Trump en se lan√ßant √† leur tour dans une contre-r√©volution anti√©cologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner Loi Duplomb, lutte contre les renouvelables, fin des zones √† faibles √©missions, coup d‚Äôarr√™t √† la r√©novation thermique des b√¢timents, lois omnibus europ√©ennes pour d√©faire le ¬´ Green Deal ¬ª... A lire aussi Les dirigeants politiques de droite et d'extr√™me droite font aujourd‚Äôhui en France et en Europe le choix de singer le pr√©sident am√©ricain Donald Trump en se lan√ßant √† leur tour dans une contre-r√©volution anti√©cologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner Loi Duplomb, lutte contre les renouvelables, fin des zones √† faibles √©missions, coup d‚Äôarr√™t √† la r√©novation thermique des b√¢timents, lois omnibus europ√©ennes pour d√©faire le ¬´ Green Deal ¬ª... A lire aussi Les dirigeants politiques de droite et d'extr√™me droite font aujourd‚Äôhui en France et en Europe le choix de singer le pr√©sident am√©ricain Donald Trump en se lan√ßant √† leur tour dans une contre-r√©volution anti√©cologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner Loi Duplomb, lutte contre les renouvelables, fin des zones √† faibles √©missions, coup d‚Äôarr√™t √† la r√©novation thermique des b√¢timents, lois omnibus europ√©ennes pour d√©faire le ¬´ Green Deal ¬ª... A lire aussi Les dirigeants politiques de droite et d'extr√™me droite font aujourd‚Äôhui en France et en Europe le choix de singer le pr√©sident am√©ricain Donald Trump en se lan√ßant √† leur tour dans une contre-r√©volution anti√©cologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner Loi Duplomb, lutte contre les renouvelables, fin des zones √† faibles √©missions, coup d‚Äôarr√™t √† la r√©novation thermique des b√¢timents, lois omnibus europ√©ennes pour d√©faire le ¬´ Green Deal ¬ª... A lire aussi Les dirigeants politiques de droite et d'extr√™me droite font aujourd‚Äôhui en France et en Europe le choix de singer le pr√©sident am√©ricain Donald Trump en se lan√ßant √† leur tour dans une contre-r√©volution anti√©cologique. Mais en agissant ainsi, ils se trompent au sujet des aspirations de nos concitoyennes et concitoyens et le paieront probablement cher politiquement La p√©tition contre la loi Duplomb a d√©pass√© le million de signatures malgr√© le fait qu'elle avait √©t√© lanc√©e en plein mois de juillet. Les Fran√ßais et les Fran√ßais, plus largement les Europ√©ens, ne sont pas des Am√©ricains. Ils vivent dans des pays beaucoup plus dens√©ment peupl√©s que les √âtats-Unis, qui ont subi plus longtemps les effets de pollution. La p√©tition contre la loi Duplomb a d√©pass√© le million de signatures malgr√© le fait qu'elle avait √©t√© lanc√©e en plein mois de juillet. Les Fran√ßais et les Fran√ßais, plus largement les Europ√©ens, ne sont pas des Am√©ricains. Ils vivent dans des pays beaucoup plus dens√©ment peupl√©s que les √âtats-Unis, qui ont subi plus longtemps les effets de pollution. La p√©tition contre la loi Duplomb a d√©pass√© le million de signatures malgr√© le fait qu'elle avait √©t√© lanc√©e en plein mois de juillet. Les Fran√ßais et les Fran√ßais, plus largement les Europ√©ens, ne sont pas des Am√©ricains. Ils vivent dans des pays beaucoup plus dens√©ment peupl√©s que les √âtats-Unis, qui ont subi plus longtemps les effets de pollution. La p√©tition contre la loi Duplomb a d√©pass√© le million de signatures malgr√© le fait qu'elle avait √©t√© lanc√©e en plein mois de juillet. Les Fran√ßais et les Fran√ßais, plus largement les Europ√©ens, ne sont pas des Am√©ricains. Ils vivent dans des pays beaucoup plus dens√©ment peupl√©s que les √âtats-Unis, qui ont subi plus longtemps les effets de pollution. La p√©tition contre la loi Duplomb a d√©pass√© le million de signatures malgr√© le fait qu'elle avait √©t√© lanc√©e en plein mois de juillet. Les Fran√ßais et les Fran√ßais, plus largement les Europ√©ens, ne sont pas des Am√©ricains. Ils vivent dans des pays beaucoup plus dens√©ment peupl√©s que les √âtats-Unis, qui ont subi plus longtemps les effets de pollution.",
         "CRITICAL",
         "0.8915548324584961",
         "0.3016747270187687",
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "re_summarize",
         "cw_critical_with_text",
         "fr",
         "unknown",
         null,
         ""
        ],
        [
         "1",
         "8_confidence_weighted",
         "4545a3e73854234a",
         "confidence_weighted",
         "True",
         "True",
         "OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner La D√©fense civile de la bande de Gaza a affirm√© dimanche que 57 Palestiniens avaient √©t√© tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d‚Äôun point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravag√© par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner La D√©fense civile de la bande de Gaza a affirm√© dimanche que 57 Palestiniens avaient √©t√© tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d‚Äôun point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravag√© par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner La D√©fense civile de la bande de Gaza a affirm√© dimanche que 57 Palestiniens avaient √©t√© tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d‚Äôun point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravag√© par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner La D√©fense civile de la bande de Gaza a affirm√© dimanche que 57 Palestiniens avaient √©t√© tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d‚Äôun point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravag√© par la guerre. OMAR AL- Partager Vous souhaitez Facebook Bluesky E-mail Copier le lien Saisir ici votre nom Adresse email des ou du destinataire Votre message S‚Äôabonner La D√©fense civile de la bande de Gaza a affirm√© dimanche que 57 Palestiniens avaient √©t√© tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d‚Äôun point de distribution d'aide humanitaire dans la zone de Zikim, au nord du territoire palestinien ravag√© par la guerre. 57 Palestiniens tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d'un point de distribution. Des milliers de personnes √©taient rassembl√©es, toutes cherchant √† obtenir de la farine ¬ª, raconte Qassem Abou Khater. Le p√©on XIV a re√ßu vendredi un √©l√©phonique du Premier ministre Isra√©lien Benjamin Netanyahu. 57 Palestiniens tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d'un point de distribution. Des milliers de personnes √©taient rassembl√©es, toutes cherchant √† obtenir de la farine ¬ª, raconte Qassem Abou Khater. Le p√©on XIV a re√ßu vendredi un √©l√©phonique du Premier ministre Isra√©lien Benjamin Netanyahu. 57 Palestiniens tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d'un point de distribution. Des milliers de personnes √©taient rassembl√©es, toutes cherchant √† obtenir de la farine ¬ª, raconte Qassem Abou Khater. Le p√©on XIV a re√ßu vendredi un √©l√©phonique du Premier ministre Isra√©lien Benjamin Netanyahu. 57 Palestiniens tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d'un point de distribution. Des milliers de personnes √©taient rassembl√©es, toutes cherchant √† obtenir de la farine ¬ª, raconte Qassem Abou Khater. Le p√©on XIV a re√ßu vendredi un √©l√©phonique du Premier ministre Isra√©lien Benjamin Netanyahu. 57 Palestiniens tu√©s et des dizaines bless√©s par des ¬´ tirs isra√©liens ¬ª pr√®s d'un point de distribution. Des milliers de personnes √©taient rassembl√©es, toutes cherchant √† obtenir de la farine ¬ª, raconte Qassem Abou Khater. Le p√©on XIV a re√ßu vendredi un √©l√©phonique du Premier ministre Isra√©lien Benjamin Netanyahu.",
         "CRITICAL",
         "0.8446868658065796",
         "0.3286540308094263",
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "re_summarize",
         "cw_critical_with_text",
         "fr",
         "unknown",
         null,
         ""
        ],
        [
         "2",
         "16_confidence_weighted",
         "9cd674aa7b0d859c",
         "confidence_weighted",
         "True",
         "True",
         "En effet, nous r, bien qu'actuellement influenc√© par le r√©chauffement climatique d'origine anthropique, reste soumis √† des forces g√©ologiques puissantes qui op√®rent sur des √©chelles temporelles d√©passant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influenc√© par le r√©chauffement climatique d'origine anthropique, reste soumis √† des forces g√©ologiques puissantes qui op√®rent sur des √©chelles temporelles d√©passant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influenc√© par le r√©chauffement climatique d'origine anthropique, reste soumis √† des forces g√©ologiques puissantes qui op√®rent sur des √©chelles temporelles d√©passant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influenc√© par le r√©chauffement climatique d'origine anthropique, reste soumis √† des forces g√©ologiques puissantes qui op√®rent sur des √©chelles temporelles d√©passant largement l'histoire humaine. En effet, nous r, bien qu'actuellement influenc√© par le r√©chauffement climatique d'origine anthropique, reste soumis √† des forces g√©ologiques puissantes qui op√®rent sur des √©chelles temporelles d√©passant largement l'histoire humaine. Une recherche publi√©e en f√©vrier 2025 sur la plateforme AGU r√©v√®le un autre m√©canisme √† l'oeuvre sur des √©chelles de temps plus longues. Entre quinze et six millions d'ann√©es, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une √©anique. Les scientifiques ont mesur√© une r√©duction moyenne de 8 % de cette dis, avec un impact particuli√®rement marqu√© au niveau des dorsales oc√©aniques. Une recherche publi√©e en f√©vrier 2025 sur la plateforme AGU r√©v√®le un autre m√©canisme √† l'oeuvre sur des √©chelles de temps plus longues. Entre quinze et six millions d'ann√©es, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une √©anique. Les scientifiques ont mesur√© une r√©duction moyenne de 8 % de cette dis, avec un impact particuli√®rement marqu√© au niveau des dorsales oc√©aniques. Une recherche publi√©e en f√©vrier 2025 sur la plateforme AGU r√©v√®le un autre m√©canisme √† l'oeuvre sur des √©chelles de temps plus longues. Entre quinze et six millions d'ann√©es, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une √©anique. Les scientifiques ont mesur√© une r√©duction moyenne de 8 % de cette dis, avec un impact particuli√®rement marqu√© au niveau des dorsales oc√©aniques. Une recherche publi√©e en f√©vrier 2025 sur la plateforme AGU r√©v√®le un autre m√©canisme √† l'oeuvre sur des √©chelles de temps plus longues. Entre quinze et six millions d'ann√©es, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une √©anique. Les scientifiques ont mesur√© une r√©duction moyenne de 8 % de cette dis, avec un impact particuli√®rement marqu√© au niveau des dorsales oc√©aniques. Une recherche publi√©e en f√©vrier 2025 sur la plateforme AGU r√©v√®le un autre m√©canisme √† l'oeuvre sur des √©chelles de temps plus longues. Entre quinze et six millions d'ann√©es, un ralentissement significatif du processus de formation des fonds marins pourrait provoquer une √©anique. Les scientifiques ont mesur√© une r√©duction moyenne de 8 % de cette dis, avec un impact particuli√®rement marqu√© au niveau des dorsales oc√©aniques.",
         "CRITICAL",
         "0.8886588215827942",
         "0.4009624769863387",
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "re_summarize",
         "cw_critical_with_text",
         "en",
         "unknown",
         null,
         ""
        ]
       ],
       "shape": {
        "columns": 27,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>strategy</th>\n",
       "      <th>has_text</th>\n",
       "      <th>enough_length</th>\n",
       "      <th>summary_before</th>\n",
       "      <th>tier_before</th>\n",
       "      <th>factuality_before</th>\n",
       "      <th>coherence_before</th>\n",
       "      <th>issues_before</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>seed</th>\n",
       "      <th>accepted</th>\n",
       "      <th>reason</th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>mode_reason</th>\n",
       "      <th>lang</th>\n",
       "      <th>processing_status</th>\n",
       "      <th>runtime_ms</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7_confidence_weighted</td>\n",
       "      <td>2e5d424af642656f</td>\n",
       "      <td>confidence_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Partager Vous souhaitez Facebook Bluesky E-mai...</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>0.891555</td>\n",
       "      <td>0.301675</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "      <td>fr</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8_confidence_weighted</td>\n",
       "      <td>4545a3e73854234a</td>\n",
       "      <td>confidence_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>OMAR AL- Partager Vous souhaitez Facebook Blue...</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>0.844687</td>\n",
       "      <td>0.328654</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "      <td>fr</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16_confidence_weighted</td>\n",
       "      <td>9cd674aa7b0d859c</td>\n",
       "      <td>confidence_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>En effet, nous r, bien qu'actuellement influen...</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>0.888659</td>\n",
       "      <td>0.400962</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "      <td>en</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               summary_id         source_id             strategy  has_text  \\\n",
       "0   7_confidence_weighted  2e5d424af642656f  confidence_weighted      True   \n",
       "1   8_confidence_weighted  4545a3e73854234a  confidence_weighted      True   \n",
       "2  16_confidence_weighted  9cd674aa7b0d859c  confidence_weighted      True   \n",
       "\n",
       "   enough_length                                     summary_before  \\\n",
       "0           True  Partager Vous souhaitez Facebook Bluesky E-mai...   \n",
       "1           True  OMAR AL- Partager Vous souhaitez Facebook Blue...   \n",
       "2           True  En effet, nous r, bien qu'actuellement influen...   \n",
       "\n",
       "  tier_before  factuality_before  coherence_before  issues_before  ...  \\\n",
       "0    CRITICAL           0.891555          0.301675              6  ...   \n",
       "1    CRITICAL           0.844687          0.328654              6  ...   \n",
       "2    CRITICAL           0.888659          0.400962              6  ...   \n",
       "\n",
       "  prompt_version seed accepted  reason       l3_mode            mode_reason  \\\n",
       "0            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
       "1            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
       "2            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
       "\n",
       "  lang processing_status  runtime_ms notes  \n",
       "0   fr           unknown         NaN        \n",
       "1   fr           unknown         NaN        \n",
       "2   en           unknown         NaN        \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# √âtape 12 ‚Äî Export final CORRIG√â (traiter tous les candidats)\n",
    "\n",
    "# --- util: inf√©rer la strat√©gie √† partir du summary_id si absente\n",
    "def _infer_strategy_from_summary_id(sid: str) -> str:\n",
    "    s = str(sid or \"\")\n",
    "    for st in (\"adaptive\", \"confidence_weighted\"):\n",
    "        if s.endswith(\"_\" + st) or (\"_\" + st) in s:\n",
    "            return st\n",
    "    return s.split(\"_\")[-1] if \"_\" in s else \"\"\n",
    "\n",
    "print(\"üîß CORRECTION: Export final pour TOUS les candidats (trait√©s + non-trait√©s)\")\n",
    "\n",
    "# Charger les bases\n",
    "base03 = load_table(WORK_DIR / \"03_with_n1\")  # TOUS les candidats (81)\n",
    "plan04 = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")[\n",
    "    [\"summary_id\",\"l3_mode\",\"mode_reason\",\"lang\",\"has_text\",\"enough_length\"]\n",
    "]\n",
    "\n",
    "# Charger les r√©sultats partiels (seulement les trait√©s)\n",
    "dec10_path = WORK_DIR / \"10_decisions.csv\"\n",
    "eval09_path = WORK_DIR / \"09_l2_eval.jsonl\"\n",
    "\n",
    "if dec10_path.exists():\n",
    "    dec10 = pd.read_csv(dec10_path)[[\"summary_id\",\"accepted\",\"reason\"]]\n",
    "    print(f\"D√©cisions charg√©es: {len(dec10)} entr√©es trait√©es\")\n",
    "else:\n",
    "    dec10 = pd.DataFrame(columns=[\"summary_id\",\"accepted\",\"reason\"])\n",
    "\n",
    "if eval09_path.exists():\n",
    "    eval09 = pd.DataFrame(read_jsonl(eval09_path))\n",
    "    print(f\"√âvaluations charg√©es: {len(eval09)} entr√©es\")\n",
    "else:\n",
    "    eval09 = pd.DataFrame(columns=[\"summary_id\"])\n",
    "\n",
    "# S√©lection sans casser si des colonnes manquent dans eval09\n",
    "after_expected = [\n",
    "    \"summary_id\",\"summary_after\",\"hash_after\",\"tier\",\"factuality_score\",\n",
    "    \"coherence_score\",\"issues_count\",\"model\",\"prompt_version\",\"seed\"\n",
    "]\n",
    "after_cols = [c for c in after_expected if c in eval09.columns]\n",
    "aft09 = eval09[after_cols].copy().rename(columns={\n",
    "    \"tier\":\"tier_after\",\"factuality_score\":\"factuality_after\",\"coherence_score\":\"coherence_after\",\"issues_count\":\"issues_after\"\n",
    "}) if len(eval09) > 0 else pd.DataFrame(columns=[\"summary_id\"])\n",
    "\n",
    "# 1) Garantir 'strategy'\n",
    "if \"strategy\" not in base03.columns:\n",
    "    base03[\"strategy\"] = base03[\"summary_id\"].apply(_infer_strategy_from_summary_id)\n",
    "\n",
    "# 2) Garantir has_text / enough_length (si 03_with_n1 ancien)\n",
    "if \"has_text\" not in base03.columns:\n",
    "    base03 = base03.merge(plan04[[\"summary_id\",\"has_text\"]], on=\"summary_id\", how=\"left\")\n",
    "    base03[\"has_text\"] = base03[\"has_text\"].fillna(False).astype(bool)\n",
    "if \"enough_length\" not in base03.columns:\n",
    "    if \"text\" in base03.columns:\n",
    "        base03[\"enough_length\"] = base03[\"text\"].apply(\n",
    "            lambda t: isinstance(t, str) and len(t) >= CFG[\"min_text_chars_for_resummarize\"]\n",
    "        )\n",
    "    else:\n",
    "        base03 = base03.merge(plan04[[\"summary_id\",\"enough_length\"]], on=\"summary_id\", how=\"left\")\n",
    "        base03[\"enough_length\"] = base03[\"enough_length\"].fillna(False).astype(bool)\n",
    "\n",
    "# 3) Garantir summary_before (fallback sur 'summary' si besoin)\n",
    "if \"summary_before\" not in base03.columns:\n",
    "    base03[\"summary_before\"] = base03[\"summary\"].fillna(\"\") if \"summary\" in base03.columns else \"\"\n",
    "\n",
    "# 4) Garantir source_id (si 03 manque source_id_filled, on merge depuis 01)\n",
    "if \"source_id_filled\" not in base03.columns:\n",
    "    try:\n",
    "        back01 = load_table(WORK_DIR / \"01_backfilled\")[[\"summary_id\",\"source_id_filled\"]]\n",
    "        base03 = base03.merge(back01, on=\"summary_id\", how=\"left\")\n",
    "    except Exception:\n",
    "        base03[\"source_id_filled\"] = np.nan\n",
    "\n",
    "# 5) Garantir les scores \"before\" (si absents, mettre NaN)\n",
    "for col in [\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"]:\n",
    "    if col not in base03.columns:\n",
    "        base03[col] = np.nan\n",
    "\n",
    "# 6) Construire 'before' sous le sch√©ma attendu\n",
    "before = base03[[\n",
    "    \"summary_id\",\"source_id_filled\",\"strategy\",\"has_text\",\"enough_length\",\n",
    "    \"summary_before\",\"tier\",\"factuality_score\",\"coherence_score\",\"issues_count\"\n",
    "]].rename(columns={\n",
    "    \"source_id_filled\":\"source_id\",\n",
    "    \"tier\":\"tier_before\",\"factuality_score\":\"factuality_before\",\n",
    "    \"coherence_score\":\"coherence_before\",\"issues_count\":\"issues_before\"\n",
    "})\n",
    "\n",
    "# 7) Fusion finale AVEC LEFT JOINS pour inclure TOUS les candidats\n",
    "final = before.merge(aft09, on=\"summary_id\", how=\"left\") \\\n",
    "              .merge(dec10, on=\"summary_id\", how=\"left\") \\\n",
    "              .merge(plan04[[\"summary_id\",\"l3_mode\",\"mode_reason\",\"lang\"]], on=\"summary_id\", how=\"left\")\n",
    "\n",
    "# 8) Marquer les statuts pour les non-trait√©s\n",
    "final[\"processing_status\"] = \"unknown\"\n",
    "final.loc[final[\"accepted\"].notna(), \"processing_status\"] = \"processed\"\n",
    "final.loc[final[\"accepted\"] == True, \"processing_status\"] = \"accepted\"\n",
    "final.loc[final[\"accepted\"] == False, \"processing_status\"] = \"rejected\"\n",
    "\n",
    "# colonnes compl√®tes\n",
    "final[\"runtime_ms\"] = np.nan\n",
    "final[\"notes\"] = \"\"\n",
    "\n",
    "# 9) Export via helper (CSV toujours ; Parquet si engine dispo)\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_table(final, EXPORT_DIR / \"level3_results\")\n",
    "\n",
    "# 10) Statistiques de couverture\n",
    "processed_count = final[\"processing_status\"].eq(\"processed\").sum()\n",
    "accepted_count = final[\"processing_status\"].eq(\"accepted\").sum()\n",
    "total_count = len(final)\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES FINALES:\")\n",
    "print(f\"   Total candidats: {total_count}\")\n",
    "print(f\"   Trait√©s: {processed_count} ({processed_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Accept√©s: {accepted_count} ({accepted_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Taux d'acceptation: {accepted_count/processed_count*100:.1f}%\" if processed_count > 0 else \"N/A\")\n",
    "\n",
    "logger.info(\"12 -> exports √©crits : %s | %s\",\n",
    "            (EXPORT_DIR / \"level3_results.parquet\"),\n",
    "            (EXPORT_DIR / \"level3_results.csv\"))\n",
    "\n",
    "print(f\"\\nüéØ PROBL√àMES R√âSOLUS:\")\n",
    "print(f\"   1. ‚úÖ Mapping textes: 100% disponible\") \n",
    "print(f\"   2. ‚úÖ Export complet: tous les candidats inclus\")\n",
    "print(f\"   3. ‚úÖ Seuils assouplies: overlap 0.07‚Üí0.01, topic 0.05‚Üí0.01\")\n",
    "\n",
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90bd5c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               summary_id         source_id             strategy  has_text  \\\n",
      "0   7_confidence_weighted  2e5d424af642656f  confidence_weighted      True   \n",
      "1   8_confidence_weighted  4545a3e73854234a  confidence_weighted      True   \n",
      "2  16_confidence_weighted  9cd674aa7b0d859c  confidence_weighted      True   \n",
      "\n",
      "   enough_length                                     summary_before  \\\n",
      "0           True  Partager Vous souhaitez Facebook Bluesky E-mai...   \n",
      "1           True  OMAR AL- Partager Vous souhaitez Facebook Blue...   \n",
      "2           True  En effet, nous r, bien qu'actuellement influen...   \n",
      "\n",
      "  tier_before  factuality_before  coherence_before  issues_before  ...  \\\n",
      "0    CRITICAL           0.891555          0.301675              6  ...   \n",
      "1    CRITICAL           0.844687          0.328654              6  ...   \n",
      "2    CRITICAL           0.888659          0.400962              6  ...   \n",
      "\n",
      "  prompt_version seed accepted  reason       l3_mode            mode_reason  \\\n",
      "0            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
      "1            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
      "2            NaN  NaN      NaN     NaN  re_summarize  cw_critical_with_text   \n",
      "\n",
      "  lang processing_status  runtime_ms notes  \n",
      "0   fr           unknown         NaN        \n",
      "1   fr           unknown         NaN        \n",
      "2   en           unknown         NaN        \n",
      "\n",
      "[3 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "print(final.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f08c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing columns: set()\n",
      "\n",
      "üìä ANALYSE DES R√âSULTATS:\n",
      "Total candidats: 81\n",
      "\n",
      "Statut de traitement:\n",
      "processing_status\n",
      "unknown     57\n",
      "rejected    24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pour les 24 cas trait√©s:\n",
      "Accepted: 0 (0.0%)\n",
      "Erreur analyse: Expected numeric dtype, got object instead.\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv(EXPORT_DIR/\"level3_results.csv\")\n",
    "needed = {\"summary_before\",\"summary_after\",\"l3_mode\",\"strategy\",\"accepted\",\"reason\"}\n",
    "print(\"missing columns:\", needed - set(final.columns))\n",
    "\n",
    "# CORRECTION: G√©rer les valeurs NaN dans accepted avant calcul\n",
    "print(\"\\nüìä ANALYSE DES R√âSULTATS:\")\n",
    "print(\"Total candidats:\", len(final))\n",
    "\n",
    "# Statistiques par processing_status\n",
    "if \"processing_status\" in final.columns:\n",
    "    print(\"\\nStatut de traitement:\")\n",
    "    print(final[\"processing_status\"].value_counts())\n",
    "\n",
    "# Statistiques pour les cas trait√©s seulement\n",
    "processed = final[final[\"accepted\"].notna()].copy()\n",
    "if len(processed) > 0:\n",
    "    print(f\"\\nPour les {len(processed)} cas trait√©s:\")\n",
    "    print(\"Accepted:\", processed[\"accepted\"].sum(), f\"({processed['accepted'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Analyse par strat√©gie et mode (safe)\n",
    "    try:\n",
    "        analysis = processed.groupby([\"strategy\",\"l3_mode\"])[\"accepted\"].agg(['count', 'sum']).reset_index()\n",
    "        analysis[\"acceptance_rate\"] = (analysis[\"sum\"] / analysis[\"count\"] * 100).round(1)\n",
    "        print(\"\\nTaux d'acceptation par strat√©gie/mode:\")\n",
    "        for _, row in analysis.iterrows():\n",
    "            print(f\"  {row['strategy']} + {row['l3_mode']}: {row['sum']}/{row['count']} ({row['acceptance_rate']}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur analyse: {e}\")\n",
    "else:\n",
    "    print(\"Aucun cas trait√© trouv√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6ff7b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_text",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "enough_length",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mode_reason",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bcf8a8da-0379-4628-97fb-d0aea566d1bf",
       "rows": [
        [
         "0",
         "7_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "1",
         "8_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "2",
         "16_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "3",
         "17_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "4",
         "22_confidence_weighted",
         "CRITICAL",
         "True",
         "False",
         "edit",
         "cw_critical_no_text"
        ],
        [
         "5",
         "29_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "6",
         "40_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "7",
         "53_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "8",
         "59_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ],
        [
         "9",
         "71_confidence_weighted",
         "CRITICAL",
         "True",
         "True",
         "re_summarize",
         "cw_critical_with_text"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>tier</th>\n",
       "      <th>has_text</th>\n",
       "      <th>enough_length</th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>mode_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>edit</td>\n",
       "      <td>cw_critical_no_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71_confidence_weighted</td>\n",
       "      <td>CRITICAL</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>re_summarize</td>\n",
       "      <td>cw_critical_with_text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               summary_id      tier  has_text  enough_length       l3_mode  \\\n",
       "0   7_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "1   8_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "2  16_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "3  17_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "4  22_confidence_weighted  CRITICAL      True          False          edit   \n",
       "5  29_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "6  40_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "7  53_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "8  59_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "9  71_confidence_weighted  CRITICAL      True           True  re_summarize   \n",
       "\n",
       "             mode_reason  \n",
       "0  cw_critical_with_text  \n",
       "1  cw_critical_with_text  \n",
       "2  cw_critical_with_text  \n",
       "3  cw_critical_with_text  \n",
       "4    cw_critical_no_text  \n",
       "5  cw_critical_with_text  \n",
       "6  cw_critical_with_text  \n",
       "7  cw_critical_with_text  \n",
       "8  cw_critical_with_text  \n",
       "9  cw_critical_with_text  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan = pd.read_csv(WORK_DIR / \"04_mode_plan.csv\")\n",
    "plan.query('strategy==\"confidence_weighted\"')[[\"summary_id\",\"tier\",\"has_text\",\"enough_length\",\"l3_mode\",\"mode_reason\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d1391e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL trait√©s: 81\n",
      "CRITICAL accept√©s: 0\n",
      "Empty DataFrame\n",
      "Columns: [summary_id, strategy, l3_mode, mode_reason, tier_before, tier_after, factuality_before, factuality_after, coherence_before, coherence_after, issues_before, issues_after, summary_before, summary_after]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "critical_done = final[(final[\"tier_before\"]==\"CRITICAL\")]\n",
    "print(\"CRITICAL trait√©s:\", len(critical_done))\n",
    "\n",
    "crit_accepted = critical_done[critical_done[\"accepted\"]==True]\n",
    "print(\"CRITICAL accept√©s:\", len(crit_accepted))\n",
    "\n",
    "# √©chantillon pour revue manuelle\n",
    "sample = crit_accepted.sample(min(5, len(crit_accepted)), random_state=42)[\n",
    "    [\"summary_id\",\"strategy\",\"l3_mode\",\"mode_reason\",\"tier_before\",\"tier_after\",\n",
    "     \"factuality_before\",\"factuality_after\",\"coherence_before\",\"coherence_after\",\n",
    "     \"issues_before\",\"issues_after\",\"summary_before\",\"summary_after\"]\n",
    "]\n",
    "sample.to_csv(REPORT_DIR / \"sample_manual_review.csv\", index=False)\n",
    "print(sample.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8ac83fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_id: nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "strategy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_text",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "enough_length",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "summary_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier_before",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "factuality_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_before",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_before",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_after",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hash_after",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tier_after",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "factuality_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coherence_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "issues_after",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_version",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accepted",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "l3_mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mode_reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lang",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processing_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "runtime_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "notes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "dca9c32f-8eb5-4be6-8e0c-46e305b261e4",
       "rows": [],
       "shape": {
        "columns": 27,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>strategy</th>\n",
       "      <th>has_text</th>\n",
       "      <th>enough_length</th>\n",
       "      <th>summary_before</th>\n",
       "      <th>tier_before</th>\n",
       "      <th>factuality_before</th>\n",
       "      <th>coherence_before</th>\n",
       "      <th>issues_before</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>seed</th>\n",
       "      <th>accepted</th>\n",
       "      <th>reason</th>\n",
       "      <th>l3_mode</th>\n",
       "      <th>mode_reason</th>\n",
       "      <th>lang</th>\n",
       "      <th>processing_status</th>\n",
       "      <th>runtime_ms</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [summary_id, source_id, strategy, has_text, enough_length, summary_before, tier_before, factuality_before, coherence_before, issues_before, summary_after, hash_after, tier_after, factuality_after, coherence_after, issues_after, model, prompt_version, seed, accepted, reason, l3_mode, mode_reason, lang, processing_status, runtime_ms, notes]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === AUDIT ACCEPT√âS ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "WORK_DIR     = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"level3\"\n",
    "EXPORT_DIR   = Path(PROJECT_ROOT) / \"outputs\" / \"level3\" / \"exports\"\n",
    "\n",
    "final = pd.read_csv(EXPORT_DIR / \"level3_results.csv\")\n",
    "ok    = final[final[\"accepted\"]==True].copy()\n",
    "\n",
    "base02 = pd.read_csv(WORK_DIR / \"02_join_articles.csv\") if (WORK_DIR / \"02_join_articles.csv\").exists() else pd.read_parquet(WORK_DIR / \"02_join_articles.parquet\")\n",
    "\n",
    "def peek(row):\n",
    "    print(\"summary_id:\", row[\"summary_id\"])\n",
    "    r = base02[base02[\"summary_id\"]==row[\"summary_id\"]].iloc[0]\n",
    "    print(\"  strategy  :\", row.get(\"strategy\"))\n",
    "    print(\"  title     :\", r.get(\"title\"))\n",
    "    print(\"  url       :\", r.get(\"url\"))\n",
    "    print(\"  lang(before) :\", row.get(\"lang\"))\n",
    "    print(\"  text[:220]:\", (r.get(\"text\") or \"\")[:220].replace(\"\\n\",\" \"))\n",
    "    print(\"  summary_after[:140]:\", (row.get(\"summary_after\") or \"\")[:140])\n",
    "    print(\"---\")\n",
    "\n",
    "ok.head(5).apply(peek, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
