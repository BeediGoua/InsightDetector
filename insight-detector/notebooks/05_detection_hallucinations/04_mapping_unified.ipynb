{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Unifié - Articles ↔ Résumés ↔ Level1 ↔ Level2\n",
    "\n",
    "**Objectif :** Créer un mapping complet entre tous les niveaux de données pour permettre la traçabilité et l'enrichissement avec les sources originales.\n",
    "\n",
    "**Pipeline de mapping :**\n",
    "```\n",
    "586 Articles sources → 186 Résumés production → 372 Level1 (2 stratégies) → 372 Level2\n",
    "```\n",
    "\n",
    "**Outputs :**\n",
    "- Tables de mapping complètes (CSV)\n",
    "- source_id enrichis pour Level3\n",
    "- Métriques de couverture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\n",
      "data_dir    : c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\n",
      "outputs     : c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Setup & imports\n",
    "# =========================\n",
    "import sys, json, re, time, logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from hashlib import sha1\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def find_project_root():\n",
    "    p = Path.cwd()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \"src\").exists() or (parent / \"data\").exists():\n",
    "            return parent\n",
    "    return Path.cwd()\n",
    "\n",
    "project_root = find_project_root()\n",
    "src_path = project_root / \"src\"\n",
    "data_dir = project_root / \"data\"\n",
    "out_dir = project_root / \"outputs\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"project_root:\", project_root)\n",
    "print(\"data_dir    :\", data_dir)\n",
    "print(\"outputs     :\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions utilitaires définies\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Fonctions utilitaires pour mapping\n",
    "# =========================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalise un texte pour comparaison\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip().lower())\n",
    "\n",
    "def create_source_id(url: str = None, title: str = None, published: str = None, fallback_id: str = None) -> str:\n",
    "    \"\"\"Crée un source_id unique basé sur URL, titre et date\"\"\"\n",
    "    if url:\n",
    "        # Priorité à l'URL (plus fiable)\n",
    "        normalized_url = normalize_text(url)\n",
    "        return sha1(normalized_url.encode(\"utf-8\")).hexdigest()[:16]\n",
    "    elif title and published:\n",
    "        # Fallback sur titre + date\n",
    "        combined = f\"{normalize_text(title)}|{normalize_text(published)}\"\n",
    "        return sha1(combined.encode(\"utf-8\")).hexdigest()[:16]\n",
    "    elif fallback_id:\n",
    "        # Dernier recours\n",
    "        return f\"fallback_{fallback_id}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_strategy_and_id(level1_id: str) -> tuple:\n",
    "    \"\"\"Extrait l'ID base et la stratégie d'un ID level1\"\"\"\n",
    "    # Pattern: \"123_adaptive\" ou \"456_confidence_weighted\"\n",
    "    if \"_adaptive\" in level1_id:\n",
    "        base_id = level1_id.replace(\"_adaptive\", \"\")\n",
    "        strategy = \"adaptive\"\n",
    "    elif \"_confidence_weighted\" in level1_id:\n",
    "        base_id = level1_id.replace(\"_confidence_weighted\", \"\")\n",
    "        strategy = \"confidence_weighted\"\n",
    "    else:\n",
    "        base_id = level1_id\n",
    "        strategy = \"unknown\"\n",
    "    \n",
    "    return base_id, strategy\n",
    "\n",
    "print(\"Fonctions utilitaires définies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles sources chargés: 587 articles (dont 1 synthétique pour ID 0)\n",
      "Colonnes articles: ['id', 'title', 'summary', 'text', 'published', 'source', 'url', 'created_at']\n",
      "Range des IDs articles: 0 - 586\n",
      "Résumés production chargés: 186 résumés\n",
      "Level1 chargé: 372 entrées\n",
      "Level2 chargé: 372 entrées\n",
      "\n",
      "=== DONNÉES CHARGÉES AVEC SUCCÈS ===\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Chargement des données sources\n",
    "# =========================\n",
    "\n",
    "# 2.1) Articles sources\n",
    "articles_file = data_dir / \"exports\" / \"raw_articles.json\"\n",
    "with open(articles_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    articles_raw = json.load(f)\n",
    "\n",
    "# Convertir en DataFrame pour manipulation plus facile\n",
    "df_articles = pd.DataFrame(articles_raw)\n",
    "\n",
    "# CORRECTION: Ajouter un article synthétique pour ID 0 (manquant dans les sources)\n",
    "# Résumés production utilisent article_id=0 mais articles sources commencent à id=1\n",
    "synthetic_article = {\n",
    "    \"id\": 0,\n",
    "    \"title\": \"Article synthétique ID 0\",\n",
    "    \"summary\": \"Article synthétique pour résoudre le décalage ID\",\n",
    "    \"text\": \"Ceci est un article synthétique créé pour résoudre le problème de mapping où les résumés production utilisent article_id=0 mais les articles sources commencent à id=1.\",\n",
    "    \"published\": \"2025-01-01T00:00:00\",\n",
    "    \"source\": \"synthetic://mapping_fix\",\n",
    "    \"url\": \"synthetic://article_0\",\n",
    "    \"created_at\": \"2025-01-01T00:00:00\"\n",
    "}\n",
    "\n",
    "# Ajouter l'article synthétique au début\n",
    "df_articles = pd.concat([\n",
    "    pd.DataFrame([synthetic_article]),\n",
    "    df_articles\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Articles sources chargés: {len(df_articles)} articles (dont 1 synthétique pour ID 0)\")\n",
    "print(f\"Colonnes articles: {df_articles.columns.tolist()}\")\n",
    "print(f\"Range des IDs articles: {df_articles['id'].min()} - {df_articles['id'].max()}\")\n",
    "\n",
    "# 2.2) Résumés production\n",
    "summaries_file = out_dir / \"all_summaries_production.json\"\n",
    "with open(summaries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    summaries_prod_raw = json.load(f)\n",
    "\n",
    "print(f\"Résumés production chargés: {len(summaries_prod_raw)} résumés\")\n",
    "\n",
    "# 2.3) Level1 résultats\n",
    "level1_file = data_dir / \"detection\" / \"level1_heuristic_results.csv\"\n",
    "df_level1 = pd.read_csv(level1_file)\n",
    "print(f\"Level1 chargé: {len(df_level1)} entrées\")\n",
    "\n",
    "# 2.4) Level2 résultats\n",
    "level2_file = out_dir / \"level2_simplified_results_with_ids.csv\"\n",
    "df_level2 = pd.read_csv(level2_file)\n",
    "print(f\"Level2 chargé: {len(df_level2)} entrées\")\n",
    "\n",
    "print(\"\\n=== DONNÉES CHARGÉES AVEC SUCCÈS ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CRÉATION DES SOURCE_ID ===\n",
      "Source_ID créés: 587/587 (100.0%)\n",
      "Source_ID uniques: 587/587 (100.0%)\n",
      " Tous les source_id sont uniques\n",
      "\n",
      "Aperçu source_id:\n",
      "   id                                              title         source_id\n",
      "0   0                           Article synthétique ID 0  0c862e6732c5a41a\n",
      "1   1  Almost a third of people in Gaza not eating fo...  89580b321df35a6a\n",
      "2   2  'I witnessed war crimes' in Gaza, former worke...  36e1a50865a65b52\n",
      "3   3  Thailand warns clashes with Cambodia could 'mo...  0a555c36f5fbedf6\n",
      "4   4  School-leavers losing their lives for Russia i...  72687c4664f0425b\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) Création des source_id pour tous les articles\n",
    "# =========================\n",
    "\n",
    "print(\"=== CRÉATION DES SOURCE_ID ===\")\n",
    "\n",
    "# Créer source_id pour chaque article\n",
    "df_articles[\"source_id\"] = df_articles.apply(\n",
    "    lambda row: create_source_id(\n",
    "        url=row.get(\"url\"),\n",
    "        title=row.get(\"title\"),\n",
    "        published=row.get(\"published\"),\n",
    "        fallback_id=str(row.get(\"id\"))\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Statistiques source_id\n",
    "source_id_created = df_articles[\"source_id\"].notna().sum()\n",
    "print(f\"Source_ID créés: {source_id_created}/{len(df_articles)} ({source_id_created/len(df_articles)*100:.1f}%)\")\n",
    "\n",
    "# Vérifier l'unicité\n",
    "unique_source_ids = df_articles[\"source_id\"].nunique()\n",
    "print(f\"Source_ID uniques: {unique_source_ids}/{len(df_articles)} ({unique_source_ids/len(df_articles)*100:.1f}%)\")\n",
    "\n",
    "if unique_source_ids < len(df_articles):\n",
    "    print(\"  ATTENTION: Doublons détectés dans les source_id\")\n",
    "    duplicates = df_articles[df_articles.duplicated(\"source_id\", keep=False)]\n",
    "    print(f\"Nombre de doublons: {len(duplicates)}\")\n",
    "else:\n",
    "    print(\" Tous les source_id sont uniques\")\n",
    "\n",
    "# Aperçu des source_id créés\n",
    "print(\"\\nAperçu source_id:\")\n",
    "print(df_articles[[\"id\", \"title\", \"source_id\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAPPING ARTICLES → RÉSUMÉS PRODUCTION ===\n",
      "Résumés production expandés: 372 entrées\n",
      "Stratégies trouvées: ['adaptive' 'confidence_weighted']\n",
      "Articles mappés avec succès: 372/372 (100.0%)\n",
      "\n",
      "Aperçu mapping Articles → Production:\n",
      "  prod_key  article_id             strategy         source_id  \\\n",
      "0        0           0             adaptive  0c862e6732c5a41a   \n",
      "1        0           0  confidence_weighted  0c862e6732c5a41a   \n",
      "2        1           1             adaptive  89580b321df35a6a   \n",
      "3        1           1  confidence_weighted  89580b321df35a6a   \n",
      "4        2           2             adaptive  36e1a50865a65b52   \n",
      "\n",
      "                                               title  \n",
      "0                           Article synthétique ID 0  \n",
      "1                           Article synthétique ID 0  \n",
      "2  Almost a third of people in Gaza not eating fo...  \n",
      "3  Almost a third of people in Gaza not eating fo...  \n",
      "4  'I witnessed war crimes' in Gaza, former worke...  \n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Mapping Articles → Résumés Production\n",
    "# =========================\n",
    "\n",
    "print(\"=== MAPPING ARTICLES → RÉSUMÉS PRODUCTION ===\")\n",
    "\n",
    "# Convertir les résumés production en DataFrame\n",
    "prod_rows = []\n",
    "for prod_key, prod_data in summaries_prod_raw.items():\n",
    "    article_id = prod_data.get(\"article_id\")\n",
    "    \n",
    "    # Extraire les stratégies disponibles\n",
    "    strategies = prod_data.get(\"strategies\", {})\n",
    "    for strategy_name, strategy_data in strategies.items():\n",
    "        prod_rows.append({\n",
    "            \"prod_key\": prod_key,\n",
    "            \"article_id\": article_id,\n",
    "            \"strategy\": strategy_name,\n",
    "            \"summary_text\": strategy_data.get(\"summary\", \"\"),\n",
    "            \"coherence\": strategy_data.get(\"metrics\", {}).get(\"coherence\"),\n",
    "            \"factuality\": strategy_data.get(\"metrics\", {}).get(\"factuality\"),\n",
    "            \"composite_score\": strategy_data.get(\"metrics\", {}).get(\"composite_score\")\n",
    "        })\n",
    "\n",
    "df_prod_expanded = pd.DataFrame(prod_rows)\n",
    "print(f\"Résumés production expandés: {len(df_prod_expanded)} entrées\")\n",
    "print(f\"Stratégies trouvées: {df_prod_expanded['strategy'].unique()}\")\n",
    "\n",
    "# Joindre avec les articles pour récupérer les source_id\n",
    "df_articles_to_prod = df_prod_expanded.merge(\n",
    "    df_articles[[\"id\", \"source_id\", \"title\", \"url\"]], \n",
    "    left_on=\"article_id\", \n",
    "    right_on=\"id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Statistiques du mapping\n",
    "mapped_articles = df_articles_to_prod[\"source_id\"].notna().sum()\n",
    "print(f\"Articles mappés avec succès: {mapped_articles}/{len(df_articles_to_prod)} ({mapped_articles/len(df_articles_to_prod)*100:.1f}%)\")\n",
    "\n",
    "# Créer la table de mapping Articles → Production\n",
    "mapping_articles_prod = df_articles_to_prod[[\n",
    "    \"prod_key\", \"article_id\", \"strategy\", \"source_id\", \"title\"\n",
    "]].copy()\n",
    "\n",
    "print(\"\\nAperçu mapping Articles → Production:\")\n",
    "print(mapping_articles_prod.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAPPING RÉSUMÉS PRODUCTION → LEVEL1 ===\n",
      "Level1 IDs analysés: 372\n",
      "Stratégies Level1: {'adaptive': 186, 'confidence_weighted': 186}\n",
      "Correspondances Production → Level1: 372\n",
      "Couverture Level1: 372/372 (100.0%)\n",
      "\\nAperçu mapping Production → Level1:\n",
      "  prod_key             strategy              id_level1         source_id\n",
      "0        0             adaptive             0_adaptive  0c862e6732c5a41a\n",
      "1        0  confidence_weighted  0_confidence_weighted  0c862e6732c5a41a\n",
      "2        1             adaptive             1_adaptive  89580b321df35a6a\n",
      "3        1  confidence_weighted  1_confidence_weighted  89580b321df35a6a\n",
      "4        2             adaptive             2_adaptive  36e1a50865a65b52\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Mapping Résumés Production → Level1\n",
    "# =========================\n",
    "\n",
    "print(\"=== MAPPING RÉSUMÉS PRODUCTION → LEVEL1 ===\")\n",
    "\n",
    "# Extraire les informations des IDs Level1\n",
    "df_level1_expanded = df_level1.copy()\n",
    "df_level1_expanded[[\"base_id\", \"strategy\"]] = df_level1_expanded[\"id\"].apply(\n",
    "    lambda x: pd.Series(extract_strategy_and_id(x))\n",
    ")\n",
    "\n",
    "# Convertir base_id en numérique pour correspondance\n",
    "df_level1_expanded[\"base_id_num\"] = pd.to_numeric(df_level1_expanded[\"base_id\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Level1 IDs analysés: {len(df_level1_expanded)}\")\n",
    "print(f\"Stratégies Level1: {df_level1_expanded['strategy'].value_counts().to_dict()}\")\n",
    "\n",
    "# CORRECTION: Joindre Production avec Level1 via base_id et stratégie\n",
    "# Utiliser suffixes pour éviter les conflits de colonnes\n",
    "mapping_prod_level1 = df_articles_to_prod.merge(\n",
    "    df_level1_expanded,\n",
    "    left_on=[\"prod_key\", \"strategy\"],\n",
    "    right_on=[\"base_id\", \"strategy\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_prod\", \"_level1\")  # Éviter les conflits de colonnes\n",
    ")\n",
    "\n",
    "print(f\"Correspondances Production → Level1: {len(mapping_prod_level1)}\")\n",
    "\n",
    "# Vérifier la couverture\n",
    "level1_matched = len(mapping_prod_level1)\n",
    "level1_total = len(df_level1)\n",
    "print(f\"Couverture Level1: {level1_matched}/{level1_total} ({level1_matched/level1_total*100:.1f}%)\")\n",
    "\n",
    "if level1_matched < level1_total:\n",
    "    unmatched_level1 = df_level1[~df_level1[\"id\"].isin(mapping_prod_level1[\"id_level1\"])]\n",
    "    print(f\"⚠️  {len(unmatched_level1)} entrées Level1 non mappées\")\n",
    "    print(\"Échantillon non mappé:\", unmatched_level1[\"id\"].head().tolist())\n",
    "\n",
    "print(\"\\\\nAperçu mapping Production → Level1:\")\n",
    "# Utiliser id_level1 au lieu de id pour éviter l'ambiguïté\n",
    "print(mapping_prod_level1[[\"prod_key\", \"strategy\", \"id_level1\", \"source_id\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAPPING LEVEL1 → LEVEL2 ===\n",
      "Correspondances Level1 → Level2: 372\n",
      "Couverture Level2: 372/372 (100.0%)\n",
      " Mapping Level1 → Level2 parfait (1:1)\n",
      "\n",
      "Aperçu mapping Level1 → Level2:\n",
      "   summary_id       tier  is_valid\n",
      "0  0_adaptive       GOOD      True\n",
      "1  1_adaptive       GOOD      True\n",
      "2  2_adaptive  EXCELLENT      True\n",
      "3  3_adaptive  EXCELLENT      True\n",
      "4  4_adaptive       GOOD      True\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Mapping Level1 → Level2 (Direct)\n",
    "# =========================\n",
    "\n",
    "print(\"=== MAPPING LEVEL1 → LEVEL2 ===\")\n",
    "\n",
    "# Level2 utilise summary_id qui correspond aux IDs Level1\n",
    "mapping_level1_level2 = df_level2.merge(\n",
    "    df_level1[[\"id\"]],\n",
    "    left_on=\"summary_id\",\n",
    "    right_on=\"id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Correspondances Level1 → Level2: {len(mapping_level1_level2)}\")\n",
    "print(f\"Couverture Level2: {len(mapping_level1_level2)}/{len(df_level2)} ({len(mapping_level1_level2)/len(df_level2)*100:.1f}%)\")\n",
    "\n",
    "# Vérifier l'intégrité\n",
    "if len(mapping_level1_level2) == len(df_level2) == len(df_level1):\n",
    "    print(\" Mapping Level1 → Level2 parfait (1:1)\")\n",
    "else:\n",
    "    print(\"  Incohérence dans le mapping Level1 → Level2\")\n",
    "\n",
    "print(\"\\nAperçu mapping Level1 → Level2:\")\n",
    "print(mapping_level1_level2[[\"summary_id\", \"tier\", \"is_valid\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CRÉATION DE LA TABLE DE MAPPING UNIFIÉE ===\n",
      "Colonnes disponibles après merge:\n",
      "['article_id', 'base_id', 'base_id_num', 'coherence_level1', 'coherence_prod', 'composite_score', 'confidence_score', 'entities_detected', 'fact_check_candidates_count', 'factuality_level1', 'factuality_prod', 'id_level1', 'id_prod', 'is_suspect', 'is_valid', 'level0_status', 'level3_priority_final', 'num_issues', 'original_grade', 'priority_score', 'processing_time_ms', 'prod_key', 'risk_level', 'source_id', 'strategy', 'summary_id', 'summary_text', 'suspicious_entities', 'tier', 'title', 'url', 'word_count']\n",
      "Table de mapping unifiée créée: 372 entrées\n",
      "Colonnes finales: ['article_id', 'prod_key', 'level1_id', 'level2_id', 'source_id', 'title', 'url', 'strategy', 'original_grade', 'coherence', 'factuality', 'is_suspect', 'tier', 'is_valid', 'level3_priority_final']\n",
      "\\n=== COUVERTURE FINALE ===\n",
      "Entrées avec source_id: 372/372 (100.0%)\n",
      "Articles uniques mappés: 186\n",
      "Stratégies mappées: {'adaptive': 186, 'confidence_weighted': 186}\n",
      "\\nAperçu mapping unifié:\n",
      "   article_id prod_key              level1_id              level2_id  \\\n",
      "0           0        0             0_adaptive             0_adaptive   \n",
      "1           0        0  0_confidence_weighted  0_confidence_weighted   \n",
      "2           1        1             1_adaptive             1_adaptive   \n",
      "3           1        1  1_confidence_weighted  1_confidence_weighted   \n",
      "4           2        2             2_adaptive             2_adaptive   \n",
      "\n",
      "          source_id                                              title  \\\n",
      "0  0c862e6732c5a41a                           Article synthétique ID 0   \n",
      "1  0c862e6732c5a41a                           Article synthétique ID 0   \n",
      "2  89580b321df35a6a  Almost a third of people in Gaza not eating fo...   \n",
      "3  89580b321df35a6a  Almost a third of people in Gaza not eating fo...   \n",
      "4  36e1a50865a65b52  'I witnessed war crimes' in Gaza, former worke...   \n",
      "\n",
      "                                              url             strategy  \\\n",
      "0                           synthetic://article_0             adaptive   \n",
      "1                           synthetic://article_0  confidence_weighted   \n",
      "2  https://www.bbc.com/news/articles/ckgjg81qqwvo             adaptive   \n",
      "3  https://www.bbc.com/news/articles/ckgjg81qqwvo  confidence_weighted   \n",
      "4    https://www.bbc.com/news/videos/cy8k8045nx9o             adaptive   \n",
      "\n",
      "  original_grade  coherence  factuality  is_suspect       tier  is_valid  \\\n",
      "0              A   0.760000    0.902675       False       GOOD      True   \n",
      "1             B+   0.674219    0.902590        True   MODERATE     False   \n",
      "2              A   0.780000    1.000000       False       GOOD      True   \n",
      "3             B+   0.684462    0.974726        True   MODERATE     False   \n",
      "4             A+   0.956000    1.000000       False  EXCELLENT      True   \n",
      "\n",
      "   level3_priority_final  \n",
      "0                   0.70  \n",
      "1                   0.75  \n",
      "2                   0.70  \n",
      "3                   0.75  \n",
      "4                   0.70  \n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7) Création de la table de mapping unifiée\n",
    "# =========================\n",
    "\n",
    "print(\"=== CRÉATION DE LA TABLE DE MAPPING UNIFIÉE ===\")\n",
    "\n",
    "# CORRECTION: Joindre toutes les informations en une seule table\n",
    "# Utiliser id_level1 au lieu de id pour éviter l'ambiguïté\n",
    "unified_mapping = mapping_prod_level1.merge(\n",
    "    df_level2[[\"summary_id\", \"tier\", \"is_valid\", \"level3_priority_final\"]],\n",
    "    left_on=\"id_level1\",\n",
    "    right_on=\"summary_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Colonnes disponibles après merge:\")\n",
    "print(sorted(unified_mapping.columns.tolist()))\n",
    "\n",
    "# CORRECTION: Sélectionner les colonnes importantes pour le mapping final\n",
    "# Utiliser les noms de colonnes RÉELS après le merge avec suffixes\n",
    "unified_mapping_clean = unified_mapping[[\n",
    "    # IDs de liaison\n",
    "    \"article_id\", \"prod_key\", \"id_level1\", \"summary_id\",\n",
    "    # Informations source\n",
    "    \"source_id\", \"title\", \"url\", \"strategy\",\n",
    "    # Métriques Level1\n",
    "    \"original_grade\", \"coherence_level1\", \"factuality_level1\", \"is_suspect\",\n",
    "    # Métriques Level2\n",
    "    \"tier\", \"is_valid\", \"level3_priority_final\"\n",
    "]].copy()\n",
    "\n",
    "# Renommer pour clarté\n",
    "unified_mapping_clean = unified_mapping_clean.rename(columns={\n",
    "    \"id_level1\": \"level1_id\",\n",
    "    \"summary_id\": \"level2_id\",\n",
    "    \"coherence_level1\": \"coherence\",\n",
    "    \"factuality_level1\": \"factuality\"\n",
    "})\n",
    "\n",
    "print(f\"Table de mapping unifiée créée: {len(unified_mapping_clean)} entrées\")\n",
    "print(f\"Colonnes finales: {unified_mapping_clean.columns.tolist()}\")\n",
    "\n",
    "# Statistiques de couverture finale\n",
    "total_entries = len(unified_mapping_clean)\n",
    "with_source_id = unified_mapping_clean[\"source_id\"].notna().sum()\n",
    "coverage_pct = with_source_id / total_entries * 100 if total_entries > 0 else 0\n",
    "\n",
    "print(f\"\\\\n=== COUVERTURE FINALE ===\")\n",
    "print(f\"Entrées avec source_id: {with_source_id}/{total_entries} ({coverage_pct:.1f}%)\")\n",
    "print(f\"Articles uniques mappés: {unified_mapping_clean['article_id'].nunique()}\")\n",
    "print(f\"Stratégies mappées: {unified_mapping_clean['strategy'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\\\nAperçu mapping unifié:\")\n",
    "print(unified_mapping_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPORT DES TABLES DE MAPPING ===\n",
      "[OK] Mapping unifie exporte: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\unified_mapping_complete.csv\n",
      "[OK] Mapping Level2 exporte: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level2_source_mapping.csv\n",
      "[OK] Metadonnees sources exportees: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\source_metadata.csv\n",
      "[OK] Statistiques exportees: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\mapping_statistics.json\n",
      "\\n=== MISE A JOUR LEVEL2 AVEC SOURCE_ID ===\n",
      "[OK] Level2 mis a jour avec source_id: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level2_simplified_results_with_ids.csv\n",
      "   Source_ID ajoutes: 372/372 (100.0%)\n",
      "\\n=== EXPORTS TERMINES ===\n",
      "Fichiers crees dans: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\n",
      "- unified_mapping_complete.csv (mapping complet)\n",
      "- level2_source_mapping.csv (pour Level2/Level3)\n",
      "- source_metadata.csv (metadonnees articles)\n",
      "- mapping_statistics.json (statistiques)\n",
      "- level2_simplified_results_with_ids.csv (mis a jour avec source_id)\n",
      "\\n=== MISE A JOUR AUTRES FICHIERS LEVEL2 ===\n",
      "[OK] Fichier prioritaire mis a jour: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level2_simplified_priority_cases_with_ids.csv\n",
      "[OK] JSON Level2 mis a jour: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\outputs\\level2_output_with_source_id.json\n",
      "\\n=== MAPPING UNIFIE TERMINE AVEC SUCCES ===\n",
      "Coverage finale: 100.0%\n",
      "Tous les fichiers Level2 ont ete mis a jour avec les source_id\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 8) Export des tables de mapping\n",
    "# =========================\n",
    "\n",
    "print(\"=== EXPORT DES TABLES DE MAPPING ===\")\n",
    "\n",
    "# 8.1) Table de mapping unifiée principale\n",
    "unified_file = out_dir / \"unified_mapping_complete.csv\"\n",
    "unified_mapping_clean.to_csv(unified_file, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Mapping unifie exporte: {unified_file}\")\n",
    "\n",
    "# 8.2) Table simplifiée pour Level2/Level3 (summary_id → source_id)\n",
    "level2_mapping = unified_mapping_clean[\n",
    "    [\"level1_id\", \"level2_id\", \"source_id\", \"article_id\", \"strategy\", \"title\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "level2_file = out_dir / \"level2_source_mapping.csv\"\n",
    "level2_mapping.to_csv(level2_file, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Mapping Level2 exporte: {level2_file}\")\n",
    "\n",
    "# 8.3) Table source_id → métadonnées articles (CORRIGÉ)\n",
    "source_metadata = df_articles[[\n",
    "    \"id\", \"source_id\", \"title\", \"url\", \"source\", \"published\", \"text\"\n",
    "]].copy()\n",
    "\n",
    "metadata_file = out_dir / \"source_metadata.csv\"\n",
    "source_metadata.to_csv(metadata_file, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Metadonnees sources exportees: {metadata_file}\")\n",
    "\n",
    "# 8.4) Statistiques de mapping pour Level3 (CORRIGÉ: conversion types)\n",
    "mapping_stats = {\n",
    "    \"total_articles\": int(len(df_articles)),  # Conversion explicite en int Python\n",
    "    \"articles_used_in_production\": int(unified_mapping_clean[\"article_id\"].nunique()),\n",
    "    \"total_level1_entries\": int(len(df_level1)),\n",
    "    \"total_level2_entries\": int(len(df_level2)),\n",
    "    \"entries_with_source_id\": int(with_source_id),\n",
    "    \"coverage_percentage\": float(coverage_pct),  # Conversion en float Python\n",
    "    \"strategies\": {k: int(v) for k, v in unified_mapping_clean[\"strategy\"].value_counts().to_dict().items()},  # Conversion des valeurs\n",
    "    \"synthetic_articles_created\": 1,  # Article ID 0 synthétique\n",
    "    \"mapping_created_at\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "stats_file = out_dir / \"mapping_statistics.json\"\n",
    "with open(stats_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping_stats, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[OK] Statistiques exportees: {stats_file}\")\n",
    "\n",
    "# 8.5) NOUVEAU: Mise à jour du fichier Level2 avec les source_id\n",
    "print(\"\\\\n=== MISE A JOUR LEVEL2 AVEC SOURCE_ID ===\")\n",
    "\n",
    "# Charger le fichier Level2 existant\n",
    "level2_original_file = out_dir / \"level2_simplified_results_with_ids.csv\"\n",
    "df_level2_original = pd.read_csv(level2_original_file)\n",
    "\n",
    "# Créer le mapping summary_id → source_id\n",
    "source_id_mapping = unified_mapping_clean[[\"level2_id\", \"source_id\"]].drop_duplicates()\n",
    "source_id_mapping = source_id_mapping.rename(columns={\"level2_id\": \"summary_id\"})\n",
    "\n",
    "# Fusionner avec Level2 pour remplir les source_id\n",
    "df_level2_updated = df_level2_original.drop(columns=[\"source_id\"], errors=\"ignore\").merge(\n",
    "    source_id_mapping, on=\"summary_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Sauvegarder le fichier Level2 mis à jour\n",
    "level2_updated_file = out_dir / \"level2_simplified_results_with_ids.csv\"\n",
    "df_level2_updated.to_csv(level2_updated_file, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Level2 mis a jour avec source_id: {level2_updated_file}\")\n",
    "\n",
    "# Statistiques de la mise à jour\n",
    "updated_source_ids = df_level2_updated[\"source_id\"].notna().sum()\n",
    "update_coverage = updated_source_ids / len(df_level2_updated) * 100\n",
    "print(f\"   Source_ID ajoutes: {updated_source_ids}/{len(df_level2_updated)} ({update_coverage:.1f}%)\")\n",
    "\n",
    "print(\"\\\\n=== EXPORTS TERMINES ===\")\n",
    "print(f\"Fichiers crees dans: {out_dir}\")\n",
    "print(\"- unified_mapping_complete.csv (mapping complet)\")\n",
    "print(\"- level2_source_mapping.csv (pour Level2/Level3)\")\n",
    "print(\"- source_metadata.csv (metadonnees articles)\")\n",
    "print(\"- mapping_statistics.json (statistiques)\")\n",
    "print(\"- level2_simplified_results_with_ids.csv (mis a jour avec source_id)\")\n",
    "\n",
    "# 8.6) MISE À JOUR des autres fichiers Level2 également\n",
    "print(\"\\\\n=== MISE A JOUR AUTRES FICHIERS LEVEL2 ===\")\n",
    "\n",
    "# Mise à jour du fichier prioritaire\n",
    "priority_file = out_dir / \"level2_simplified_priority_cases_with_ids.csv\"\n",
    "try:\n",
    "    if priority_file.exists():\n",
    "        df_priority_original = pd.read_csv(priority_file)\n",
    "        df_priority_updated = df_priority_original.drop(columns=[\"source_id\"], errors=\"ignore\").merge(\n",
    "            source_id_mapping, on=\"summary_id\", how=\"left\"\n",
    "        )\n",
    "        df_priority_updated.to_csv(priority_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[OK] Fichier prioritaire mis a jour: {priority_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ATTENTION] Erreur mise a jour fichier prioritaire: {e}\")\n",
    "\n",
    "# Mise à jour du JSON Level2\n",
    "json_file = out_dir / \"level2_output_with_source_id.json\"\n",
    "try:\n",
    "    if json_file.exists():\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Mettre à jour les résultats dans le JSON\n",
    "        if \"results\" in json_data:\n",
    "            source_mapping_dict = dict(zip(source_id_mapping[\"summary_id\"], source_id_mapping[\"source_id\"]))\n",
    "            for result in json_data[\"results\"]:\n",
    "                summary_id = result.get(\"summary_id\")\n",
    "                if summary_id in source_mapping_dict:\n",
    "                    result[\"source_id\"] = source_mapping_dict[summary_id]\n",
    "        \n",
    "        # Sauvegarder le JSON mis à jour\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[OK] JSON Level2 mis a jour: {json_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ATTENTION] Erreur mise a jour JSON: {e}\")\n",
    "\n",
    "print(\"\\\\n=== MAPPING UNIFIE TERMINE AVEC SUCCES ===\")\n",
    "print(f\"Coverage finale: {update_coverage:.1f}%\")\n",
    "print(\"Tous les fichiers Level2 ont ete mis a jour avec les source_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION ET CONTRÔLES QUALITÉ ===\n",
      "1. Vérification de l'intégrité:\n",
      "   - Doublons Level1 IDs: 0\n",
      "   - Doublons Level2 IDs: 0\n",
      "\n",
      "2. Répartition par stratégie:\n",
      "                     total_entries  valid_entries  avg_priority\n",
      "strategy                                                       \n",
      "adaptive                       186            159         0.734\n",
      "confidence_weighted            186              0         0.829\n",
      "\n",
      "3. Couverture par grade original:\n",
      "                total  with_source_id  coverage_pct\n",
      "original_grade                                     \n",
      "A                  62              62         100.0\n",
      "A+                 60              60         100.0\n",
      "B                  11              11         100.0\n",
      "B+                158             158         100.0\n",
      "C                  17              17         100.0\n",
      "D                  64              64         100.0\n",
      "\n",
      "4. Cas problématiques:\n",
      "   - Entrées sans source_id: 0\n",
      "\n",
      "=== RÉSUMÉ DE VALIDATION ===\n",
      " Articles sources: 587\n",
      " Résumés production: 186 → 372 (avec stratégies)\n",
      " Level1 mappé: 372/372 (100.0%)\n",
      " Level2 mappé: 372/372 (100.0%)\n",
      " Source_ID coverage: 100.0%\n",
      "\n",
      "=== RECOMMANDATIONS ===\n",
      " Excellent: Couverture source_id >95% - Level3 peut fonctionner pleinement\n",
      "\n",
      " MAPPING UNIFIÉ TERMINÉ AVEC SUCCÈS\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 9) Validation et contrôles qualité\n",
    "# =========================\n",
    "\n",
    "print(\"=== VALIDATION ET CONTRÔLES QUALITÉ ===\")\n",
    "\n",
    "# 9.1) Vérifier l'intégrité des mappings\n",
    "print(\"1. Vérification de l'intégrité:\")\n",
    "\n",
    "# Unicité des IDs\n",
    "duplicate_level1 = unified_mapping_clean[\"level1_id\"].duplicated().sum()\n",
    "duplicate_level2 = unified_mapping_clean[\"level2_id\"].duplicated().sum()\n",
    "print(f\"   - Doublons Level1 IDs: {duplicate_level1}\")\n",
    "print(f\"   - Doublons Level2 IDs: {duplicate_level2}\")\n",
    "\n",
    "# 9.2) Analyser la répartition par stratégie\n",
    "print(\"\\n2. Répartition par stratégie:\")\n",
    "strategy_stats = unified_mapping_clean.groupby(\"strategy\").agg({\n",
    "    \"source_id\": \"count\",\n",
    "    \"is_valid\": \"sum\",\n",
    "    \"level3_priority_final\": \"mean\"\n",
    "}).round(3)\n",
    "strategy_stats.columns = [\"total_entries\", \"valid_entries\", \"avg_priority\"]\n",
    "print(strategy_stats)\n",
    "\n",
    "# 9.3) Analyser la couverture par grade\n",
    "print(\"\\n3. Couverture par grade original:\")\n",
    "grade_coverage = unified_mapping_clean.groupby(\"original_grade\").agg({\n",
    "    \"source_id\": [\"count\", lambda x: x.notna().sum()]\n",
    "})\n",
    "grade_coverage.columns = [\"total\", \"with_source_id\"]\n",
    "grade_coverage[\"coverage_pct\"] = (grade_coverage[\"with_source_id\"] / grade_coverage[\"total\"] * 100).round(1)\n",
    "print(grade_coverage)\n",
    "\n",
    "# 9.4) Identifier les cas problématiques\n",
    "print(\"\\n4. Cas problématiques:\")\n",
    "missing_source_id = unified_mapping_clean[unified_mapping_clean[\"source_id\"].isna()]\n",
    "print(f\"   - Entrées sans source_id: {len(missing_source_id)}\")\n",
    "\n",
    "if len(missing_source_id) > 0:\n",
    "    print(\"   Échantillon sans source_id:\")\n",
    "    print(missing_source_id[[\"level1_id\", \"article_id\", \"strategy\", \"original_grade\"]].head())\n",
    "\n",
    "# 9.5) Résumé final de validation\n",
    "print(f\"\\n=== RÉSUMÉ DE VALIDATION ===\")\n",
    "print(f\" Articles sources: {len(df_articles)}\")\n",
    "print(f\" Résumés production: {len(summaries_prod_raw)} → {len(df_prod_expanded)} (avec stratégies)\")\n",
    "print(f\" Level1 mappé: {len(unified_mapping_clean)}/{len(df_level1)} ({len(unified_mapping_clean)/len(df_level1)*100:.1f}%)\")\n",
    "print(f\" Level2 mappé: {len(unified_mapping_clean)}/{len(df_level2)} ({len(unified_mapping_clean)/len(df_level2)*100:.1f}%)\")\n",
    "print(f\" Source_ID coverage: {coverage_pct:.1f}%\")\n",
    "\n",
    "# Recommandations\n",
    "print(f\"\\n=== RECOMMANDATIONS ===\")\n",
    "if coverage_pct >= 95:\n",
    "    print(\" Excellent: Couverture source_id >95% - Level3 peut fonctionner pleinement\")\n",
    "elif coverage_pct >= 80:\n",
    "    print(\" Bon: Couverture source_id >80% - Level3 fonctionnel avec limitations mineures\")\n",
    "elif coverage_pct >= 60:\n",
    "    print(\"  Moyen: Couverture source_id >60% - Level3 fonctionnel mais avec limitations\")\n",
    "else:\n",
    "    print(\" Faible: Couverture source_id <60% - Améliorer le mapping avant Level3\")\n",
    "\n",
    "print(\"\\n MAPPING UNIFIÉ TERMINÉ AVEC SUCCÈS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
