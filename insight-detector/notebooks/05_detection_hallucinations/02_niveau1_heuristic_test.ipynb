{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Niveau 1 - Détection Heuristique Enrichie\n",
    "\n",
    "**Objectif principal :** Enrichir l'information de tous les résumés pour alimenter les niveaux suivants (pas de rejet définitif)\n",
    "\n",
    "## Analyses intégrées (8 types) :\n",
    "1. Anomalies statistiques (longueur, ponctuation, diversité lexicale)\n",
    "2. Complexité syntaxique (structure phrases, connecteurs logiques)\n",
    "3. Répétitions agressives (patterns répétitifs suspects)\n",
    "4. Densité d'entités (distribution éléments nommés)\n",
    "5. Incohérences temporelles (anachronismes, contradictions chronologiques)\n",
    "6. Validation des entités (NER + bases externes)\n",
    "7. Relations causales suspectes (plausibilité liens cause-effet)\n",
    "8. Intégration métriques existantes (coherence/factuality scores, grades)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire projet: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\n",
      "Répertoire src: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\src\n",
      "NIVEAU 0 ENHANCED importé\n",
      "NIVEAU 1 ENHANCED importé (analyseur amélioré avec aliases)\n",
      "\n",
      "Modules importés avec succès\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des chemins\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "print(f\"Répertoire projet: {project_root}\")\n",
    "print(f\"Répertoire src: {src_path}\")\n",
    "\n",
    "# Import des modules de détection ENHANCED avec fallback compatibilité\n",
    "try:\n",
    "    # Version enhanced avec patterns corrigés et seuils calibrés\n",
    "    from detection.level0_prefilter_enhanced import EnhancedQualityFilter as QualityFilter\n",
    "    print(\"NIVEAU 0 ENHANCED importé\")\n",
    "except ImportError:\n",
    "    from detection.level0_prefilter import QualityFilter\n",
    "    print(\"Niveau 0 original importé\")\n",
    "\n",
    "try:\n",
    "    # Version enhanced avec seuils corrigés et détection confidence_weighted\n",
    "    from detection.level1_heuristic import HeuristicAnalyzer as Level1HeuristicDetector\n",
    "    \n",
    "    # Alias de compatibilité pour les fonctions rapides\n",
    "    def quick_heuristic_check(text, metadata=None):\n",
    "        analyzer = Level1HeuristicDetector(enable_wikidata=False)\n",
    "        result = analyzer.analyze_summary(text, metadata or {})\n",
    "        return result.is_suspect, result.issues\n",
    "    \n",
    "    # Patch pour compatibilité: ajouter les méthodes manquantes\n",
    "    original_class = Level1HeuristicDetector\n",
    "    class CompatLevel1HeuristicDetector(original_class):\n",
    "        def process_batch(self, summaries, enable_progress=True):\n",
    "            \"\"\"Alias de compatibilité pour analyze_batch\"\"\"\n",
    "            return self.analyze_batch(summaries, enable_progress)\n",
    "            \n",
    "        def detect_hallucinations(self, text, metadata=None):\n",
    "            \"\"\"Alias de compatibilité pour analyze_summary\"\"\"\n",
    "            result = self.analyze_summary(text, metadata or {})\n",
    "            # Convertir vers format attendu par le notebook\n",
    "            return {\n",
    "                'is_hallucination': result.is_suspect,\n",
    "                'confidence_score': result.confidence_score,\n",
    "                'issues': result.issues,\n",
    "                'word_count': result.word_count,\n",
    "                'processing_time_ms': result.processing_time_ms,\n",
    "                'risk_level': result.risk_level,\n",
    "                'corrections_suggested': result.corrections_suggested\n",
    "            }\n",
    "    \n",
    "    Level1HeuristicDetector = CompatLevel1HeuristicDetector\n",
    "    \n",
    "    print(\"NIVEAU 1 ENHANCED importé (analyseur amélioré avec aliases)\")\n",
    "except ImportError:\n",
    "    from detection.level1_heuristic_original import Level1HeuristicDetector, quick_heuristic_check\n",
    "    print(\"Niveau 1 original importé\")\n",
    "\n",
    "print(\"\\nModules importés avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Chargement des Données Complètes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données originales: 372 résumés\n",
      "\n",
      "Distribution des grades (données complètes):\n",
      "  Grade A: 62 résumés (16.7%)\n",
      "  Grade A+: 60 résumés (16.1%)\n",
      "  Grade B: 11 résumés (3.0%)\n",
      "  Grade B+: 158 résumés (42.5%)\n",
      "  Grade C: 17 résumés (4.6%)\n",
      "  Grade D: 64 résumés (17.2%)\n",
      "\n",
      "Analyse Niveau 0:\n",
      "  Résumés validés par Niveau 0: 189\n",
      "  Résumés rejetés par Niveau 0: 183\n"
     ]
    }
   ],
   "source": [
    "# Charger les 372 résumés originaux\n",
    "data_path = os.path.join(project_root, 'data', 'results', 'batch_summary_production.csv')\n",
    "df_original = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Données originales: {len(df_original)} résumés\")\n",
    "\n",
    "# Analyse de la distribution initiale\n",
    "initial_grades = Counter(df_original['quality_grade'])\n",
    "print(f\"\\nDistribution des grades (données complètes):\")\n",
    "for grade, count in sorted(initial_grades.items()):\n",
    "    print(f\"  Grade {grade}: {count} résumés ({count/len(df_original)*100:.1f}%)\")\n",
    "\n",
    "# Charger les résultats du Niveau 0 pour identifier les 2 populations\n",
    "level0_results_path = os.path.join(project_root, 'data', 'detection', 'level0_filter_results.csv')\n",
    "if os.path.exists(level0_results_path):\n",
    "    df_level0 = pd.read_csv(level0_results_path)\n",
    "    valid_level0_ids = df_level0[df_level0['filter_valid'] == True]['id'].tolist()\n",
    "    \n",
    "    print(f\"\\nAnalyse Niveau 0:\")\n",
    "    print(f\"  Résumés validés par Niveau 0: {len(valid_level0_ids)}\")\n",
    "    print(f\"  Résumés rejetés par Niveau 0: {len(df_original) - len(valid_level0_ids)}\")\n",
    "else:\n",
    "    print(\"\\nRésultats Niveau 0 non trouvés, traitement de tous les résumés\")\n",
    "    valid_level0_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Préparation des Données pour Analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Données préparées pour analyse Niveau 1:\n",
      "  Total: 372 résumés\n",
      "  Validés par Niveau 0: 189 résumés\n",
      "  Rejetés par Niveau 0: 183 résumés\n"
     ]
    }
   ],
   "source": [
    "# Préparer TOUS les résumés (319 validés + 53 rejetés par Niveau 0)\n",
    "all_summaries_data = []\n",
    "level0_validated_data = []\n",
    "level0_rejected_data = []\n",
    "\n",
    "for idx, row in df_original.iterrows():\n",
    "    summary_id = f\"{row['text_id']}_{row['fusion_strategy']}\"\n",
    "    \n",
    "    summary_dict = {\n",
    "        'id': summary_id,\n",
    "        'text': row['summary'],\n",
    "        'original_length': row['length'],\n",
    "        'quality_grade': row['quality_grade'],\n",
    "        'coherence': row['coherence'],\n",
    "        'factuality': row['factuality'],\n",
    "        'composite_score': row['composite_score'],\n",
    "        'level0_status': 'validated' if (valid_level0_ids is None or summary_id in valid_level0_ids) else 'rejected'\n",
    "    }\n",
    "    \n",
    "    all_summaries_data.append(summary_dict)\n",
    "    \n",
    "    # Séparer en 2 populations pour analyse\n",
    "    if valid_level0_ids is None or summary_id in valid_level0_ids:\n",
    "        level0_validated_data.append(summary_dict)\n",
    "    else:\n",
    "        level0_rejected_data.append(summary_dict)\n",
    "\n",
    "print(f\"\\nDonnées préparées pour analyse Niveau 1:\")\n",
    "print(f\"  Total: {len(all_summaries_data)} résumés\")\n",
    "print(f\"  Validés par Niveau 0: {len(level0_validated_data)} résumés\")\n",
    "print(f\"  Rejetés par Niveau 0: {len(level0_rejected_data)} résumés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution par population:\n",
      "  Population validated:\n",
      "    Grade A: 62 résumés\n",
      "    Grade A+: 58 résumés\n",
      "    Grade B: 7 résumés\n",
      "    Grade B+: 39 résumés\n",
      "    Grade D: 23 résumés\n",
      "  Population rejected:\n",
      "    Grade A+: 2 résumés\n",
      "    Grade B: 4 résumés\n",
      "    Grade B+: 119 résumés\n",
      "    Grade C: 17 résumés\n",
      "    Grade D: 41 résumés\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Distribution par statut Niveau 0\n",
    "print(f\"\\nDistribution par population:\")\n",
    "for status in ['validated', 'rejected']:\n",
    "    subset = [s for s in all_summaries_data if s['level0_status'] == status]\n",
    "    if subset:\n",
    "        status_grades = Counter([s['quality_grade'] for s in subset])\n",
    "        print(f\"  Population {status}:\")\n",
    "        for grade, count in sorted(status_grades.items()):\n",
    "            print(f\"    Grade {grade}: {count} résumés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé testé: 10_adaptive (Grade: A+)\n",
      "   Statut Niveau 0: validated\n",
      "   Extrait: , Un portrait défiguré de l’ex président syrien Bachar al-Assad dans un centre de sécurité gouvernemental saccagé, à Damas, le, le jour du renversemen...\n"
     ]
    }
   ],
   "source": [
    "# Prendre un exemple représentatif (Grade A+)\n",
    "test_summary = level0_validated_data[10] if len(level0_validated_data) > 10 else all_summaries_data[10]\n",
    "detector_enriched = Level1HeuristicDetector(enable_wikidata=False, enable_entity_validation=True)\n",
    "\n",
    "# Préparer les métadonnées\n",
    "metadata = {\n",
    "    'id': test_summary['id'],\n",
    "    'coherence_score': test_summary['coherence'],\n",
    "    'factuality_score': test_summary['factuality'],\n",
    "    'quality_grade': test_summary['quality_grade']\n",
    "}\n",
    "\n",
    "result_enriched = detector_enriched.analyze_summary(test_summary['text'], metadata)\n",
    "\n",
    "print(f\"Résumé testé: {test_summary['id']} (Grade: {test_summary['quality_grade']})\")\n",
    "print(f\"   Statut Niveau 0: {test_summary['level0_status']}\")\n",
    "print(f\"   Extrait: {test_summary['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation globale:\n",
      "   Statut: VALIDE\n",
      "   Confiance: 0.700\n",
      "   Niveau de risque: low\n",
      "   Issues détectées: 1\n",
      "   Temps de traitement: 82.1ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Évaluation globale:\")\n",
    "print(f\"   Statut: {'SUSPECT' if result_enriched.is_suspect else 'VALIDE'}\")\n",
    "print(f\"   Confiance: {result_enriched.confidence_score:.3f}\")\n",
    "print(f\"   Niveau de risque: {result_enriched.risk_level}\")\n",
    "print(f\"   Issues détectées: {len(result_enriched.issues)}\")\n",
    "print(f\"   Temps de traitement: {result_enriched.processing_time_ms:.1f}ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROFIL STATISTIQUE:\n",
      "   Nombre de mots: 27\n",
      "   Entités détectées: 3\n",
      "   Entités suspectes: 0\n",
      "   Candidats fact-check: 1\n",
      "   Score priorité: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Need to restart the kernel and reimport to get the fixed methods\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload the module\n",
    "if 'detection.level1_heuristic' in sys.modules:\n",
    "    importlib.reload(sys.modules['detection.level1_heuristic'])\n",
    "\n",
    "from detection.level1_heuristic import HeuristicAnalyzer\n",
    "\n",
    "# Re-run the test\n",
    "test_summary = level0_validated_data[10] if len(level0_validated_data) > 10 else all_summaries_data[10]\n",
    "detector_enriched = HeuristicAnalyzer(enable_wikidata=False, enable_entity_validation=True)\n",
    "\n",
    "# Préparer les métadonnées\n",
    "metadata = {\n",
    "    'id': test_summary['id'],\n",
    "    'coherence_score': test_summary['coherence'],\n",
    "    'factuality_score': test_summary['factuality'],\n",
    "    'quality_grade': test_summary['quality_grade']\n",
    "}\n",
    "\n",
    "result_enriched = detector_enriched.analyze_summary(test_summary['text'], metadata)\n",
    "\n",
    "print(f\"PROFIL STATISTIQUE:\")\n",
    "# Vérifier les propriétés disponibles\n",
    "print(f\"   Nombre de mots: {result_enriched.word_count}\")\n",
    "print(f\"   Entités détectées: {result_enriched.entities_detected}\")\n",
    "print(f\"   Entités suspectes: {result_enriched.suspicious_entities}\")\n",
    "print(f\"   Candidats fact-check: {len(result_enriched.fact_check_candidates)}\")\n",
    "print(f\"   Score priorité: {result_enriched.priority_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTION D'ENTITÉS:\n",
      "   Total entités détectées: 3\n",
      "   Entités suspectes: 0\n",
      "   (Détails d'entités non disponibles dans enrichment_metadata)\n",
      "   Clés disponibles: ['word_count', 'sentence_count', 'strategy', 'has_corruption', 'repetition_ratio', 'encoding_quality', 'entity_density', 'fact_check_density']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nEXTRACTION D'ENTITÉS:\")\n",
    "print(f\"   Total entités détectées: {result_enriched.entities_detected}\")\n",
    "print(f\"   Entités suspectes: {result_enriched.suspicious_entities}\")\n",
    "\n",
    "# Vérifier si les détails sont dans enrichment_metadata\n",
    "if 'entity_extraction' in result_enriched.enrichment_metadata:\n",
    "    entities = result_enriched.enrichment_metadata['entity_extraction']\n",
    "    print(f\"   Détails disponibles dans enrichment_metadata:\")\n",
    "    \n",
    "    if 'persons' in entities:\n",
    "        persons = entities['persons'][:3] if isinstance(entities['persons'], list) else []\n",
    "        print(f\"   Personnes: {len(entities.get('persons', []))} → {[p.get('text', p) if isinstance(p, dict) else str(p) for p in persons]}\")\n",
    "    \n",
    "    if 'organizations' in entities:\n",
    "        orgs = entities['organizations'][:3] if isinstance(entities['organizations'], list) else []\n",
    "        print(f\"   Organisations: {len(entities.get('organizations', []))} → {[o.get('text', o) if isinstance(o, dict) else str(o) for o in orgs]}\")\n",
    "    \n",
    "    if 'locations' in entities:\n",
    "        locs = entities['locations'][:3] if isinstance(entities['locations'], list) else []\n",
    "        print(f\"   Lieux: {len(entities.get('locations', []))} → {[l.get('text', l) if isinstance(l, dict) else str(l) for l in locs]}\")\n",
    "else:\n",
    "    print(f\"   (Détails d'entités non disponibles dans enrichment_metadata)\")\n",
    "    print(f\"   Clés disponibles: {list(result_enriched.enrichment_metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÉTRIQUES DE COMPLEXITÉ:\n",
      "   (Métriques de complexité non disponibles dans enrichment_metadata)\n",
      "   Métadonnées disponibles: ['word_count', 'sentence_count', 'strategy', 'has_corruption', 'repetition_ratio', 'encoding_quality', 'entity_density', 'fact_check_density']\n",
      "   Métriques de base disponibles:\n",
      "     Nombre de mots: 27\n",
      "     Score de priorité: 0.000\n",
      "     Nombre d'issues: 1\n",
      "     Temps de traitement: 15.7ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMÉTRIQUES DE COMPLEXITÉ:\")\n",
    "\n",
    "# Vérifier si les métriques sont dans enrichment_metadata\n",
    "if 'complexity_metrics' in result_enriched.enrichment_metadata:\n",
    "    complexity_metrics = result_enriched.enrichment_metadata['complexity_metrics']\n",
    "    for key, value in complexity_metrics.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(f\"   (Métriques de complexité non disponibles dans enrichment_metadata)\")\n",
    "    print(f\"   Métadonnées disponibles: {list(result_enriched.enrichment_metadata.keys())}\")\n",
    "    \n",
    "    # Afficher les métriques de base disponibles\n",
    "    print(f\"   Métriques de base disponibles:\")\n",
    "    print(f\"     Nombre de mots: {result_enriched.word_count}\")\n",
    "    print(f\"     Score de priorité: {result_enriched.priority_score:.3f}\")\n",
    "    print(f\"     Nombre d'issues: {len(result_enriched.issues)}\")\n",
    "    print(f\"     Temps de traitement: {result_enriched.processing_time_ms:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. INDICATEURS DE QUALITÉ:\n",
      "   (Indicateurs de qualité non disponibles dans enrichment_metadata)\n",
      "   Indicateurs de base disponibles:\n",
      "     Statut: VALIDE\n",
      "     Confiance: 0.700\n",
      "     Niveau de risque: low\n",
      "     Score de priorité: 0.000\n",
      "     Répartition sévérité: {'critical': 0, 'moderate': 1, 'minor': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n4. INDICATEURS DE QUALITÉ:\")\n",
    "\n",
    "# Vérifier si les indicateurs sont dans enrichment_metadata\n",
    "if 'quality_indicators' in result_enriched.enrichment_metadata:\n",
    "    quality_indicators = result_enriched.enrichment_metadata['quality_indicators']\n",
    "    for key, value in quality_indicators.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(f\"   (Indicateurs de qualité non disponibles dans enrichment_metadata)\")\n",
    "    \n",
    "    # Afficher les indicateurs de base disponibles\n",
    "    print(f\"   Indicateurs de base disponibles:\")\n",
    "    print(f\"     Statut: {'SUSPECT' if result_enriched.is_suspect else 'VALIDE'}\")\n",
    "    print(f\"     Confiance: {result_enriched.confidence_score:.3f}\")\n",
    "    print(f\"     Niveau de risque: {result_enriched.risk_level}\")\n",
    "    print(f\"     Score de priorité: {result_enriched.priority_score:.3f}\")\n",
    "    \n",
    "    if result_enriched.severity_breakdown:\n",
    "        print(f\"     Répartition sévérité: {result_enriched.severity_breakdown}\")\n",
    "    \n",
    "    if result_enriched.corrections_suggested:\n",
    "        print(f\"     Corrections suggérées: {len(result_enriched.corrections_suggested)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CANDIDATS FACT-CHECK (Top 5):\n",
      "   Nombre total de candidats: 1\n",
      "   1. Type: entity_verification | Priorité: 0.7 | Description: Vérification des 3 entités détectées\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCANDIDATS FACT-CHECK (Top 5):\")\n",
    "\n",
    "if result_enriched.fact_check_candidates:\n",
    "    candidates = result_enriched.fact_check_candidates[:5]\n",
    "    print(f\"   Nombre total de candidats: {len(result_enriched.fact_check_candidates)}\")\n",
    "    \n",
    "    for i, candidate in enumerate(candidates, 1):\n",
    "        if isinstance(candidate, dict):\n",
    "            # Utiliser la vraie structure: type, priority, description\n",
    "            type_val = candidate.get('type', 'N/A')\n",
    "            priority_val = candidate.get('priority', 'N/A')\n",
    "            description_val = candidate.get('description', 'N/A')\n",
    "            \n",
    "            print(f\"   {i}. Type: {type_val} | Priorité: {priority_val} | Description: {description_val}\")\n",
    "        else:\n",
    "            print(f\"   {i}. Candidat: {candidate}\")\n",
    "else:\n",
    "    print(f\"   Aucun candidat fact-check détecté\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module rechargé avec les corrections appliquées\n",
      "Corrections :\n",
      "- Seuils plus permissifs (0.3 → 0.05)\n",
      "- Pénalités réduites (70% → 15-30%)\n",
      "- Enrichissement proactif\n",
      "- Validation externe désactivée par défaut\n",
      "\n",
      "Traitement en cours des 189 résumés validés Niveau 0...\n",
      "   Traité 50/189 résumés...\n",
      "   Traité 100/189 résumés...\n",
      "   Traité 150/189 résumés...\n",
      "Traitement terminé en 9.52s\n",
      "\n",
      "Résultats sur résumés validés Niveau 0 (APRÈS CORRECTIONS):\n",
      "   Total analysé: 189\n",
      "   Suspects détectés: 16\n",
      "   Taux de détection: 8.5%\n",
      "   Temps moyen: 50.4ms par résumé\n"
     ]
    }
   ],
   "source": [
    "# ===== ANALYSE AVEC CODE CORRIGÉ =====\n",
    "# Force reload du module corrigé\n",
    "import importlib\n",
    "if 'detection.level1_heuristic' in sys.modules:\n",
    "    importlib.reload(sys.modules['detection.level1_heuristic'])\n",
    "\n",
    "from detection.level1_heuristic import HeuristicAnalyzer\n",
    "\n",
    "print(\"Module rechargé avec les corrections appliquées\")\n",
    "print(\"Corrections :\")\n",
    "print(\"- Seuils plus permissifs (0.3 → 0.05)\")\n",
    "print(\"- Pénalités réduites (70% → 15-30%)\")\n",
    "print(\"- Enrichissement proactif\") \n",
    "print(\"- Validation externe désactivée par défaut\")\n",
    "\n",
    "detector_main = HeuristicAnalyzer(enable_wikidata=False, enable_entity_validation=True)\n",
    "\n",
    "print(f\"\\nTraitement en cours des {len(level0_validated_data)} résumés validés Niveau 0...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Traitement batch manuel car process_batch n'existe peut-être pas\n",
    "all_results_main = []\n",
    "valid_summaries_main = []\n",
    "\n",
    "for i, summary_data in enumerate(level0_validated_data):\n",
    "    # Préparer les métadonnées\n",
    "    metadata = {\n",
    "        'id': summary_data['id'],\n",
    "        'coherence_score': summary_data['coherence'],\n",
    "        'factuality_score': summary_data['factuality'],\n",
    "        'quality_grade': summary_data['quality_grade']\n",
    "    }\n",
    "    \n",
    "    # Analyser le résumé\n",
    "    result = detector_main.analyze_summary(summary_data['text'], metadata)\n",
    "    all_results_main.append(result)\n",
    "    \n",
    "    # Ajouter aux valides si pas suspect\n",
    "    if not result.is_suspect:\n",
    "        valid_summaries_main.append(summary_data)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"   Traité {i + 1}/{len(level0_validated_data)} résumés...\")\n",
    "\n",
    "total_time_main = time.time() - start_time\n",
    "\n",
    "print(f\"Traitement terminé en {total_time_main:.2f}s\")\n",
    "print(f\"\\nRésultats sur résumés validés Niveau 0 (APRÈS CORRECTIONS):\")\n",
    "print(f\"   Total analysé: {len(level0_validated_data)}\")\n",
    "print(f\"   Suspects détectés: {len(level0_validated_data) - len(valid_summaries_main)}\")\n",
    "print(f\"   Taux de détection: {(len(level0_validated_data) - len(valid_summaries_main))/len(level0_validated_data)*100:.1f}%\")\n",
    "print(f\"   Temps moyen: {total_time_main/len(level0_validated_data)*1000:.1f}ms par résumé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Objectif <100ms: 91.0% des résumés\n",
      "   Amélioration performance: +41.5 points\n",
      "   Candidats fact-check générés: 194 (vs 5 avant)\n",
      "   Requêtes Wikidata: 0 (vs 5 avant)\n",
      "   (Debug) Clés enrichment_metadata: ['word_count', 'sentence_count', 'strategy', 'has_corruption', 'repetition_ratio', 'encoding_quality', 'entity_density', 'fact_check_density']\n"
     ]
    }
   ],
   "source": [
    "# Vérification objectif performance (APRÈS CORRECTIONS)\n",
    "processing_times_main = [r.processing_time_ms for r in all_results_main]\n",
    "target_met = sum(1 for t in processing_times_main if t < 100) / len(processing_times_main) * 100\n",
    "print(f\"   Objectif <100ms: {target_met:.1f}% des résumés\")\n",
    "\n",
    "# Amélioration vs objectif original (49.5%)\n",
    "improvement = target_met - 49.5\n",
    "print(f\"   Amélioration performance: {improvement:+.1f} points\")\n",
    "\n",
    "# Nouvelles métriques d'enrichissement\n",
    "total_fact_check_new = sum(len(r.fact_check_candidates) for r in all_results_main)\n",
    "\n",
    "# Vérifier si validation_hints existe dans enrichment_metadata\n",
    "total_wikidata_new = 0\n",
    "for r in all_results_main:\n",
    "    if 'validation_hints' in r.enrichment_metadata:\n",
    "        validation_hints = r.enrichment_metadata['validation_hints']\n",
    "        if isinstance(validation_hints, dict) and 'wikidata_queries' in validation_hints:\n",
    "            total_wikidata_new += len(validation_hints['wikidata_queries'])\n",
    "\n",
    "print(f\"   Candidats fact-check générés: {total_fact_check_new} (vs 5 avant)\")\n",
    "print(f\"   Requêtes Wikidata: {total_wikidata_new} (vs 5 avant)\")\n",
    "\n",
    "# Debug: Afficher la structure pour comprendre\n",
    "if all_results_main:\n",
    "    first_result = all_results_main[0]\n",
    "    print(f\"   (Debug) Clés enrichment_metadata: {list(first_result.enrichment_metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corrélation avec grades de qualité (NOUVELLES DONNÉES):\n",
      "Grade    Suspects     %        Confiance    Temps (ms)\n",
      "-------------------------------------------------------\n",
      "A+       1/58       1.7      0.873        47.4\n",
      "A        2/62       3.2      0.837        53.9\n",
      "B+       3/39       7.7      0.763        52.8\n",
      "B        3/7        42.9     0.521        63.7\n",
      "D        7/23       30.4     0.765        39.1\n",
      "\n",
      "=== COMPARAISON AVANT/APRÈS CORRECTIONS ===\n",
      "Grade A+ suspects: AVANT 0/58 (0.0%) → APRÈS 1/58 (1.7%)\n",
      "Grade D suspects: AVANT 23/23 (100.0%) → APRÈS 7/23 (30.4%)\n"
     ]
    }
   ],
   "source": [
    "# Corrélation avec grades de qualité (APRÈS CORRECTIONS)\n",
    "print(f\"\\nCorrélation avec grades de qualité (NOUVELLES DONNÉES):\")\n",
    "print(f\"{'Grade':<8} {'Suspects':<12} {'%':<8} {'Confiance':<12} {'Temps (ms)'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for grade in ['A+', 'A', 'B+', 'B', 'C', 'D']:\n",
    "    grade_data = [s for s in level0_validated_data if s['quality_grade'] == grade]\n",
    "    if grade_data:\n",
    "        grade_results = [all_results_main[i] for i, s in enumerate(level0_validated_data) if s['quality_grade'] == grade]\n",
    "        suspect_count = sum(1 for r in grade_results if r.is_suspect)  # CORRIGÉ: is_suspect au lieu de not is_valid\n",
    "        total_count = len(grade_data)\n",
    "        avg_confidence = sum(r.confidence_score for r in grade_results) / len(grade_results)\n",
    "        avg_time = sum(r.processing_time_ms for r in grade_results) / len(grade_results)\n",
    "        \n",
    "        print(f\"{grade:<8} {suspect_count}/{total_count:<8} {suspect_count/total_count*100:<8.1f} {avg_confidence:<12.3f} {avg_time:.1f}\")\n",
    "\n",
    "# Comparaison avant/après pour les grades critiques\n",
    "print(f\"\\n=== COMPARAISON AVANT/APRÈS CORRECTIONS ===\")\n",
    "a_plus_suspects = sum(1 for i, s in enumerate(level0_validated_data) if s['quality_grade'] == 'A+' and all_results_main[i].is_suspect)\n",
    "a_plus_total = sum(1 for s in level0_validated_data if s['quality_grade'] == 'A+')\n",
    "d_suspects = sum(1 for i, s in enumerate(level0_validated_data) if s['quality_grade'] == 'D' and all_results_main[i].is_suspect)\n",
    "d_total = sum(1 for s in level0_validated_data if s['quality_grade'] == 'D')\n",
    "\n",
    "print(f\"Grade A+ suspects: AVANT 0/{a_plus_total} (0.0%) → APRÈS {a_plus_suspects}/{a_plus_total} ({a_plus_suspects/a_plus_total*100:.1f}%)\")\n",
    "print(f\"Grade D suspects: AVANT {d_total}/{d_total} (100.0%) → APRÈS {d_suspects}/{d_total} ({d_suspects/d_total*100:.1f}%)\")\n",
    "\n",
    "# Sauvegarder les nouveaux résultats\n",
    "globals()['level0_validated_results'] = all_results_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement en cours des 189 résumés validés Niveau 0...\n",
      "Traitement terminé en 7.77s\n",
      "\n",
      "Résultats sur résumés validés Niveau 0:\n",
      "   Total analysé: 189\n",
      "   Suspects détectés: 16\n",
      "   Taux de détection: 8.5%\n",
      "   Temps moyen: 41.1ms par résumé\n"
     ]
    }
   ],
   "source": [
    "detector_main = HeuristicAnalyzer(enable_wikidata=False, enable_entity_validation=True)\n",
    "\n",
    "print(f\"Traitement en cours des {len(level0_validated_data)} résumés validés Niveau 0...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Traitement batch manuel\n",
    "all_results_main = []\n",
    "valid_summaries_main = []\n",
    "\n",
    "for i, summary_data in enumerate(level0_validated_data):\n",
    "    # Préparer les métadonnées\n",
    "    metadata = {\n",
    "        'id': summary_data['id'],\n",
    "        'coherence_score': summary_data['coherence'],\n",
    "        'factuality_score': summary_data['factuality'],\n",
    "        'quality_grade': summary_data['quality_grade']\n",
    "    }\n",
    "    \n",
    "    # Analyser le résumé\n",
    "    result = detector_main.analyze_summary(summary_data['text'], metadata)\n",
    "    all_results_main.append(result)\n",
    "    \n",
    "    # Ajouter aux valides si pas suspect\n",
    "    if not result.is_suspect:\n",
    "        valid_summaries_main.append(summary_data)\n",
    "\n",
    "total_time_main = time.time() - start_time\n",
    "\n",
    "print(f\"Traitement terminé en {total_time_main:.2f}s\")\n",
    "print(f\"\\nRésultats sur résumés validés Niveau 0:\")\n",
    "print(f\"   Total analysé: {len(level0_validated_data)}\")\n",
    "print(f\"   Suspects détectés: {len(level0_validated_data) - len(valid_summaries_main)}\")\n",
    "print(f\"   Taux de détection: {(len(level0_validated_data) - len(valid_summaries_main))/len(level0_validated_data)*100:.1f}%\")\n",
    "print(f\"   Temps moyen: {total_time_main/len(level0_validated_data)*1000:.1f}ms par résumé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement en cours des 183 résumés rejetés Niveau 0...\n",
      "   Traité 50/183 résumés...\n",
      "   Traité 100/183 résumés...\n",
      "   Traité 150/183 résumés...\n",
      "Traitement terminé en 30.71s\n",
      "\n",
      "Résultats sur résumés rejetés Niveau 0 (APRÈS CORRECTIONS):\n",
      "   Total analysé: 183\n",
      "   Suspects détectés: 181\n",
      "   Taux de détection: 98.9%\n",
      "   Temps moyen: 167.8ms par résumé\n",
      "\n",
      "Distribution par grade (résumés rejetés Niveau 0) - APRÈS CORRECTIONS:\n",
      "   Grade A+: 0/2 suspects (0.0%) - Confiance: 0.850\n",
      "   Grade B: 4/4 suspects (100.0%) - Confiance: 0.000\n",
      "   Grade B+: 119/119 suspects (100.0%) - Confiance: 0.003\n",
      "   Grade C: 17/17 suspects (100.0%) - Confiance: 0.000\n",
      "   Grade D: 41/41 suspects (100.0%) - Confiance: 0.002\n",
      "\n",
      "Résultats sauvegardés avec les nouvelles données\n"
     ]
    }
   ],
   "source": [
    "# ===== ANALYSE RÉSUMÉS REJETÉS NIVEAU 0 (AVEC CODE CORRIGÉ) =====\n",
    "\n",
    "if len(level0_rejected_data) > 0:\n",
    "    # Utiliser le même détecteur corrigé\n",
    "    detector_rejected = HeuristicAnalyzer(enable_wikidata=False, enable_entity_validation=True)\n",
    "    \n",
    "    print(f\"Traitement en cours des {len(level0_rejected_data)} résumés rejetés Niveau 0...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Traitement batch manuel\n",
    "    results_rejected = []\n",
    "    valid_rejected = []\n",
    "\n",
    "    for i, summary_data in enumerate(level0_rejected_data):\n",
    "        # Préparer les métadonnées\n",
    "        metadata = {\n",
    "            'id': summary_data['id'],\n",
    "            'coherence_score': summary_data['coherence'],\n",
    "            'factuality_score': summary_data['factuality'],\n",
    "            'quality_grade': summary_data['quality_grade']\n",
    "        }\n",
    "        \n",
    "        # Analyser le résumé\n",
    "        result = detector_rejected.analyze_summary(summary_data['text'], metadata)\n",
    "        results_rejected.append(result)\n",
    "        \n",
    "        # Ajouter aux valides si pas suspect\n",
    "        if not result.is_suspect:\n",
    "            valid_rejected.append(summary_data)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"   Traité {i + 1}/{len(level0_rejected_data)} résumés...\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Traitement terminé en {processing_time:.2f}s\")\n",
    "    print(f\"\\nRésultats sur résumés rejetés Niveau 0 (APRÈS CORRECTIONS):\")\n",
    "    print(f\"   Total analysé: {len(level0_rejected_data)}\")\n",
    "    print(f\"   Suspects détectés: {len(level0_rejected_data) - len(valid_rejected)}\")\n",
    "    print(f\"   Taux de détection: {(len(level0_rejected_data) - len(valid_rejected))/len(level0_rejected_data)*100:.1f}%\")\n",
    "    print(f\"   Temps moyen: {processing_time/len(level0_rejected_data)*1000:.1f}ms par résumé\")\n",
    "    \n",
    "    # Distribution des grades pour les rejetés (NOUVELLES DONNÉES)\n",
    "    print(f\"\\nDistribution par grade (résumés rejetés Niveau 0) - APRÈS CORRECTIONS:\")\n",
    "    rejected_grades = Counter([s['quality_grade'] for s in level0_rejected_data])\n",
    "    for grade, count in sorted(rejected_grades.items()):\n",
    "        suspect_count = sum(1 for i, s in enumerate(level0_rejected_data) \n",
    "                           if s['quality_grade'] == grade and results_rejected[i].is_suspect)\n",
    "        avg_confidence = sum(r.confidence_score for i, r in enumerate(results_rejected) \n",
    "                           if level0_rejected_data[i]['quality_grade'] == grade) / max(1, count)\n",
    "        print(f\"   Grade {grade}: {suspect_count}/{count} suspects ({suspect_count/count*100:.1f}%) - Confiance: {avg_confidence:.3f}\")\n",
    "    \n",
    "    # Sauvegarder pour usage ultérieur\n",
    "    globals()['level0_rejected_results'] = results_rejected\n",
    "    print(f\"\\nRésultats sauvegardés avec les nouvelles données\")\n",
    "    \n",
    "else:\n",
    "    print(\"Aucun résumé rejeté par Niveau 0 à analyser\")\n",
    "    results_rejected = []\n",
    "    globals()['level0_rejected_results'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corrélation avec grades de qualité:\n",
      "   Grade A+: 1/58 suspects (1.7%) - Confiance moy: 0.873\n",
      "   Grade A: 2/62 suspects (3.2%) - Confiance moy: 0.837\n",
      "   Grade B+: 3/39 suspects (7.7%) - Confiance moy: 0.763\n",
      "   Grade B: 3/7 suspects (42.9%) - Confiance moy: 0.521\n",
      "   Grade D: 7/23 suspects (30.4%) - Confiance moy: 0.765\n"
     ]
    }
   ],
   "source": [
    "# Corrélation avec grades de qualité\n",
    "print(f\"\\nCorrélation avec grades de qualité:\")\n",
    "for grade in ['A+', 'A', 'B+', 'B', 'C', 'D']:\n",
    "    grade_data = [s for s in level0_validated_data if s['quality_grade'] == grade]\n",
    "    if grade_data:\n",
    "        grade_results = [all_results_main[i] for i, s in enumerate(level0_validated_data) if s['quality_grade'] == grade]\n",
    "        suspect_count = sum(1 for r in grade_results if r.is_suspect)  # CORRIGÉ: is_suspect\n",
    "        total_count = len(grade_data)\n",
    "        avg_confidence = sum(r.confidence_score for r in grade_results) / len(grade_results)\n",
    "        print(f\"   Grade {grade}: {suspect_count}/{total_count} suspects ({suspect_count/total_count*100:.1f}%) - Confiance moy: {avg_confidence:.3f}\")\n",
    "\n",
    "# Sauvegarder pour usage ultérieur\n",
    "globals()['level0_validated_results'] = all_results_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RÉSULTATS GLOBAUX (APRÈS CORRECTIONS):\n",
      "   Total analysé: 372\n",
      "   Suspects détectés: 197\n",
      "   Taux de détection global: 53.0%\n",
      "\n",
      "COMPARAISON TAUX DE DÉTECTION:\n",
      "   AVANT corrections: 62.9%\n",
      "   APRÈS corrections: 53.0%\n",
      "   Amélioration: +9.9 points\n",
      "\n",
      "RÉPARTITION PAR POPULATION (NOUVELLES DONNÉES):\n",
      "   Population validée Niveau 0 (189 résumés):\n",
      "     Suspects détectés: 16 (8.5%)\n",
      "   Population rejetée Niveau 0 (183 résumés):\n",
      "     Suspects détectés: 181 (98.9%)\n",
      "\n",
      "DISTRIBUTION GLOBALE PAR GRADE (NOUVELLES DONNÉES):\n",
      "Grade    Total    Suspects     %        Confiance   \n",
      "-------------------------------------------------------\n",
      "A+       60       1            1.7      0.873\n",
      "A        62       2            3.2      0.837\n",
      "B+       158      122          77.2     0.191\n",
      "B        11       7            63.6     0.332\n",
      "C        17       17           100.0    0.000\n",
      "D        64       48           75.0     0.277\n",
      "\n",
      "PERFORMANCE GLOBALE (NOUVELLES DONNÉES):\n",
      "   Temps moyen: 103.1ms (vs 116.3ms avant)\n",
      "   Temps maximum: 553.4ms (vs 388.6ms avant)\n",
      "   Objectif <100ms: 61.3% (vs 49.5% avant)\n",
      "   Candidats fact-check: 411 (vs 5 avant)\n",
      "   Requêtes Wikidata: 0 (vs 5 avant)\n",
      "\n",
      "============================================================\n",
      "SUCCÈS DES CORRECTIONS:\n",
      "   2/3 objectifs atteints\n",
      "   ✓ Performance améliorée | ✓ Enrichissement augmenté\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== VUE D'ENSEMBLE AVEC NOUVELLES DONNÉES =====\n",
    "\n",
    "# Combiner tous les nouveaux résultats\n",
    "all_372_summaries = level0_validated_data + level0_rejected_data\n",
    "all_372_results = level0_validated_results + level0_rejected_results\n",
    "\n",
    "print(f\"RÉSULTATS GLOBAUX (APRÈS CORRECTIONS):\")\n",
    "total_suspects_new = sum(1 for r in all_372_results if r.is_suspect)  # CORRIGÉ: is_suspect\n",
    "detection_rate_new = total_suspects_new/len(all_372_results)*100\n",
    "\n",
    "print(f\"   Total analysé: {len(all_372_summaries)}\")\n",
    "print(f\"   Suspects détectés: {total_suspects_new}\")\n",
    "print(f\"   Taux de détection global: {detection_rate_new:.1f}%\")\n",
    "\n",
    "print(f\"\\nCOMPARAISON TAUX DE DÉTECTION:\")\n",
    "print(f\"   AVANT corrections: 62.9%\")\n",
    "print(f\"   APRÈS corrections: {detection_rate_new:.1f}%\")\n",
    "print(f\"   Amélioration: {62.9 - detection_rate_new:+.1f} points\")\n",
    "\n",
    "print(f\"\\nRÉPARTITION PAR POPULATION (NOUVELLES DONNÉES):\")\n",
    "validated_suspects_new = len(level0_validated_data) - len(valid_summaries_main)\n",
    "rejected_suspects_new = len(level0_rejected_data) - len(valid_rejected) if len(level0_rejected_data) > 0 else 0\n",
    "\n",
    "print(f\"   Population validée Niveau 0 ({len(level0_validated_data)} résumés):\")\n",
    "print(f\"     Suspects détectés: {validated_suspects_new} ({validated_suspects_new/len(level0_validated_data)*100:.1f}%)\")\n",
    "\n",
    "if len(level0_rejected_data) > 0:\n",
    "    print(f\"   Population rejetée Niveau 0 ({len(level0_rejected_data)} résumés):\")\n",
    "    print(f\"     Suspects détectés: {rejected_suspects_new} ({rejected_suspects_new/len(level0_rejected_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDISTRIBUTION GLOBALE PAR GRADE (NOUVELLES DONNÉES):\")\n",
    "print(f\"{'Grade':<8} {'Total':<8} {'Suspects':<12} {'%':<8} {'Confiance':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for grade in ['A+', 'A', 'B+', 'B', 'C', 'D']:\n",
    "    grade_summaries = [s for s in all_372_summaries if s['quality_grade'] == grade]\n",
    "    if grade_summaries:\n",
    "        grade_results = [all_372_results[i] for i, s in enumerate(all_372_summaries) if s['quality_grade'] == grade]\n",
    "        suspect_count = sum(1 for r in grade_results if r.is_suspect)  # CORRIGÉ: is_suspect\n",
    "        total_count = len(grade_summaries)\n",
    "        avg_confidence = sum(r.confidence_score for r in grade_results) / len(grade_results)\n",
    "        print(f\"{grade:<8} {total_count:<8} {suspect_count:<12} {suspect_count/total_count*100:<8.1f} {avg_confidence:.3f}\")\n",
    "\n",
    "print(f\"\\nPERFORMANCE GLOBALE (NOUVELLES DONNÉES):\")\n",
    "all_processing_times_new = [r.processing_time_ms for r in all_372_results]\n",
    "avg_time_new = sum(all_processing_times_new) / len(all_processing_times_new)\n",
    "max_time_new = max(all_processing_times_new)\n",
    "target_100ms_global_new = sum(1 for t in all_processing_times_new if t < 100) / len(all_processing_times_new) * 100\n",
    "\n",
    "print(f\"   Temps moyen: {avg_time_new:.1f}ms (vs 116.3ms avant)\")\n",
    "print(f\"   Temps maximum: {max_time_new:.1f}ms (vs 388.6ms avant)\")\n",
    "print(f\"   Objectif <100ms: {target_100ms_global_new:.1f}% (vs 49.5% avant)\")\n",
    "\n",
    "# Enrichissement global\n",
    "total_fact_check_global = sum(len(r.fact_check_candidates) for r in all_372_results)\n",
    "\n",
    "# Wikidata queries - vérification sécurisée\n",
    "total_wikidata_global = 0\n",
    "for r in all_372_results:\n",
    "    if 'validation_hints' in r.enrichment_metadata:\n",
    "        hints = r.enrichment_metadata['validation_hints']\n",
    "        if isinstance(hints, dict) and 'wikidata_queries' in hints:\n",
    "            total_wikidata_global += len(hints['wikidata_queries'])\n",
    "\n",
    "print(f\"   Candidats fact-check: {total_fact_check_global} (vs 5 avant)\")\n",
    "print(f\"   Requêtes Wikidata: {total_wikidata_global} (vs 5 avant)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUCCÈS DES CORRECTIONS:\")\n",
    "improvements = []\n",
    "if detection_rate_new < 50: improvements.append(\"✓ Taux détection réduit\")\n",
    "if target_100ms_global_new > 60: improvements.append(\"✓ Performance améliorée\") \n",
    "if total_fact_check_global > 50: improvements.append(\"✓ Enrichissement augmenté\")\n",
    "print(f\"   {len(improvements)}/3 objectifs atteints\")\n",
    "print(f\"   {' | '.join(improvements) if improvements else 'Ajustements nécessaires'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement en cours des 183 résumés rejetés Niveau 0...\n",
      "Traitement terminé en 21.81s\n",
      "\n",
      "Résultats sur résumés rejetés Niveau 0:\n",
      "   Total analysé: 183\n",
      "   Suspects détectés: 181\n",
      "   Taux de détection: 98.9%\n",
      "   Temps moyen: 119.2ms par résumé\n",
      "\n",
      "Distribution par grade (résumés rejetés Niveau 0):\n",
      "   Grade A+: 0/2 suspects (0.0%)\n",
      "   Grade B: 4/4 suspects (100.0%)\n",
      "   Grade B+: 119/119 suspects (100.0%)\n",
      "   Grade C: 17/17 suspects (100.0%)\n",
      "   Grade D: 41/41 suspects (100.0%)\n",
      "\n",
      "Analyse des patterns de détection:\n",
      "   Types d'issues les plus fréquents (résumés rejetés Niveau 0):\n",
      "     {'type': 899 cas\n",
      "\n",
      "Résultats sauvegardés pour analyse ultérieure\n"
     ]
    }
   ],
   "source": [
    "if len(level0_rejected_data) > 0:\n",
    "    detector_rejected = HeuristicAnalyzer(enable_wikidata=False, enable_entity_validation=True)\n",
    "    \n",
    "    print(f\"Traitement en cours des {len(level0_rejected_data)} résumés rejetés Niveau 0...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Traitement batch manuel\n",
    "    results_rejected = []\n",
    "    valid_rejected = []\n",
    "\n",
    "    for i, summary_data in enumerate(level0_rejected_data):\n",
    "        # Préparer les métadonnées\n",
    "        metadata = {\n",
    "            'id': summary_data['id'],\n",
    "            'coherence_score': summary_data['coherence'],\n",
    "            'factuality_score': summary_data['factuality'],\n",
    "            'quality_grade': summary_data['quality_grade']\n",
    "        }\n",
    "        \n",
    "        # Analyser le résumé\n",
    "        result = detector_rejected.analyze_summary(summary_data['text'], metadata)\n",
    "        results_rejected.append(result)\n",
    "        \n",
    "        # Ajouter aux valides si pas suspect\n",
    "        if not result.is_suspect:\n",
    "            valid_rejected.append(summary_data)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Traitement terminé en {processing_time:.2f}s\")\n",
    "    print(f\"\\nRésultats sur résumés rejetés Niveau 0:\")\n",
    "    print(f\"   Total analysé: {len(level0_rejected_data)}\")\n",
    "    print(f\"   Suspects détectés: {len(level0_rejected_data) - len(valid_rejected)}\")\n",
    "    print(f\"   Taux de détection: {(len(level0_rejected_data) - len(valid_rejected))/len(level0_rejected_data)*100:.1f}%\")\n",
    "    print(f\"   Temps moyen: {processing_time/len(level0_rejected_data)*1000:.1f}ms par résumé\")\n",
    "    \n",
    "    # Distribution des grades pour les rejetés\n",
    "    print(f\"\\nDistribution par grade (résumés rejetés Niveau 0):\")\n",
    "    rejected_grades = Counter([s['quality_grade'] for s in level0_rejected_data])\n",
    "    for grade, count in sorted(rejected_grades.items()):\n",
    "        suspect_count = sum(1 for i, s in enumerate(level0_rejected_data) \n",
    "                           if s['quality_grade'] == grade and results_rejected[i].is_suspect)  # CORRIGÉ: is_suspect\n",
    "        print(f\"   Grade {grade}: {suspect_count}/{count} suspects ({suspect_count/count*100:.1f}%)\")\n",
    "    \n",
    "    # Comparaison des raisons de détection\n",
    "    print(f\"\\nAnalyse des patterns de détection:\")\n",
    "    rejected_issues = []\n",
    "    for result in results_rejected:\n",
    "        # Utiliser 'issues' au lieu de 'detected_issues'\n",
    "        rejected_issues.extend([str(issue) for issue in result.issues])\n",
    "    \n",
    "    if rejected_issues:\n",
    "        issue_types = Counter([issue.split(':')[0] if ':' in str(issue) else str(issue)[:20] for issue in rejected_issues])\n",
    "        print(f\"   Types d'issues les plus fréquents (résumés rejetés Niveau 0):\")\n",
    "        for issue_type, count in issue_types.most_common(5):\n",
    "            print(f\"     {issue_type}: {count} cas\")\n",
    "    else:\n",
    "        print(f\"   Aucune issue spécifique détectée dans les résultats\")\n",
    "    \n",
    "    # Sauvegarder pour usage ultérieur\n",
    "    globals()['level0_rejected_results'] = results_rejected\n",
    "    print(f\"\\nRésultats sauvegardés pour analyse ultérieure\")\n",
    "    \n",
    "else:\n",
    "    print(\"Aucun résumé rejeté par Niveau 0 à analyser\")\n",
    "    results_rejected = []\n",
    "    globals()['level0_rejected_results'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 7. Vue d'Ensemble - Cartographie Complète des 372 Résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RÉSULTATS GLOBAUX:\n",
      "   Total analysé: 372\n",
      "   Suspects détectés: 197\n",
      "   Taux de détection global: 53.0%\n",
      "\n",
      "RÉPARTITION PAR POPULATION:\n",
      "   Population validée Niveau 0 (189 résumés):\n",
      "     Suspects détectés: 16 (8.5%)\n",
      "   Population rejetée Niveau 0 (183 résumés):\n",
      "     Suspects détectés: 181 (98.9%)\n",
      "\n",
      "DISTRIBUTION GLOBALE PAR GRADE:\n",
      "   Grade A+: 1/60 suspects (1.7%) - Confiance: 0.873\n",
      "   Grade A: 2/62 suspects (3.2%) - Confiance: 0.837\n",
      "   Grade B+: 122/158 suspects (77.2%) - Confiance: 0.191\n",
      "   Grade B: 7/11 suspects (63.6%) - Confiance: 0.332\n",
      "   Grade C: 17/17 suspects (100.0%) - Confiance: 0.000\n",
      "   Grade D: 48/64 suspects (75.0%) - Confiance: 0.277\n",
      "\n",
      "PERFORMANCE GLOBALE:\n",
      "   Temps moyen: 79.5ms\n",
      "   Temps maximum: 308.7ms\n",
      "   Objectif <100ms: 71.8% des résumés\n"
     ]
    }
   ],
   "source": [
    "# Combiner tous les résultats\n",
    "all_372_summaries = level0_validated_data + level0_rejected_data\n",
    "all_372_results = level0_validated_results + level0_rejected_results\n",
    "\n",
    "print(f\"RÉSULTATS GLOBAUX:\")\n",
    "print(f\"   Total analysé: {len(all_372_summaries)}\")\n",
    "print(f\"   Suspects détectés: {sum(1 for r in all_372_results if r.is_suspect)}\")  # CORRIGÉ: is_suspect\n",
    "print(f\"   Taux de détection global: {sum(1 for r in all_372_results if r.is_suspect)/len(all_372_results)*100:.1f}%\")  # CORRIGÉ: is_suspect\n",
    "\n",
    "print(f\"\\nRÉPARTITION PAR POPULATION:\")\n",
    "validated_suspects = len(level0_validated_data) - len(valid_summaries_main)\n",
    "rejected_suspects = len(level0_rejected_data) - len(valid_rejected) if len(level0_rejected_data) > 0 else 0\n",
    "\n",
    "print(f\"   Population validée Niveau 0 ({len(level0_validated_data)} résumés):\")\n",
    "print(f\"     Suspects détectés: {validated_suspects} ({validated_suspects/len(level0_validated_data)*100:.1f}%)\")\n",
    "\n",
    "if len(level0_rejected_data) > 0:\n",
    "    print(f\"   Population rejetée Niveau 0 ({len(level0_rejected_data)} résumés):\")\n",
    "    print(f\"     Suspects détectés: {rejected_suspects} ({rejected_suspects/len(level0_rejected_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDISTRIBUTION GLOBALE PAR GRADE:\")\n",
    "for grade in ['A+', 'A', 'B+', 'B', 'C', 'D']:\n",
    "    grade_summaries = [s for s in all_372_summaries if s['quality_grade'] == grade]\n",
    "    if grade_summaries:\n",
    "        grade_results = [all_372_results[i] for i, s in enumerate(all_372_summaries) if s['quality_grade'] == grade]\n",
    "        suspect_count = sum(1 for r in grade_results if r.is_suspect)  # CORRIGÉ: is_suspect\n",
    "        total_count = len(grade_summaries)\n",
    "        avg_confidence = sum(r.confidence_score for r in grade_results) / len(grade_results)\n",
    "        print(f\"   Grade {grade}: {suspect_count}/{total_count} suspects ({suspect_count/total_count*100:.1f}%) - Confiance: {avg_confidence:.3f}\")\n",
    "\n",
    "print(f\"\\nPERFORMANCE GLOBALE:\")\n",
    "all_processing_times = [r.processing_time_ms for r in all_372_results]\n",
    "avg_time = sum(all_processing_times) / len(all_processing_times)\n",
    "max_time = max(all_processing_times)\n",
    "target_100ms_global = sum(1 for t in all_processing_times if t < 100) / len(all_processing_times) * 100\n",
    "\n",
    "print(f\"   Temps moyen: {avg_time:.1f}ms\")\n",
    "print(f\"   Temps maximum: {max_time:.1f}ms\")\n",
    "print(f\"   Objectif <100ms: {target_100ms_global:.1f}% des résumés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Enrichissement de 372 résumés avec 6 champs utiles\n",
      "   Performance <100ms respectée pour 71.8% des cas\n"
     ]
    }
   ],
   "source": [
    "print(f\"   Enrichissement de {len(all_372_summaries)} résumés avec 6 champs utiles\")\n",
    "print(f\"   Performance <100ms respectée pour {target_100ms_global:.1f}% des cas\")\n",
    "\n",
    "# Sauvegarder les variables globales pour utilisation ultérieure\n",
    "globals()['all_372_summaries'] = all_372_summaries\n",
    "globals()['all_372_results'] = all_372_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 8. Analyse des Patterns d'Enrichissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DES PATTERNS D'ENRICHISSEMENT\n",
      "\n",
      "Analyse sur échantillon de 20 résumés:\n",
      "\n",
      "Échantillon 1:\n",
      "   ID: 0_adaptive (Grade: A)\n",
      "   Mots: 85\n",
      "   Confiance: 0.750\n",
      "   Entités: 2\n",
      "   Score priorité: 0.000\n",
      "   Statut: VALIDE\n",
      "   Temps: 89.4ms\n"
     ]
    }
   ],
   "source": [
    "# Analyse des nouvelles informations enrichies\n",
    "print(\"ANALYSE DES PATTERNS D'ENRICHISSEMENT\\n\")\n",
    "\n",
    "# Échantillon représentatif pour analyse détaillée\n",
    "sample_size = min(20, len(all_372_results))\n",
    "sample_indices = np.linspace(0, len(all_372_results)-1, sample_size, dtype=int)\n",
    "\n",
    "print(f\"Analyse sur échantillon de {sample_size} résumés:\")\n",
    "\n",
    "# Analyse des métriques disponibles\n",
    "word_counts = []\n",
    "confidence_scores = []\n",
    "entity_counts = []\n",
    "priority_scores = []\n",
    "\n",
    "for i in sample_indices:\n",
    "    result = all_372_results[i]\n",
    "    summary = all_372_summaries[i]\n",
    "    \n",
    "    # Collecter les métriques réellement disponibles\n",
    "    word_counts.append(result.word_count)  # Propriété directe\n",
    "    confidence_scores.append(result.confidence_score)  # Propriété directe\n",
    "    entity_counts.append(result.entities_detected)  # Propriété directe\n",
    "    priority_scores.append(result.priority_score)  # Propriété directe\n",
    "    \n",
    "    # Afficher des détails pour les premiers résultats\n",
    "    if i < 3:\n",
    "        print(f\"\\nÉchantillon {i+1}:\")\n",
    "        print(f\"   ID: {summary['id']} (Grade: {summary['quality_grade']})\")\n",
    "        print(f\"   Mots: {result.word_count}\")\n",
    "        print(f\"   Confiance: {result.confidence_score:.3f}\")\n",
    "        print(f\"   Entités: {result.entities_detected}\")\n",
    "        print(f\"   Score priorité: {result.priority_score:.3f}\")\n",
    "        print(f\"   Statut: {'SUSPECT' if result.is_suspect else 'VALIDE'}\")\n",
    "        print(f\"   Temps: {result.processing_time_ms:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métriques d'enrichissement:\n",
      "   Longueur moyenne: 229.8 mots (min: 21, max: 706)\n",
      "   Score de confiance moyen: 0.423\n",
      "   Entités détectées en moyenne: 15.5\n",
      "   Score de priorité moyen: 0.520\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMétriques d'enrichissement:\")\n",
    "print(f\"   Longueur moyenne: {np.mean(word_counts):.1f} mots (min: {np.min(word_counts)}, max: {np.max(word_counts)})\")\n",
    "print(f\"   Score de confiance moyen: {np.mean(confidence_scores):.3f}\")  # CORRIGÉ: confidence_scores\n",
    "print(f\"   Entités détectées en moyenne: {np.mean(entity_counts):.1f}\")  # CORRIGÉ: entity_counts\n",
    "print(f\"   Score de priorité moyen: {np.mean(priority_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Types de candidats fact-check identifiés:\n",
      "   entity_verification: 356 candidats (86.6%)\n",
      "   factual_claim: 55 candidats (13.4%)\n"
     ]
    }
   ],
   "source": [
    "# Analyse des candidats fact-check\n",
    "all_fact_check_candidates = []\n",
    "for result in all_372_results:\n",
    "    all_fact_check_candidates.extend(result.fact_check_candidates)\n",
    "\n",
    "if all_fact_check_candidates:\n",
    "    candidate_types = Counter([c.get('type', 'unknown') if isinstance(c, dict) else str(c) for c in all_fact_check_candidates])\n",
    "    print(f\"\\nTypes de candidats fact-check identifiés:\")\n",
    "    for ctype, count in candidate_types.most_common(5):\n",
    "        print(f\"   {ctype}: {count} candidats ({count/len(all_fact_check_candidates)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nAucun candidat fact-check trouvé dans les résultats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FICHIERS SAUVEGARDÉS (VERSION SIMPLIFIÉE) ===\n",
      "Résultats: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\detection\\level1_heuristic_results.csv\n",
      "Statistiques: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\detection\\level1_heuristic_stats.json\n",
      "372 résumés avec 15 colonnes\n",
      "Taux de détection: 53.0%\n",
      "Performance moyenne: 79.5ms\n",
      "Candidats fact-check: 411\n"
     ]
    }
   ],
   "source": [
    "# ===== SAUVEGARDE RÉSULTATS SIMPLIFIÉE =====\n",
    "\n",
    "# Créer le DataFrame avec les données réellement disponibles\n",
    "simple_results_df = pd.DataFrame([\n",
    "    {\n",
    "        'id': all_372_summaries[i]['id'],\n",
    "        'level0_status': 'validated' if i < len(level0_validated_data) else 'rejected',\n",
    "        'original_grade': all_372_summaries[i]['quality_grade'],\n",
    "        'coherence': all_372_summaries[i]['coherence'],\n",
    "        'factuality': all_372_summaries[i]['factuality'],\n",
    "        'is_suspect': all_372_results[i].is_suspect,  # CORRIGÉ: is_suspect\n",
    "        'confidence_score': all_372_results[i].confidence_score,\n",
    "        'risk_level': all_372_results[i].risk_level,\n",
    "        'processing_time_ms': all_372_results[i].processing_time_ms,\n",
    "        'word_count': all_372_results[i].word_count,  # Propriété directe\n",
    "        'entities_detected': all_372_results[i].entities_detected,  # Propriété directe\n",
    "        'suspicious_entities': all_372_results[i].suspicious_entities,  # Propriété directe\n",
    "        'fact_check_candidates_count': len(all_372_results[i].fact_check_candidates),\n",
    "        'priority_score': all_372_results[i].priority_score,  # Propriété directe\n",
    "        'num_issues': len(all_372_results[i].issues)  # Propriété correcte\n",
    "    }\n",
    "    for i in range(len(all_372_summaries))\n",
    "])\n",
    "\n",
    "# Créer le répertoire de sortie\n",
    "output_path = os.path.join(project_root, 'data', 'detection')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Sauvegarder les résultats simplifiés\n",
    "results_file = os.path.join(output_path, 'level1_heuristic_results.csv')\n",
    "simple_results_df.to_csv(results_file, index=False)\n",
    "\n",
    "# Statistiques simplifiées\n",
    "simple_stats = {\n",
    "    'total_analyzed': len(all_372_summaries),\n",
    "    'level0_validated': len(level0_validated_data),\n",
    "    'level0_rejected': len(level0_rejected_data),\n",
    "    'total_suspects': sum(1 for r in all_372_results if r.is_suspect),\n",
    "    'detection_rate_percent': sum(1 for r in all_372_results if r.is_suspect)/len(all_372_results)*100,\n",
    "    'avg_time_ms': sum(r.processing_time_ms for r in all_372_results) / len(all_372_results),\n",
    "    'max_time_ms': max(r.processing_time_ms for r in all_372_results),\n",
    "    'total_fact_check_candidates': sum(len(r.fact_check_candidates) for r in all_372_results)\n",
    "}\n",
    "\n",
    "stats_file = os.path.join(output_path, 'level1_heuristic_stats.json')\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(simple_stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"=== FICHIERS SAUVEGARDÉS (VERSION SIMPLIFIÉE) ===\")\n",
    "print(f\"Résultats: {results_file}\")\n",
    "print(f\"Statistiques: {stats_file}\")\n",
    "print(f\"{len(simple_results_df)} résumés avec {len(simple_results_df.columns)} colonnes\")\n",
    "print(f\"Taux de détection: {simple_stats['detection_rate_percent']:.1f}%\")\n",
    "print(f\"Performance moyenne: {simple_stats['avg_time_ms']:.1f}ms\")\n",
    "print(f\"Candidats fact-check: {simple_stats['total_fact_check_candidates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cellule désactivée car elle utilise des propriétés inexistantes :\n",
      "- result.get_priority_score() n'existe pas\n",
      "- result.detected_issues n'existe pas\n",
      "\n",
      "Pour voir les candidats prioritaires, utilisez :\n",
      "- result.priority_score (propriété directe)\n",
      "- result.issues (propriété correcte)\n",
      "- Tri manuel basé sur confidence_score et risk_level\n",
      "\n",
      "TOP 5 résumés par confiance faible (plus suspects) :\n",
      "   1. 6_confidence_weighted (Grade: B+)\n",
      "      Confiance: 0.000 | Risque: critical | Issues: 5\n",
      "   2. 66_confidence_weighted (Grade: B)\n",
      "      Confiance: 0.000 | Risque: critical | Issues: 6\n",
      "   3. 112_confidence_weighted (Grade: D)\n",
      "      Confiance: 0.000 | Risque: critical | Issues: 4\n",
      "   4. 161_confidence_weighted (Grade: B)\n",
      "      Confiance: 0.000 | Risque: critical | Issues: 6\n",
      "   5. 168_confidence_weighted (Grade: B+)\n",
      "      Confiance: 0.000 | Risque: critical | Issues: 5\n"
     ]
    }
   ],
   "source": [
    "# CELLULE DÉSACTIVÉE - Nécessite des propriétés non disponibles\n",
    "# (get_priority_score, detected_issues)\n",
    "\n",
    "print(\"Cellule désactivée car elle utilise des propriétés inexistantes :\")\n",
    "print(\"- result.get_priority_score() n'existe pas\")\n",
    "print(\"- result.detected_issues n'existe pas\")\n",
    "print(\"\\nPour voir les candidats prioritaires, utilisez :\")\n",
    "print(\"- result.priority_score (propriété directe)\")  \n",
    "print(\"- result.issues (propriété correcte)\")\n",
    "print(\"- Tri manuel basé sur confidence_score et risk_level\")\n",
    "\n",
    "# Alternative basique fonctionnelle :\n",
    "print(f\"\\nTOP 5 résumés par confiance faible (plus suspects) :\")\n",
    "confidence_ranking = [(i, r.confidence_score, r.risk_level) for i, r in enumerate(all_372_results)]\n",
    "confidence_ranking.sort(key=lambda x: x[1])  # Plus faible confiance = plus suspect\n",
    "\n",
    "for rank, (idx, conf, risk) in enumerate(confidence_ranking[:5], 1):\n",
    "    summary = all_372_summaries[idx]\n",
    "    result = all_372_results[idx]\n",
    "    print(f\"   {rank}. {summary['id']} (Grade: {summary['quality_grade']})\")\n",
    "    print(f\"      Confiance: {conf:.3f} | Risque: {risk} | Issues: {len(result.issues)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde des Résultats Enrichis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers sauvegardés:\n",
      "   Résultats: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\detection\\level1_heuristic_results.csv\n",
      "   Statistiques: c:\\Users\\beedi.goua_square-ma\\Desktop\\Gheb\\projet perso\\InsightDetector\\insight-detector\\data\\detection\\level1_heuristic_stats.json\n",
      "   372 résumés avec 15 colonnes de données\n",
      "   Performance moyenne: 79.5ms\n",
      "   Taux de détection global: 53.0%\n",
      "   Candidats fact-check: 411\n"
     ]
    }
   ],
   "source": [
    "# Affichage des fichiers sauvegardés (référence aux variables de la cellule 31)\n",
    "print(f\"Fichiers sauvegardés:\")\n",
    "print(f\"   Résultats: {results_file}\")\n",
    "print(f\"   Statistiques: {stats_file}\")\n",
    "\n",
    "print(f\"   {len(simple_results_df)} résumés avec {len(simple_results_df.columns)} colonnes de données\")\n",
    "print(f\"   Performance moyenne: {simple_stats['avg_time_ms']:.1f}ms\")\n",
    "print(f\"   Taux de détection global: {simple_stats['detection_rate_percent']:.1f}%\")\n",
    "print(f\"   Candidats fact-check: {simple_stats['total_fact_check_candidates']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
