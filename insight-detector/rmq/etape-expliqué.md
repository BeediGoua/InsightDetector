
## L'id√©e g√©n√©rale

Ton projet s'appelle **InsightDetector**.
C'est un outil qui sert √† **v√©rifier les textes g√©n√©r√©s par des IA** (comme ChatGPT ou BART), parce que ces IA √©crivent parfois des phrases qui semblent vraies mais sont **fausses ou invent√©es**.
On appelle √ßa des **hallucinations**.

**But** : cr√©er un syst√®me qui lit un texte, le r√©sume et d√©tecte automatiquement s'il contient des erreurs, incoh√©rences ou inventions.

### Pourquoi ce projet ?

Dans le monde actuel, les IA g√©n√©ratives sont partout :
- **Journalisme** : Les journaux utilisent l'IA pour r√©sumer des articles
- **Marketing** : Les entreprises g√©n√®rent du contenu automatiquement  
- **Recherche** : Les chercheurs s'appuient sur l'IA pour analyser des textes

**Le probl√®me** : Ces IA peuvent inventer des faits, m√©langer des dates, ou cr√©er des liens causaux qui n'existent pas. Exemple typique : "Le pr√©sident X a d√©clar√© Y le 15 mars" alors que cette d√©claration n'a jamais eu lieu.

**Ton solution** : Un syst√®me automatique qui agit comme un "fact-checker" intelligent, capable de dire "Attention, ce r√©sum√© contient probablement des erreurs".

---

## üõ† Le pipeline du projet

Ton syst√®me marche comme une **cha√Æne d‚Äô√©tapes**, qu‚Äôon appelle un **pipeline**.
Voici les √©tapes :

```
Articles (sources RSS) 
   ‚Üí Pr√©traitement 
   ‚Üí R√©sum√© automatique 
   ‚Üí √âvaluation & d√©tection d‚Äôhallucinations 
   ‚Üí Interface pour validation humaine
```

On va les d√©tailler une par une.

---

### 1. Collecter des articles (Phase 1)

**Ce qu'on fait concr√®tement**
Tu as cr√©√© un script qui va chercher des **articles d'actualit√©** depuis des flux RSS (des listes d'articles fournies par les journaux).

**√âtapes d√©taill√©es :**
1. **Configuration des sources RSS** : Tu as d√©fini une liste de flux RSS fiables (Le Monde, Le Figaro, Reuters, etc.)
2. **Scraping automatique** : Le script `rss_collector.py` se connecte √† chaque flux RSS
3. **Parsing XML/RSS** : Il analyse le format XML pour extraire titre, contenu, date, auteur
4. **Gestion d'erreurs** : Si un article ne se charge pas (erreur 404, timeout), le script continue sans s'arr√™ter
5. **Stockage JSON** : Chaque article est sauv√© avec sa m√©tadonn√©e dans `raw_articles.json`

**R√©sultats obtenus :**
* **547 articles** r√©colt√©s au total
* Sources diversifi√©es (actualit√©s, tech, science, √©conomie)
* Articles en fran√ßais principalement
* P√©riode de collecte : [dates que tu as utilis√©es]

**Difficult√©s rencontr√©es :**
- Certains sites bloquent les bots ‚Üí solution : ajouter des headers HTTP r√©alistes
- Flux RSS parfois indisponibles ‚Üí solution : retry automatique avec d√©lais
- Formats RSS diff√©rents selon les sites ‚Üí solution : parser flexible

**Pourquoi cette √©tape est cruciale ?**
Il faut un **corpus** (base de donn√©es de textes) vari√© et r√©aliste pour tester ton syst√®me. Sans donn√©es diversifi√©es, ton d√©tecteur d'hallucinations ne sera pas robuste.

**Code principal utilis√© :**
```python
# Dans rss_collector.py
import feedparser
import requests
import json

def collect_rss_articles(rss_urls):
    articles = []
    for url in rss_urls:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                # Extraction des m√©tadonn√©es
                article = {
                    'title': entry.title,
                    'content': entry.summary,
                    'url': entry.link,
                    'published': entry.published,
                    'source': feed.feed.title
                }
                articles.append(article)
        except Exception as e:
            print(f"Erreur avec {url}: {e}")
    return articles
```

---

### 2.  Nettoyer et pr√©parer les articles (Phase 2)

**Ce qu'on fait concr√®tement**
Cette phase est cruciale : on transforme des donn√©es "brutes" en donn√©es "exploitables" par les algorithmes.

**√âtapes d√©taill√©es du preprocessing :**

1. **D√©tection de la langue**
   ```python
   from langdetect import detect
   
   def filter_french_articles(articles):
       french_articles = []
       for article in articles:
           try:
               if detect(article['content']) == 'fr':
                   french_articles.append(article)
           except:
               pass  # Ignore les textes trop courts pour d√©tecter la langue
       return french_articles
   ```
   - **Pourquoi ?** Ton syst√®me est con√ßu pour le fran√ßais. Garder des articles en anglais/espagnol causerait des erreurs dans l'analyse s√©mantique.

2. **Suppression des doublons intelligente**
   ```python
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.metrics.pairwise import cosine_similarity
   
   def remove_duplicates(articles, threshold=0.8):
       # Calcule la similarit√© TF-IDF entre tous les articles
       vectorizer = TfidfVectorizer()
       tfidf_matrix = vectorizer.fit_transform([a['content'] for a in articles])
       similarity_matrix = cosine_similarity(tfidf_matrix)
       
       # Garde seulement les articles uniques
       unique_articles = []
       for i, article in enumerate(articles):
           is_duplicate = False
           for j in range(i):
               if similarity_matrix[i][j] > threshold:
                   is_duplicate = True
                   break
           if not is_duplicate:
               unique_articles.append(article)
       return unique_articles
   ```
   - **Pourquoi ?** Deux articles peuvent parler du m√™me √©v√©nement avec des mots diff√©rents. On utilise TF-IDF (fr√©quence des termes) pour d√©tecter ces doublons s√©mantiques.

3. **Extraction d'entit√©s nomm√©es (NER)**
   ```python
   import spacy
   
   nlp = spacy.load("fr_core_news_sm")
   
   def extract_entities(text):
       doc = nlp(text)
       entities = {
           'PERSON': [ent.text for ent in doc.ents if ent.label_ == 'PERSON'],
           'ORG': [ent.text for ent in doc.ents if ent.label_ == 'ORG'],
           'GPE': [ent.text for ent in doc.ents if ent.label_ == 'GPE'],  # Lieux
           'DATE': [ent.text for ent in doc.ents if ent.label_ == 'DATE']
       }
       return entities
   ```
   - **Pourquoi ?** Les entit√©s sont cruciales pour d√©tecter les hallucinations. Si un r√©sum√© change "Emmanuel Macron" en "Fran√ßois Mitterrand", c'est une erreur grave.

4. **Calcul de scores de qualit√©**
   ```python
   def calculate_quality_scores(article):
       content = article['content']
       
       # Score de lisibilit√© (indices de Flesch)
       readability = textstat.flesch_reading_ease(content)
       
       # Score de richesse informationnelle
       entities = extract_entities(content)
       entity_density = sum(len(v) for v in entities.values()) / len(content.split())
       
       # Score de structure (pr√©sence intro/d√©veloppement/conclusion)
       structure_score = analyze_text_structure(content)
       
       return {
           'readability': readability,
           'entity_density': entity_density,
           'structure': structure_score,
           'length': len(content.split()),
           'composite_score': (readability + entity_density*100 + structure_score) / 3
       }
   ```

**R√©sultats obtenus :**
* **Avant nettoyage** : 547 articles bruts
* **Apr√®s filtrage langue** : 398 articles fran√ßais
* **Apr√®s suppression doublons** : 186 articles uniques  
* **Fichier final** : `calibration_corpus_300.json` (enrichi avec m√©tadonn√©es)

**Difficult√©s rencontr√©es :**
- **D√©tection de langue imparfaite** : Certains articles multilingues mal class√©s ‚Üí solution : v√©rification manuelle des cas limites
- **Seuil de similarit√© d√©licat** : Trop bas = perte d'articles diff√©rents, trop haut = doublons restants ‚Üí solution : tests A/B avec diff√©rents seuils
- **Entit√©s mal reconnues** : SpaCy confond parfois pr√©noms/noms de lieux ‚Üí solution : post-processing manuel pour les entit√©s critiques

**Pourquoi cette √©tape est essentielle ?**
Un syst√®me de r√©sum√© et de d√©tection doit partir de **donn√©es propres et bien structur√©es**. C'est comme cuisiner : si tes ingr√©dients sont pourris, ton plat sera mauvais m√™me avec la meilleure recette.

---

### 3. G√©n√©rer des r√©sum√©s automatiques (Phase 3)

**Ce qu‚Äôon fait**
Tu as d√©velopp√© un **moteur de r√©sum√©** (`summarizer_engine.py`).
Il essaie plusieurs mod√®les pour r√©sumer un article :

1. **R√©sum√© abstractive**

   * Le mod√®le **r√©√©crit** le texte avec ses propres mots.
   * Exemple :
     Texte original ‚Üí
     *‚ÄúMicrosoft a licenci√© 9‚ÄØ000 employ√©s‚Ä¶‚Äù*
     R√©sum√© ‚Üí
     *‚ÄúMicrosoft a r√©duit ses effectifs de 9‚ÄØ000 personnes.‚Äù*

2. **Fallback extractif** (si le mod√®le 1 √©choue)

   * On prend directement des phrases importantes du texte.

3. **Baseline LeadK** (si tout √©choue)

   * On garde les 3 premi√®res phrases.

Ensuite, tu **fusionnes les r√©sum√©s** avec `ensemble_manager.py` :

* soit en donnant plus de poids au r√©sum√© le plus **confiant**,
* soit selon le **domaine** (ex. juridique, scientifique‚Ä¶),
* soit selon la **longueur du texte source**.

**Pourquoi ?**
Un mod√®le seul peut se tromper ‚Üí tu combines plusieurs r√©sum√©s pour √™tre plus robuste.

---

### 4. D√©tecter les hallucinations (Phase 4 ‚Äì prochaine √©tape)

C‚Äôest **le c≈ìur de ton projet**.
Une fois que tu as un r√©sum√©, tu veux savoir : **est-il fiable ?**

Ton plan est d‚Äôavoir **3 niveaux de v√©rification** :

#### Niveau 1 ‚Äì V√©rification rapide

* V√©rifie si le r√©sum√© reprend bien les mots du texte (ROUGE).
* V√©rifie la similarit√© s√©mantique (BERTScore).
* V√©rifie que les entit√©s (noms, lieux, dates) n‚Äôont pas chang√©.

#### Niveau 2 ‚Äì V√©rification factuelle

* Compare avec des bases de connaissances (Wikidata).
* V√©rifie les chiffres et les dates.
* D√©tecte les contradictions logiques.

#### Niveau 3 ‚Äì Analyse profonde

* Utilise un grand mod√®le (comme GPT-4) pour **juger la fiabilit√©**.
* Analyse plus subtile : contextes implicites, plausibilit√© globale.

**Types d‚Äôhallucinations d√©tect√©es** :

* Mauvais nom de personne (**Entity\_Substitution**)
* Chiffres invent√©s (**Numerical\_Distortion**)
* Causes invent√©es (**Causal\_Invention**)
* Dates impossibles (**Temporal\_Inconsistency**)
* etc.

**Pourquoi ?**
Parce que la fluidit√© d‚Äôun texte ‚â† v√©rit√©. Tu cr√©es un **filet de s√©curit√©**.

---

### 5. Interface utilisateur (Phase 5)

**Ce qu‚Äôon fait**
Tu as commenc√© un **dashboard Streamlit** :

* Pour afficher les articles et r√©sum√©s.
* Pour montrer les scores (ROUGE, BERTScore, etc.).
* Pour permettre aux humains de valider ou corriger.

Exemple :
Un journaliste peut charger un article ‚Üí voir le r√©sum√© + score de factualit√© ‚Üí d√©cider si le publier ou non.

**Pourquoi ?**
Les utilisateurs finaux (journalistes, analystes, entreprises) doivent **voir et comprendre les r√©sultats**.

---

### 6. D√©ploiement cloud (Phase 6)

**Ce qui est pr√©vu**

* Rendre InsightDetector disponible via une API (FastAPI).
* H√©bergement sur AWS/GCP.
* Monitoring (temps de r√©ponse, erreurs).
* S√©curit√© (authentification, quotas).

**Pourquoi ?**
Passer d‚Äôun prototype de recherche ‚Üí √† un **outil utilisable en entreprise**.

---

##  M√©triques et √©valuation

Tu ne fais pas que g√©n√©rer des r√©sum√©s, tu les **notes** avec des m√©triques :

* **ROUGE** : recouvrement lexical.
* **BERTScore** : similarit√© s√©mantique.
* **Factualit√©** : exactitude des faits.
* **Coh√©rence** : logique du texte.
* **Lisibilit√©** : fluidit√© de lecture.
* **Score composite** : m√©lange des crit√®res.

Tu visualises ces scores avec des **graphiques (barplots, heatmaps)** pour comparer plusieurs r√©sum√©s.

---

## Concr√®tement, tu fais donc :

1. **Tu r√©coltes des articles**
   ‚Üí comme construire ta biblioth√®que.

2. **Tu les nettoies et tu choisis les meilleurs**
   ‚Üí tu enl√®ves les doublons et tu gardes les textes fiables.

3. **Tu les r√©sumes automatiquement**
   ‚Üí tu essaies plusieurs mod√®les, avec des plans B si √ßa √©choue.

4. **Tu pr√©pares la d√©tection d‚Äôhallucinations**
   ‚Üí en construisant un syst√®me multi-niveaux pour rep√©rer les erreurs.

5. **Tu cr√©es une interface pour les humains**
   ‚Üí afin qu‚Äôils puissent v√©rifier et corriger facilement.

6. **Tu planifies le d√©ploiement cloud**
   ‚Üí pour qu‚Äôon puisse utiliser ton syst√®me partout et rapidement.

 